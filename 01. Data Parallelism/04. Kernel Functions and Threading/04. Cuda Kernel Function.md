## Kernel Functions in CUDA: The `_global_` Keyword and Threading Model

### Introdução

Em CUDA, a execução paralela de tarefas computacionais é orquestrada através de *kernels*. Um kernel CUDA é uma função que é executada no dispositivo (GPU) e chamada a partir do host (CPU) [^1]. A correta definição e execução de kernels são fundamentais para o aproveitamento do poder de processamento paralelo das GPUs. Este capítulo detalha a função do qualificador `_global_`, a estrutura de um kernel, e a relação entre kernels e o modelo de threading do CUDA.

### Conceitos Fundamentais

#### O Qualificador `_global_`

A palavra-chave `_global_` é um qualificador de declaração de função que define um kernel CUDA. As seguintes características são inerentes a funções declaradas com `_global_`:

1.  **Execução no Dispositivo:** O kernel é executado na GPU.
2.  **Chamada a partir do Host:** O kernel só pode ser chamado a partir do código do host (CPU).
3.  **Assíncrono:** A chamada de um kernel é *assíncrona*, ou seja, o host continua a execução sem esperar que o kernel termine, a menos que uma sincronização explícita seja imposta.

Um exemplo de declaração de kernel é:

```c++
__global__ void myKernel(float *data, int size) {
    // Código do kernel aqui
}
```

Aqui, `myKernel` é uma função que será executada na GPU.

![Tabela de qualificadores CUDA C para declaração de funções, mostrando onde são executadas e de onde podem ser chamadas.](./../images/image1.jpg)

#### Kernel Structure and Threading Model

A estrutura de um kernel CUDA difere significativamente de funções tradicionais executadas na CPU. Em vez de laços iterativos explícitos, a execução do kernel é definida por um *grid* de threads [^1].

1.  **Grid de Threads:** Quando um kernel é lançado, ele é executado por uma grade (grid) de threads. Essa grade é composta por blocos, e cada bloco contém threads. A dimensão da grade e dos blocos é especificada no momento do lançamento do kernel.

2.  **Substituição de Laços por Threads:** Em vez de iterar sobre dados usando laços `for` ou `while`, cada thread em um kernel processa uma porção dos dados. Idealmente, cada thread executa uma única iteração da tarefa a ser paralelizada [^1].

3.  **Identificação de Threads:** Cada thread dentro de um kernel é identificado de forma única por dois vetores tridimensionais: `blockIdx` e `threadIdx`. `blockIdx` representa o índice do bloco ao qual a thread pertence, e `threadIdx` representa o índice da thread dentro do bloco. Além disso, as variáveis `blockDim` e `gridDim` fornecem as dimensões do bloco e da grade, respectivamente.

4.  **Cálculo do Índice Global:** Para acessar elementos específicos de um array de dados, é comum calcular um índice global para cada thread, usando as informações `blockIdx`, `threadIdx`, `blockDim` e `gridDim`.

    $$
    index = blockIdx.x * blockDim.x + threadIdx.x
    $$

    Esta é a fórmula básica para um grid unidimensional. Para grids bidimensionais ou tridimensionais, a fórmula é estendida para incluir as dimensões adicionais.

![Illustration of CUDA thread grid and block organization with global data index calculation.](./../images/image7.jpg)

#### Condicionais em Kernels

Em muitos casos, o número total de threads lançados pode exceder o tamanho dos dados a serem processados. Para evitar acessos inválidos à memória, é crucial incluir condicionais dentro do kernel para desabilitar threads que excedam o tamanho do vetor ou conjunto de dados [^1].

Por exemplo:

```c++
__global__ void vectorAdd(float *a, float *b, float *c, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) {
        c[i] = a[i] + b[i];
    }
}
```

Neste kernel `vectorAdd`, a condicional `if (i < n)` garante que apenas threads com um índice `i` menor que o tamanho do vetor `n` realizem a operação de adição. As outras threads permanecem inativas, evitando erros.

### Conclusão

O qualificador `_global_` é essencial para definir kernels CUDA, que são a espinha dorsal da programação GPU. A estrutura dos kernels, baseada em grids de threads, possibilita a execução paralela eficiente de tarefas. A inclusão de condicionais para desabilitar threads excedentes é fundamental para garantir a correção e a estabilidade dos programas CUDA [^1]. O entendimento profundo desses conceitos é crucial para escrever código CUDA otimizado e eficaz.

### Referências

[^1]: Informação fornecida no contexto: "_global_ keyword identifies a function as a CUDA kernel function, which is executed on the device and can only be called from the host code. In CUDA kernels, the loop structure is replaced by the grid of threads. Each thread handles one iteration. The kernel may include a conditional to disable threads that exceed vector length."

<!-- END -->