## Kernel Functions and Threading in CUDA

### Introdução
Em CUDA, o **kernel** é a unidade fundamental de execução paralela em GPUs. Este capítulo explora em detalhes as **funções kernel** e o modelo de *threading* hierárquico que possibilita a execução paralela massiva de código em CUDA. A compreensão do funcionamento dos kernels e da organização dos threads é crucial para otimizar o desempenho de aplicações CUDA.

### Conceitos Fundamentais

A computação paralela em CUDA é estruturada em torno do conceito de **kernel functions** [^1]. Uma função kernel é um trecho de código C/C++ que é executado por múltiplos threads em paralelo na GPU. Quando o *host* (CPU) lança um kernel, o sistema de *runtime* CUDA gera uma **grid** de threads, que é organizada em uma hierarquia de dois níveis: **grid** e **blocks** [^1].

#### Modelo de Programação SPMD

CUDA adota o paradigma de programação **Single Program, Multiple Data (SPMD)** [^1]. Isso significa que cada thread executa o *mesmo* código (o kernel) [^1], mas em *diferentes* dados. Essa abordagem permite que os programadores explorem o paralelismo de dados inerente a muitos problemas computacionais.



#### Hierarquia Grid-Block

A hierarquia **grid-block** é fundamental para a organização e execução de threads em CUDA.

*   **Grid:** Uma grid representa a unidade máxima de paralelismo em CUDA. Ela consiste em um conjunto de **blocos** de threads. A grid é uma estrutura lógica que organiza todos os threads que executarão o kernel.

*   **Block:** Um bloco é um grupo de threads que podem cooperar entre si usando recursos compartilhados, como memória compartilhada (shared memory) e barreiras de sincronização (`__syncthreads()`). Os threads dentro de um bloco são executados no mesmo *multiprocessor* (SM) da GPU.

#### Identificação de Threads

Cada thread dentro de um grid é identificado de forma única por duas coordenadas:

*   `blockIdx`: Índice do bloco ao qual o thread pertence.

*   `threadIdx`: Índice do thread dentro do bloco.

Além disso, CUDA fornece as variáveis `blockDim` e `gridDim`, que representam as dimensões de um bloco e de um grid, respectivamente. Essas variáveis são usadas para calcular o índice global de um thread dentro do grid.



![Illustration of CUDA thread grid and block organization with global data index calculation.](./../images/image7.jpg)

#### Exemplo

Para ilustrar, considere um exemplo onde um kernel é lançado com uma grid de dimensão (2, 2) e cada bloco tem dimensão (4, 4). Isso significa que:

*   `gridDim.x = 2`, `gridDim.y = 2`
*   `blockDim.x = 4`, `blockDim.y = 4`

O número total de threads no grid é `gridDim.x * gridDim.y * blockDim.x * blockDim.y = 2 * 2 * 4 * 4 = 64` threads. Cada thread pode calcular seu índice global usando as coordenadas `blockIdx` e `threadIdx`, juntamente com as dimensões `blockDim` e `gridDim`.

#### Lançamento de Kernel

O lançamento de um kernel é feito utilizando a seguinte sintaxe:

```c++
kernel_name<<<gridDim, blockDim>>>(argumentos);
```

onde:

*   `kernel_name` é o nome da função kernel.
*   `gridDim` é a dimensão do grid.
*   `blockDim` é a dimensão de cada bloco.
*   `argumentos` são os argumentos a serem passados para o kernel.

Por exemplo:

```c++
__global__ void myKernel(float *data, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        data[idx] = data[idx] * 2.0f;
    }
}

int main() {
    int size = 1024;
    float *data;
    cudaMallocManaged(&data, size * sizeof(float));

    // Inicializar os dados
    for (int i = 0; i < size; ++i) {
        data[i] = (float)i;
    }

    dim3 gridDim(32); // 32 blocos
    dim3 blockDim(32); // 32 threads por bloco

    myKernel<<<gridDim, blockDim>>>(data, size);

    cudaDeviceSynchronize(); // Aguardar a conclusão do kernel

    // Verificar os resultados (opcional)
    for (int i = 0; i < size; ++i) {
        //printf("data[%d] = %f\n", i, data[i]);
    }

    cudaFree(data);

    return 0;
}

```

Neste exemplo, o kernel `myKernel` é lançado com 32 blocos, cada um contendo 32 threads. Cada thread calcula seu índice global `idx` e, se `idx` estiver dentro dos limites do tamanho dos dados, o thread multiplica o elemento correspondente do array `data` por 2.0.

### Conclusão

A programação CUDA, baseada no modelo SPMD, utiliza funções kernel e uma hierarquia grid-block para facilitar a execução paralela em GPUs [^1]. A correta compreensão desses conceitos é essencial para o desenvolvimento de aplicações CUDA eficientes. Dominar o lançamento de kernels, a organização dos threads e a utilização das variáveis `blockIdx`, `threadIdx`, `blockDim` e `gridDim` permite aos programadores explorar ao máximo o potencial de paralelismo oferecido pelas GPUs.

### Referências
[^1]: In CUDA, a kernel function specifies the code executed by all threads during a parallel phase. CUDA programming is an instance of the Single Program, Multiple Data (SPMD) parallel programming style. Each thread executes the same code on different data. When the host launches a kernel, the CUDA runtime system generates a grid of threads organized in a two-level hierarchy: grid and blocks.
<!-- END -->