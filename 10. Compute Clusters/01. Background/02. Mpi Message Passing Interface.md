## MPI Basics in Heterogeneous Computing Clusters

### Introdução
Em sistemas de computação heterogêneos de alto desempenho (HPC), a agregação de poder computacional através de clusters de nós de computação é uma prática comum [^1]. Historicamente, a Interface de Passagem de Mensagens (MPI) tem sido a interface de programação dominante para esses clusters [^1]. Este capítulo visa fornecer uma introdução aos conceitos-chave do MPI necessários para escalar aplicações heterogêneas em um ambiente de cluster de múltiplos nós [^1]. O foco principal será no particionamento de domínio, comunicação ponto a ponto e comunicação coletiva no contexto de escalonamento de um kernel CUDA em múltiplos nós [^1].

### Conceitos Fundamentais
**MPI (Message Passing Interface)** é um conjunto de funções API para comunicação entre processos executando em um cluster de computação [^2]. O MPI adota um modelo de memória distribuída, no qual os processos trocam informações enviando mensagens uns aos outros [^2]. Uma característica importante do MPI é que ele abstrai as complexidades da rede de interconexão, permitindo que os desenvolvedores se concentrem na lógica da aplicação em vez dos detalhes de baixo nível da comunicação [^2]. A implementação MPI permite que os processos se enderecem usando números lógicos, similar ao uso de números de telefone em um sistema telefônico [^2].

Em uma aplicação MPI típica, os dados e o trabalho são particionados entre os processos [^2]. Cada nó pode conter um ou mais processos, e esses processos podem precisar trocar dados entre si à medida que progridem [^2]. Essa necessidade é satisfeita pelo envio e recebimento de mensagens [^2]. Em alguns casos, os processos também precisam sincronizar-se e gerar resultados coletivos ao colaborarem em uma grande tarefa. Isso é feito com funções API de comunicação coletiva [^2].

Assim como CUDA, os programas MPI são baseados no modelo de execução paralela SPMD (single program, multiple data) [^3]. Todos os processos MPI executam o mesmo programa [^3]. O sistema MPI fornece um conjunto de funções API para estabelecer sistemas de comunicação que permitem que os processos se comuniquem entre si [^3].

**Funções MPI Essenciais:**
O MPI fornece um conjunto de funções API essenciais para configurar e desmontar sistemas de comunicação [^3]. Cinco funções básicas são cruciais:

*   `MPI_Init(int *argc, char ***argv)`: Inicializa o MPI [^4].
*   `MPI_Comm_rank(MPI_Comm comm, int *rank)`: Retorna o *rank* (identificador único) do processo de chamada no grupo de comunicadores `comm` [^4].
*   `MPI_Comm_size(MPI_Comm comm, int *size)`: Retorna o número total de processos no grupo de comunicadores `comm` [^4].
*   `MPI_Comm_abort(MPI_Comm comm)`: Termina a conexão de comunicação MPI com um *error flag* [^4].
*   `MPI_Finalize()`: Finaliza o MPI, liberando todos os recursos alocados [^4].

Cada processo começa inicializando o *runtime* MPI com uma chamada `MPI_Init()` [^5]. Isso inicializa o sistema de comunicação para todos os processos que executam a aplicação [^5]. Uma vez que o *runtime* MPI é inicializado, cada processo chama duas funções para se preparar para a comunicação [^5]. A primeira função é `MPI_Comm_rank()`, que retorna um número único para cada processo de chamada, chamado de *MPI rank* ou *process ID* [^5]. Os números recebidos pelos processos variam de 0 até o número de processos menos 1 [^5]. O *MPI rank* para um processo é equivalente à expressão `blockIdx.x*blockDim.x + threadIdx.x` para um *thread* CUDA [^5]. Ele identifica unicamente o processo em uma comunicação, similar ao número de telefone em um sistema telefônico [^5].

A função `MPI_Comm_rank()` recebe dois parâmetros [^5]. O primeiro é um tipo interno do MPI, `MPI_Comm`, que especifica o escopo da requisição [^5]. Os valores de `MPI_Comm` são comumente referidos como um **comunicador** [^5]. `MPI_Comm` e outros tipos internos do MPI são definidos em um arquivo de cabeçalho `mpi.h` que deve ser incluído em todos os arquivos de programa C que usam MPI [^5]. Isso é similar ao arquivo de cabeçalho `cuda.h` para programas CUDA [^5]. Uma aplicação MPI pode criar um ou mais *intracomunicadores* [^5]. Membros de cada *intracomunicador* são processos MPI [^5]. `MPI_Comm_rank()` atribui um ID único para cada processo em um *intracomunicador* [^5]. O valor do parâmetro passado é `MPI_COMM_WORLD`, o que significa que o *intracomunicador* inclui todos os processos MPI executando a aplicação [^5].

O segundo parâmetro para a função `MPI_Comm_rank()` é um ponteiro para uma variável inteira na qual a função depositará o valor do *rank* retornado [^5]. Após o retorno de `MPI_Comm_rank()`, a variável conterá o ID único para o processo de chamada [^5].

A segunda função API é `MPI_Comm_size()`, que retorna o número total de processos MPI executando no *intracomunicador* [^6]. A função `MPI_Comm_size()` recebe dois parâmetros [^6]. O primeiro é um tipo interno do MPI, `MPI_Comm`, que dá o escopo da requisição [^7]. O escopo é `MPI_COMM_WORLD`, o valor retornado é o número de processos MPI executando a aplicação [^7]. Isso é requisitado por um usuário quando a aplicação é submetida usando o comando `mpirun` ou o comando `mpiexec` [^7]. No entanto, o usuário pode não ter requisitado um número suficiente de processos [^7]. Além disso, o sistema pode ou não ser capaz de criar todos os processos requisitados [^7]. Portanto, é uma boa prática para um programa de aplicação MPI verificar o número atual de processos executando [^7].

O segundo parâmetro é um ponteiro para uma variável inteira na qual a função `MPI_Comm_size()` depositará o valor de retorno [^7]. Após a função retornar, a variável conterá o número de processos MPI executando a aplicação [^7].

Conforme mostrado na Figura 19.5 [^7], a função `MPI_Comm_abort()` recebe dois parâmetros [^7]. O primeiro é o escopo da requisição [^7]. O escopo é todos os processos MPI executando a aplicação [^7]. O segundo parâmetro é um código para o tipo de erro que causou o aborto [^7]. Qualquer número diferente de 0 indica que um erro aconteceu [^7].

Após a aplicação completar sua computação, ela notifica o *runtime* MPI com uma chamada para `MPI_Finalize()`, que libera todos os recursos de comunicação MPI alocados para a aplicação [^7]. A aplicação pode então sair com um valor de retorno 0, que indica que nenhum erro ocorreu [^7].

### Conclusão
O MPI é uma ferramenta poderosa para programação paralela em clusters de computação heterogêneos. Ao entender os conceitos básicos do MPI, como comunicadores, ranks e funções de inicialização e finalização, os desenvolvedores podem efetivamente escalar suas aplicações para múltiplos nós. As funções de comunicação ponto a ponto e coletiva fornecem os meios para os processos trocarem dados e sincronizarem, permitindo a execução paralela de tarefas complexas. O conhecimento apresentado neste capítulo fornece uma base sólida para explorar recursos mais avançados do MPI e padrões de programação.

### Referências
[^1]: Programming a Heterogeneous Computing Cluster, p. 407
[^2]: Programming a Heterogeneous Computing Cluster, p. 408
[^3]: Programming a Heterogeneous Computing Cluster, p. 410
[^4]: Programming a Heterogeneous Computing Cluster, p. 412
[^5]: Programming a Heterogeneous Computing Cluster, p. 411
[^6]: Programming a Heterogeneous Computing Cluster, p. 412
[^7]: Programming a Heterogeneous Computing Cluster, p. 413
<!-- END -->