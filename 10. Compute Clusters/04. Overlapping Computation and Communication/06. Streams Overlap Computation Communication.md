## Streams for Overlapping Computation and Communication

### Introdução
Em continuidade ao tópico de *Overlapping Computation and Communication* [^421], este capítulo se aprofundará no uso de **streams** para habilitar atividades paralelas no contexto de programação CUDA. Como vimos anteriormente, uma estratégia simples de computação, na qual cada processo computa sua partição inteira, troca dados de halo com seus vizinhos e repete, não é eficiente. Para melhorar o desempenho, é crucial utilizar tanto a rede de comunicação quanto o hardware de computação simultaneamente [^421]. O uso de **streams** e **pinned memory** são essenciais para suportar as atividades paralelas necessárias para otimizar a sobreposição de computação e comunicação [^422].

### Conceitos Fundamentais

#### Pinned Memory (Memória Pinned)
Para entender a necessidade do *pinned memory*, é crucial compreender como os sistemas operacionais modernos gerenciam a memória virtual [^423]. O sistema operacional aloca um espaço de memória virtual para cada aplicação. Este espaço é dividido em páginas, e apenas as páginas ativamente usadas são mapeadas para a memória física. Em situações de alta demanda por memória, o sistema operacional pode precisar mover páginas da memória física para um armazenamento em massa, como discos (*page out*) [^423].

A implementação de `cudaMemcpy()` utiliza um dispositivo de acesso direto à memória (DMA) [^424]. O hardware DMA opera em endereços físicos. Se os dados forem paginados antes da conclusão da operação DMA, a operação poderá ser corrompida [^424]. Para evitar esse problema, o CUDA executa a operação de cópia em duas etapas: primeiro, copia os dados para um *pinned memory buffer* e, em seguida, usa o DMA para copiar os dados do *pinned memory buffer* para a memória do dispositivo (ou vice-versa). *Pinned memory buffers* são marcados para que o mecanismo de paginação do sistema operacional não possa paginá-los [^424].

Embora essa abordagem resolva o problema de corrupção, ela introduz um atraso adicional devido à cópia extra e leva a uma implementação síncrona de `cudaMemcpy()` [^424]. Isso significa que o programa host não pode continuar a execução até que a função `cudaMemcpy()` seja concluída, serializando todas as operações de cópia [^424]. Para suportar cópias rápidas com mais paralelismo, o CUDA fornece a função `cudaMemcpyAsync()` [^424]. Para usar esta função, o buffer de memória do host deve ser alocado como *pinned memory buffer* [^424].

A alocação de *pinned memory* é realizada utilizando a função `cudaHostAlloc()` [^423], que garante que a memória alocada seja *page-locked*. Esta função recebe três parâmetros: um ponteiro para um ponteiro para a memória alocada, o tamanho em bytes da memória a ser alocada, e um conjunto de opções para uso avançado. Para a maioria dos casos básicos, o valor padrão `cudaHostAllocDefault` é suficiente [^424]. Nas linhas 19-22 de Figure 19.13 [^423], podemos observar a alocação de *pinned memory* para os buffers de memória do host das *boundary slices* esquerda e direita, e das *halo slices* esquerda e direita.

#### Streams
A segunda característica avançada do CUDA é o conceito de **streams**, que permite a execução gerenciada e simultânea de funções da API CUDA [^425]. Um **stream** é uma sequência ordenada de operações. Quando o código host chama uma função `cudaMemcpyAsync()` ou lança um kernel, ele pode especificar um **stream** como um de seus parâmetros. Todas as operações no mesmo **stream** serão executadas sequencialmente, enquanto operações de diferentes **streams** podem ser executadas em paralelo [^425].

Na linha 23 de Figure 19.13 [^423], duas variáveis do tipo `cudaStream_t` são declaradas. Este tipo é definido em `cuda.h`. Estas variáveis são então utilizadas para chamar a função `cudaStreamCreate()`. Cada chamada a `cudaStreamCreate()` cria um novo **stream** e armazena um ponteiro para o **stream** em seu parâmetro. Após as chamadas nas linhas 24 e 25 [^423], o código host pode utilizar `stream0` ou `stream1` em chamadas subsequentes a `cudaMemcpyAsync()` e lançamentos de kernel [^425].

Linhas 36 e 37 em Figure 19.14 [^426] lançam kernels dentro de `stream0`, e o kernel é executado sequencialmente. A Linha 38 [^426] lança um kernel em `stream1` e é executado paralelamente àqueles lançados nas linhas 36 e 37 [^425].

### Conclusão
O uso de *pinned memory* e **streams** permite a sobreposição de comunicação e computação, otimizando o uso dos recursos computacionais e da rede de comunicação [^421]. A alocação de *pinned memory* garante transferências de dados mais rápidas e eficientes, enquanto os **streams** permitem a execução paralela de operações de cópia e kernels, maximizando o desempenho geral da aplicação [^425].

### Referências
[^421]: Programming a Heterogeneous Computing Cluster, Capítulo 19, Seção 19.5: Overlapping Computation and Communication.
[^422]: Programming a Heterogeneous Computing Cluster, Capítulo 19, Seção 19.5: Overlapping Computation and Communication.
[^423]: Programming a Heterogeneous Computing Cluster, Capítulo 19, Seção 19.5: Overlapping Computation and Communication.
[^424]: Programming a Heterogeneous Computing Cluster, Capítulo 19, Seção 19.5: Overlapping Computation and Communication.
[^425]: Programming a Heterogeneous Computing Cluster, Capítulo 19, Seção 19.5: Overlapping Computation and Communication.
[^426]: Programming a Heterogeneous Computing Cluster, Capítulo 19, Seção 19.5: Overlapping Computation and Communication.
<!-- END -->