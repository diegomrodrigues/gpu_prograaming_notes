## Pinned Memory Allocation for Overlapping Computation and Communication

### Introdução
Este capítulo explora a técnica de alocação de memória *pinned* (*page-locked*) como um componente crucial para otimizar a sobreposição de computação e comunicação em ambientes heterogêneos, particularmente ao usar CUDA. A alocação de memória *pinned* garante que a memória alocada não seja paginada pelo sistema operacional, permitindo transferências de dados mais rápidas entre a CPU e a GPU, utilizando a chamada de API `cudaHostAlloc()` [^17]. A capacidade de sobrepor a computação e a comunicação é fundamental para alcançar alto desempenho em sistemas de computação heterogêneos, e a memória *pinned* desempenha um papel fundamental para possibilitar essa sobreposição de maneira eficiente.

### Conceitos Fundamentais

Em sistemas computacionais modernos, o sistema operacional gerencia um espaço de memória virtual para cada aplicação [^17]. Cada aplicação tem acesso a um espaço de endereçamento grande e consecutivo. Na realidade, o sistema tem uma quantidade limitada de memória física que precisa ser compartilhada entre todas as aplicações em execução. Esse compartilhamento é realizado particionando o espaço de memória virtual em *páginas* e mapeando apenas as páginas ativamente usadas para a memória física. Quando há muita demanda por memória, o sistema operacional precisa "paginar para fora" algumas das páginas da memória física para o armazenamento em massa, como discos. Portanto, uma aplicação pode ter seus dados paginados para fora a qualquer momento durante sua execução [^17].

A implementação de `cudaMemcpy()` usa um tipo de hardware chamado dispositivo de acesso direto à memória (DMA) [^18]. Quando uma função `cudaMemcpy()` é chamada para copiar entre as memórias do host e do dispositivo, sua implementação usa um dispositivo DMA para concluir a tarefa. No lado da memória do host, o hardware DMA opera em endereços físicos. Ou seja, o sistema operacional precisa fornecer um endereço físico traduzido para o dispositivo DMA. No entanto, existe a chance de que os dados possam ser paginados para fora antes que a operação DMA seja concluída. Os locais de memória física para os dados podem ser reatribuídos a outros dados de memória virtual. Nesse caso, a operação DMA pode ser potencialmente corrompida, pois seus dados podem ser sobrescritos pela atividade de paginação [^18].

Uma solução comum para esse problema de corrupção é que o tempo de execução CUDA execute a operação de cópia em duas etapas [^18]. Para uma cópia de *host-to-device*, o tempo de execução CUDA primeiro copia os dados da memória do host de origem em um buffer de memória "*pinned*", o que significa que os locais de memória são marcados para que o mecanismo de paginação operacional não pagine os dados para fora. Em seguida, ele usa o dispositivo DMA para copiar os dados do buffer de memória *pinned* para a memória do dispositivo. Para uma cópia de *device-to-host*, o tempo de execução CUDA primeiro usa um dispositivo DMA para copiar os dados da memória do dispositivo para um buffer de memória *pinned*. Em seguida, ele copia os dados da memória *pinned* para o local da memória do host de destino. Ao usar um buffer de memória *pinned* extra, a cópia DMA estará segura contra qualquer atividade de paginação [^18].

Existem dois problemas com essa abordagem [^18]. Um é que a cópia extra adiciona atraso à operação `cudaMemcpy()`. O segundo é que a complexidade extra envolvida leva a uma implementação síncrona da função `cudaMemcpy()`. Ou seja, o programa host não pode continuar a ser executado até que a função `cudaMemcpy()` conclua sua operação e retorne. Isso serializa todas as operações de cópia. Para oferecer suporte a cópias rápidas com mais paralelismo, o CUDA fornece uma função `cudaMemcpyAsync()` [^18].

Para usar a função `cudaMemcpyAsync()`, o buffer de memória do host deve ser alocado como um buffer de memória *pinned* [^18]. Isso é feito nas linhas 19-22 para os buffers de memória do host da fronteira esquerda, fronteira direita, halo esquerdo e fatias de halo direito [^17]. Esses buffers são alocados com uma função `cudaHostAlloc()` especial, que garante que a memória alocada seja *pinned* ou bloqueada por página das atividades de paginação [^18]. Observe que a função `cudaHostAlloc()` usa três parâmetros. Os dois primeiros são os mesmos que `cudaMalloc()`. O terceiro especifica algumas opções para uso mais avançado. Para a maioria dos casos de uso básicos, podemos simplesmente usar o valor padrão `cudaHostAllocDefault` [^18].

```c++
cudaHostAlloc((void **) &h_left_boundary, num_halo_bytes, cudaHostAllocDefault);
cudaHostAlloc((void **)&h_right_boundary, num_halo_bytes, cudaHostAllocDefault);
cudaHostAlloc((void **) &h_left_halo, num_halo_bytes, cudaHostAllocDefault);
cudaHostAlloc((void **)&h_right_halo, num_halo_bytes, cudaHostAllocDefault);
```

A alocação de memória *pinned* garante que a memória alocada não será paginada pelo sistema operacional [^17]. Isso é feito com a chamada de API `cudaHostAlloc()` [^17]. As linhas 19-22 alocam buffers de memória para as fatias de fronteira esquerda e direita e os halos esquerdo e direito. As fatias de fronteira esquerda e direita precisam ser enviadas da memória do dispositivo para os processos vizinhos esquerdo e direito. Os buffers são usados como uma área de preparação de memória do host para a qual os dados são copiados e, em seguida, usados como o buffer de origem para `MPI_Send()` para processos vizinhos. As fatias de halo esquerdo e direito precisam ser recebidas dos processos vizinhos. Os buffers são usados como uma área de preparação de memória do host para `MPI_Recv()` para usar como um buffer de destino e, em seguida, copiados para a memória do dispositivo [^17].

### Conclusão

A alocação de memória *pinned* é uma técnica essencial para otimizar a sobreposição de computação e comunicação em sistemas heterogêneos [^17]. Ao garantir que a memória alocada não seja paginada pelo sistema operacional, a memória *pinned* permite transferências de dados mais rápidas e eficientes entre a CPU e a GPU [^17]. Isso, por sua vez, permite que a computação e a comunicação sejam sobrepostas de forma mais eficaz, levando a melhorias significativas de desempenho. O uso da API `cudaHostAlloc()` é fundamental para implementar a alocação de memória *pinned* em CUDA.

### Referências
[^17]: Capítulo 19, Seção 19.5
[^18]: Capítulo 19, Seção 19.5

<!-- END -->