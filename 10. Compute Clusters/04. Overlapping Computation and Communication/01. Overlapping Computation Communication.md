## Overlapping Computation and Communication in Parallel Applications

### Introdução
Em aplicações paralelas, a busca por otimização de desempenho é constante. Uma técnica eficaz para atingir esse objetivo é o *overlapping computation and communication*, que visa maximizar a utilização dos recursos computacionais e de comunicação simultaneamente [^1]. Este capítulo explora essa técnica, detalhando sua implementação e os benefícios que ela proporciona, especialmente no contexto de clusters heterogêneos e programação MPI/CUDA.

### Conceitos Fundamentais
A técnica de *overlapping computation and communication* consiste em dividir as tarefas de computação de cada processo em duas etapas [^1]. Em vez de executar uma etapa de computação completa seguida por uma etapa de comunicação completa, essa abordagem busca intercalar essas operações para melhor utilizar os recursos disponíveis.

A motivação por trás dessa técnica reside no fato de que, em uma estratégia simples onde a computação é realizada primeiro e a comunicação depois, o sistema opera em um de dois modos distintos [^1]:
1.  **Modo de Computação:** Todos os processos computacionais estão ativos, mas a rede de comunicação permanece ociosa.
2.  **Modo de Comunicação:** Todos os processos trocam dados, enquanto o hardware de computação fica subutilizado.

Essa alternância entre computação e comunicação leva a um uso ineficiente dos recursos. O ideal seria utilizar tanto a rede de comunicação quanto o hardware de computação de forma contínua [^1].

Para alcançar esse objetivo, a tarefa de computação de cada processo é dividida em duas etapas [^1]:

**Estágio 1: Computação da Borda (Boundary Computation)**

Nesta etapa, cada processo computa as fatias de dados de sua partição que serão necessárias como células de halo (halo cells) pelos seus vizinhos na próxima iteração [^1, 11]. Assumindo o uso de quatro fatias de dados de halo, cada processo calcula quatro fatias de seus dados de contorno. Isso garante que os dados necessários para a comunicação futura estejam prontos com antecedência.

**Estágio 2: Comunicação e Computação Interna**

Nesta etapa, cada processo executa duas atividades em paralelo [^1]:
1.  **Comunicação:** Os novos valores de contorno são comunicados aos processos vizinhos. Isso envolve copiar os dados da memória do dispositivo (device memory) para a memória do host (host memory) e, em seguida, enviar mensagens MPI para os vizinhos.
2.  **Computação Interna:** O restante dos dados na partição é calculado.

Ao sobrepor a comunicação com a computação interna, a latência da comunicação pode ser efetivamente "escondida", desde que o tempo necessário para a computação interna seja maior ou igual ao tempo necessário para a comunicação [^1].

**Implementação com CUDA e MPI**

A implementação do *overlapping computation and communication* pode ser facilitada com o uso de recursos avançados do CUDA, como *pinned memory* (memória fixa) e *streams* [^1].

*   **Pinned Memory:** A alocação de memória fixa (pinned memory), também conhecida como *page-locked memory*, garante que a memória alocada não seja paginada para o disco pelo sistema operacional [^1, 18]. Isso é crucial porque as operações de DMA (Direct Memory Access) usadas pelo CUDA para transferir dados entre a memória do host e a memória do dispositivo operam em endereços físicos. Se a memória do host puder ser paginada, o sistema operacional precisará traduzir os endereços virtuais para físicos, o que pode introduzir atrasos e corrupção de dados [^18]. A alocação de memória fixa é realizada usando a função `cudaHostAlloc()` [^17, 18].

*   **Streams:** Streams no CUDA permitem a execução simultânea de operações da API CUDA [^19]. Um stream é uma sequência ordenada de operações, e todas as operações em um mesmo stream são executadas sequencialmente. No entanto, operações em streams diferentes podem ser executadas em paralelo. Isso permite que a computação e a comunicação sejam sobrepostas, lançando kernels e operações de cópia de memória em streams diferentes [^19].

No código exemplo, os streams `stream0` e `stream1` são criados para gerenciar a execução paralela das diferentes tarefas [^17, 19].

A sincronização entre os streams é crucial para garantir a consistência dos dados. A função `cudaStreamSynchronize()` garante que todas as operações em um determinado stream sejam concluídas antes que o programa continue [^22]. Adicionalmente, `MPI_Barrier()` é usada para garantir que todos os processos atinjam um ponto de sincronização antes de prosseguir [^20, 26].

### Conclusão
A técnica de *overlapping computation and communication* oferece uma maneira eficaz de melhorar o desempenho de aplicações paralelas, maximizando a utilização dos recursos de computação e comunicação. Ao dividir a computação em estágios e usar recursos como *pinned memory* e *streams* do CUDA, é possível sobrepor a comunicação com a computação, reduzindo a latência e aumentando a eficiência geral da aplicação [^1].

### Referências
[^1]: Capítulo 19 do livro "Programming a Heterogeneous Computing Cluster".
<!-- END -->