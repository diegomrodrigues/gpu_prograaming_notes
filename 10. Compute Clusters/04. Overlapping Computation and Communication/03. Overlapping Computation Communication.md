## Overlapping Computation and Communication: Stage 2 Details

### Introdução
Em continuidade à discussão sobre **overlapping computation and communication**, este capítulo aprofunda a segunda etapa do processo, onde cada processo executa atividades em paralelo: a comunicação de novos valores de contorno para os processos vizinhos e o cálculo dos dados restantes em sua partição [^15, ^16]. Este método, ilustrado na Figura 19.12 [^16], visa maximizar a utilização dos recursos de computação e comunicação, ocultando os atrasos de comunicação por meio da sobreposição com o cálculo.

### Conceitos Fundamentais
A segunda etapa do processo de **overlapping computation and communication** envolve duas atividades principais executadas em paralelo [^16]:

1.  **Comunicação dos novos valores de contorno:** Após a primeira etapa, onde os valores de contorno são calculados, esses valores precisam ser trocados com os processos vizinhos para que eles possam realizar seus cálculos na próxima iteração. Esta etapa envolve copiar os dados da memória do dispositivo (device memory) para a memória do host (host memory) e, em seguida, usar funções MPI para enviar os dados aos vizinhos [^16].

2.  **Cálculo dos dados restantes na partição:** Enquanto a comunicação está em andamento, o processo calcula os dados restantes em sua partição que não dependem dos valores de contorno recém-calculados de outros processos. Esta etapa visa utilizar a capacidade de computação do sistema enquanto a comunicação está em andamento [^16].

Para suportar essas atividades paralelas, são utilizadas duas funcionalidades avançadas do modelo de programação CUDA: **pinned memory allocation** e **streams** [^16].

*   **Pinned Memory Allocation:** A alocação de memória *pinned* garante que a memória alocada não será paginada pelo sistema operacional [^16, ^17]. Isso é crucial porque as operações de acesso direto à memória (DMA), utilizadas pelas funções `cudaMemcpy()`, operam em endereços físicos. Se a memória for paginada, o endereço físico pode mudar durante a operação DMA, levando à corrupção dos dados [^18]. Para evitar isso, a memória é alocada como *pinned* usando a função `cudaHostAlloc()` [^17, ^18].
    > A função `cudaHostAlloc()` aloca um buffer de memória *pinned*, também conhecido como buffer de memória *page-locked* [^17].

*   **Streams:** Um *stream* é uma sequência ordenada de operações [^19]. As operações em um mesmo *stream* são executadas sequencialmente, enquanto as operações em *streams* diferentes podem ser executadas em paralelo. No contexto do **overlapping computation and communication**, são criados dois *streams*: `stream0` e `stream1` [^17, ^19]. O *stream* `stream0` é usado para os cálculos iniciais dos valores de contorno e para as operações de cópia de dados [^17, ^22]. O *stream* `stream1` é usado para o cálculo dos pontos internos restantes [^17, ^22]. Isso permite que os cálculos e as cópias de dados ocorram em paralelo, maximizando a utilização dos recursos [^19].

**Exemplo de implementação (simplificado):**

```c++
cudaStream_t stream0, stream1;
cudaStreamCreate(&stream0);
cudaStreamCreate(&stream1);

// Etapa 1: Calcular os valores de contorno (em stream0)
launch_kernel_boundary(d_output + left_stage1_offset, d_input + left_stage1_offset, dimx, dimy, 12, stream0);
launch_kernel_boundary(d_output + right_stage1_offset, d_input + right_stage1_offset, dimx, dimy, 12, stream0);

// Etapa 2: Comunicar os valores de contorno e calcular os pontos internos restantes
// Copiar os valores de contorno para a memória do host (em stream0)
cudaMemcpyAsync(h_left_boundary, d_output + num_halo_points, num_halo_bytes, cudaMemcpyDeviceToHost, stream0);
cudaMemcpyAsync(h_right_boundary, d_output + right_stage1_offset + num_halo_points, num_halo_bytes, cudaMemcpyDeviceToHost, stream0);

// Calcular os pontos internos restantes (em stream1)
launch_kernel_internal(d_output + stage2_offset, d_input + stage2_offset, dimx, dimy, dimz, stream1);

// Sincronizar stream0 antes de trocar dados
cudaStreamSynchronize(stream0);

// Trocar dados com os vizinhos
MPI_Sendrecv(h_left_boundary, num_halo_points, MPI_FLOAT, left_neighbor, i, h_right_halo, num_halo_points, MPI_FLOAT, right_neighbor, i, MPI_COMM_WORLD, &status);
MPI_Sendrecv(h_right_boundary, num_halo_points, MPI_FLOAT, right_neighbor, i, h_left_halo, num_halo_points, MPI_FLOAT, left_neighbor, i, MPI_COMM_WORLD, &status);

// Copiar os novos halos para a memória do dispositivo (em stream0)
cudaMemcpyAsync(d_output+left_halo_offset, h_left_halo, num_halo_bytes, cudaMemcpyHostToDevice, stream0);
cudaMemcpyAsync(d_output+right_ghost_offset, h_right_ghost, num_halo_bytes, cudaMemcpyHostToDevice, stream0);

// Sincronizar todos os streams
cudaDeviceSynchronize();
```

### Conclusão
A segunda etapa do **overlapping computation and communication** [^16] é crucial para maximizar a utilização dos recursos de computação e comunicação em sistemas paralelos. Ao executar a comunicação dos valores de contorno em paralelo com o cálculo dos dados restantes, o tempo de inatividade do sistema é reduzido, levando a um melhor desempenho geral. A utilização de *pinned memory allocation* e *streams* em CUDA é fundamental para permitir essa sobreposição eficiente [^16, ^17, ^19].

### Referências
[^15]: Programming a Heterogeneous Computing Cluster, Chapter 19, Overlapping Computation and Communication, Page 421
[^16]: Programming a Heterogeneous Computing Cluster, Chapter 19, Overlapping Computation and Communication, Page 422
[^17]: Programming a Heterogeneous Computing Cluster, Chapter 19, Overlapping Computation and Communication, Page 423
[^18]: Programming a Heterogeneous Computing Cluster, Chapter 19, Overlapping Computation and Communication, Page 424
[^19]: Programming a Heterogeneous Computing Cluster, Chapter 19, Overlapping Computation and Communication, Page 425
[^22]: Programming a Heterogeneous Computing Cluster, Chapter 19, Overlapping Computation and Communication, Page 426
<!-- END -->