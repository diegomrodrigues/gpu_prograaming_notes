## Pinned Memory and Streams for Overlapping Computation and Communication

### Introdução
Em continuidade ao conceito de *overlapping computation and communication*, para otimizar o desempenho de aplicações heterogêneas em clusters, é crucial maximizar a utilização tanto da rede de comunicação quanto do hardware de computação [^421]. Uma estratégia eficaz envolve dividir as tarefas de computação de cada processo em dois estágios [^421]. O primeiro estágio calcula as fatias de contorno que serão usadas como células de halo pelos seus vizinhos na próxima iteração [^421]. O segundo estágio envolve a comunicação desses novos valores de contorno aos processos vizinhos, enquanto simultaneamente calcula o restante dos dados na partição [^422]. Para suportar essas atividades paralelas no segundo estágio, o CUDA oferece dois recursos avançados: alocação de memória *pinned* e *streams* [^422].

### Conceitos Fundamentais

#### Pinned Memory Allocation

Em sistemas computacionais modernos, o sistema operacional gerencia um espaço de memória virtual para as aplicações, onde cada aplicação tem acesso a um grande espaço de endereçamento consecutivo [^423]. No entanto, a quantidade de memória física é limitada e deve ser compartilhada entre todas as aplicações em execução. Para gerenciar essa memória limitada, o sistema operacional divide o espaço de memória virtual em páginas e mapeia apenas as páginas ativamente usadas na memória física [^423]. Em situações de alta demanda por memória, o sistema operacional pode precisar mover páginas da memória física para o armazenamento em massa, como discos, em um processo conhecido como "paginação" ("*page out*") [^423].

A implementação de `cudaMemcpy()` utiliza um dispositivo de acesso direto à memória (*Direct Memory Access - DMA*) para copiar dados entre a memória do host e a memória do dispositivo [^424]. O hardware DMA opera em endereços físicos [^424]. Existe a possibilidade de que os dados sejam paginados para fora da memória antes que a operação DMA seja concluída. Os locais de memória física para os dados podem ser reatribuídos a outros dados de memória virtual. A operação DMA pode ser corrompida, pois seus dados podem ser sobrescritos pela atividade de paginação.

Uma solução comum para esse problema de corrupção é que o tempo de execução CUDA execute a operação de cópia em duas etapas [^424]. Para uma cópia do *host-to-device*, o tempo de execução CUDA primeiro copia os dados da memória do *host* de origem em um *buffer* de memória "fixado", o que significa que os locais de memória são marcados para que o mecanismo de paginação operacional não pagine os dados para fora. Em seguida, ele usa o dispositivo DMA para copiar os dados do *buffer* de memória *pinned* para a memória do dispositivo. Para uma cópia *device-to-host*, o tempo de execução CUDA primeiro usa um dispositivo DMA para copiar os dados da memória do dispositivo em um *buffer* de memória *pinned*. Em seguida, ele copia os dados da memória *pinned* para o local da memória do *host* de destino. Ao usar um *buffer* de memória *pinned* extra, a cópia DMA estará protegida contra qualquer atividade de paginação.

Existem dois problemas com essa abordagem [^424]. Um é que a cópia extra adiciona atraso à operação `cudaMemcpy()`. O segundo é que a complexidade extra envolvida leva a uma implementação síncrona da função `cudaMemcpy()`. Ou seja, o programa *host* não pode continuar a ser executado até que a função `cudaMemcpy()` conclua sua operação e retorne. Isso serializa todas as operações de cópia. Para suportar cópias rápidas com mais paralelismo, o CUDA fornece uma função `cudaMemcpyAsync()` [^424].

Para usar a função `cudaMemcpyAsync()`, o *buffer* de memória do *host* deve ser alocado como um *buffer* de memória *pinned* [^424]. Isso é feito nas linhas 19 a 22 para os *buffers* de memória do *host* das fatias de contorno esquerdo, contorno direito, halo esquerdo e halo direito [^423]. Esses *buffers* são alocados com uma função especial `cudaHostAlloc()`, que garante que a memória alocada seja *pinned* ou bloqueada por página de atividades de paginação [^424]. A função `cudaHostAlloc()` usa três parâmetros. Os dois primeiros são os mesmos que `cudaMalloc()` [^424]. O terceiro especifica algumas opções para uso mais avançado. Para a maioria dos casos de uso básico, podemos simplesmente usar o valor padrão `cudaHostAllocDefault` [^424].

#### Streams

O segundo recurso avançado do CUDA são os *streams*, que suportam a execução simultânea gerenciada de funções da API CUDA [^425]. Um *stream* é uma sequência ordenada de operações. Quando um código *host* chama uma função `cudaMemcpyAsync()` ou inicia um *kernel*, ele pode especificar um *stream* como um de seus parâmetros [^425]. Todas as operações no mesmo *stream* serão feitas sequencialmente. Operações de dois *streams* diferentes podem ser executadas em paralelo [^425].

A linha 23 da Figura 19.13 declara duas variáveis do tipo CUDA `cudaStream_t` [^423, 425]. Essas variáveis são usadas ao chamar a função `cudaStreamCreate()` [^425]. Cada chamada para `cudaStreamCreate()` cria um novo *stream* e deposita um ponteiro para o *stream* em seu parâmetro [^425]. Após as chamadas nas linhas 24 e 25, o código *host* pode usar `stream0` ou `stream1` em chamadas subsequentes `cudaMemcpyAsync()` e lançamentos de *kernel* [^425].

### Conclusão

A utilização de memória *pinned*, alocada através de `cudaHostAlloc()`, garante que a memória alocada não seja paginada pelo sistema operacional, permitindo transferências DMA mais rápidas e seguras entre a memória do *host* e do dispositivo [^423, 424]. Adicionalmente, os *streams* permitem a execução simultânea de operações CUDA, como cópias de memória assíncronas (`cudaMemcpyAsync()`) e lançamentos de *kernel*, possibilitando sobrepor a computação e a comunicação para melhorar o desempenho geral [^425]. A combinação desses dois recursos avançados do CUDA é essencial para implementar eficazmente o *overlapping computation and communication* em aplicações heterogêneas [^422].

### Referências
[^421]: Capítulo 19, Seção 19.5: "OVERLAPPING COMPUTATION AND COMMUNICATION"
[^422]: Capítulo 19, Seção 19.5: "To support the parallel activities in stage 2, we need to use two advanced features of the CUDA programming model: pinned memory allocation and streams."
[^423]: Capítulo 19, Seção 19.5: Linhas 19-25 e parágrafos subsequentes sobre memória virtual e alocação de memória pinned.
[^424]: Capítulo 19, Seção 19.5: Explicação do uso de DMA e memória pinned para cópias entre host e device.
[^425]: Capítulo 19, Seção 19.5: Explicação do uso de streams para execução concorrente de operações CUDA.
<!-- END -->