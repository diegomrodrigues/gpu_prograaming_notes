## Synchronization for Overlapping Computation and Communication in Heterogeneous Clusters

### Introdução
Em um ambiente de computação heterogêneo, onde clusters HPC (High-Performance Computing) combinam CPUs e GPUs, a sobreposição de computação e comunicação é fundamental para otimizar o desempenho. Para garantir a correta execução e troca de dados entre os nós de computação, mecanismos de sincronização são necessários. Este capítulo detalha o uso das funções `MPI_Barrier()` e `cudaDeviceSynchronize()` no contexto da sobreposição de computação e comunicação. Este capítulo se baseia nos conceitos de programação MPI/CUDA conjunta introduzidos anteriormente, focando em como essas funções são empregadas para coordenar processos em um cluster.

### Conceitos Fundamentais
Para alcançar melhor desempenho em sistemas heterogêneos, é crucial sobrepor a computação e a comunicação [^421]. Uma abordagem simples envolve cada processo computacional realizar um passo computacional completo em sua partição, seguido pela troca de dados *halo* com os vizinhos esquerdo e direito, e repetindo este processo. No entanto, esta estratégia não é eficiente porque força o sistema a operar em dois modos distintos: computação ou comunicação [^421].

Uma estratégia mais eficaz é dividir as tarefas de computação de cada processo em dois estágios [^421]. No primeiro estágio, cada processo computacional calcula suas *boundary slices*, que serão necessárias como *halo cells* por seus vizinhos na próxima iteração. No segundo estágio, cada processo executa duas atividades em paralelo: comunicar seus novos valores de *boundary* aos processos vizinhos e calcular o restante dos dados em sua partição [^422].

Para suportar essas atividades paralelas, é necessário empregar recursos avançados de CUDA, como **pinned memory allocation** e **streams** [^422].

**MPI_Barrier()**:\nA função `MPI_Barrier()` é uma função de sincronização coletiva, o que significa que envolve um grupo de processos MPI [^426]. Todos os processos especificados pelo comunicador (geralmente `MPI_COMM_WORLD`) devem alcançar a barreira antes que qualquer um deles possa prosseguir [^426].

> A função `MPI_Barrier()` garante que todos os nós de computação recebam seus dados de entrada e estejam prontos para iniciar as etapas de computação simultaneamente [^0].

No código de exemplo, `MPI_Barrier(MPI_COMM_WORLD)` é usado para garantir que todos os nós de computação tenham recebido seus dados de entrada e estejam prontos para iniciar os passos computacionais [^34]. Sem essa barreira, alguns nós podem começar a computar antes que outros tenham recebido seus dados, levando a resultados incorretos ou a condições de corrida [^426].

**cudaDeviceSynchronize()**:\nA função `cudaDeviceSynchronize()` é uma função de sincronização CUDA que aguarda a conclusão de todas as operações na GPU, incluindo kernels e cópias de memória [^0]. Essa função garante que todas as operações assíncronas iniciadas na GPU sejam concluídas antes que o programa continue [^0].

> `cudaDeviceSynchronize()` garante que todas as operações na GPU (kernels e cópias de memória) sejam concluídas antes de prosseguir com a execução do programa [^0].

Após as mensagens MPI terem sido enviadas e recebidas, as linhas 44 e 45 do código transferem os *halo points* recém-recebidos para o buffer `d_output` da memória do dispositivo [^428]. Essas cópias são feitas em `stream0` para que executem em paralelo com o kernel lançado na linha 38 [^428].

A linha 46, `cudaDeviceSynchronize()`, é uma operação de sincronização para todas as atividades do dispositivo. Esta chamada força o processo a esperar por todas as atividades do dispositivo, incluindo kernels e cópias de dados, para completar [^428]. Quando a função `cudaDeviceSynchronize()` retorna, todos os dados `d_output` do passo de computação corrente estão no lugar: dados *halo* esquerdos do processo vizinho esquerdo, dados *boundary* do kernel lançado na linha 36, dados internos do kernel lançado na linha 38, dados *boundary* direitos do kernel lançado na linha 37, e dados *halo* direitos do vizinho direito [^428].

**Pinned Memory Allocation**:\nA alocação de memória *pinned* (fixada) garante que a memória alocada não seja paginada pelo sistema operacional [^423]. Isso é crucial para transferências de dados eficientes entre a CPU (host) e a GPU (device) usando DMA (Direct Memory Access) [^424]. Sem memória *pinned*, o sistema operacional pode paginar os dados, levando a transferências mais lentas e possíveis corrupções de dados [^424].

A função `cudaHostAlloc()` é usada para alocar memória *pinned* [^423]. No código de exemplo, a memória para os *boundary slices* esquerdo e direito, e os *halo slices* esquerdo e direito, são alocados como memória *pinned* [^19-22].

**Streams**:\nOs *streams* em CUDA permitem a execução concorrente de funções da API CUDA [^425]. As operações dentro do mesmo *stream* são executadas sequencialmente, enquanto operações de *streams* diferentes podem ser executadas em paralelo [^425]. Isso permite sobrepor a computação e a comunicação, melhorando o desempenho [^425].

No código de exemplo, dois *streams* são criados: `stream0` e `stream1` [^23-25]. O kernel para computar os valores *boundary* é lançado em `stream0`, enquanto o kernel para computar os pontos restantes é lançado em `stream1` [^36-38]. Isso permite que os dois kernels sejam executados em paralelo.

### Conclusão
A sobreposição de computação e comunicação é uma técnica essencial para otimizar o desempenho em clusters heterogêneos. O uso de `MPI_Barrier()` garante que todos os processos estejam sincronizados antes de iniciar a computação, enquanto `cudaDeviceSynchronize()` garante que todas as operações da GPU sejam concluídas antes de prosseguir [^0]. Além disso, o emprego de memória *pinned* e *streams* em CUDA permite a execução concorrente de operações e a transferência eficiente de dados entre a CPU e a GPU [^425]. Ao combinar essas técnicas, é possível alcançar um desempenho significativamente melhorado em aplicações de computação heterogênea.

### Referências
[^0]: The MPI_Barrier() synchronization is used to ensure that all computing nodes receive their input data and are ready to start the computation steps simultaneously. The `cudaDeviceSynchronize()` function ensures that all operations on the GPU (kernels and memory copies) are completed before proceeding with the execution of the program.
[^421]: A simple way to perform the computation steps is for each compute process to perform a computation step on its entire partition, exchange halo data with the left and right neighbors, and repeat. While this is a very simple strategy, it is not very effective. The reason is that this strategy forces the system to be in one of the two modes. In the first mode, all compute processes are performing computation steps. During this time, the communication network is not used. In the second mode, all compute processes exchange halo data with their left and right neighbors. During this time, the computation hardware is not well utilized. Ideally, we would like to achieve better performance by utilizing both the communication network and computation hardware all the time. This can be achieved by dividing the computation tasks of each compute process into two stages, as illustrated in Figure 19.13.
[^422]: During the second stage (stage 2), each compute process performs two parallel activities. The first is to communicate its new boundary values to its neighbor processes. This is done by first copying the data from the device memory into the host memory, followed by sending MPI messages to the neighbors. As we will discuss later, we need to be careful that the data received from the neighbors is used in the next iteration, not the current iteration. The second activity is to calculate the rest of the data in the partition. If the communication activity takes a shorter amount of time than the calculation activity, we can hide the communication delay and fully utilize the computing hardware all the time. This is usually achieved by having enough slices in the internal part of each partition allow each compute process to perform computation steps in between communications.
[^423]: Note that the host memory allocation is done with the cudaHostAlloc() function rather than the standard malloc() function. The difference is that the cudaHostAlloc() function allocates a pinned memory buffer, some-times also referred to as page-locked memory buffer. We need to present a little more background on the memory management in operating systems to fully understand the concept of pinned memory buffers.
[^424]: The implementation of cudaMemcpy() uses a type of hardware called a direct memory access (DMA) device. When a cudaMemcpy() function is called to copy between the host and device memories, its implementation uses a DMA device to complete the task. On the host memory side, the DMA hardware operates on physical addresses. That is, the operating sys-tem needs to give a translated physical address to the DMA device. However, there is a chance that the data may be paged out before the DMA operation is complete. The physical memory locations for the data may be reassigned to another virtual memory data. In this case, the DMA operation can be potentially corrupted since its data can be overwritten by the paging activity.
[^425]: The second advanced CUDA feature is streams, which supports the man-aged concurrent execution of CUDA API functions. A stream is an ordered sequence of operations. When a host code calls a cudaMemcpyAsync() func-tion or launches a kernel, it can specify a stream as one of its parameters. All operations in the same stream will be done sequentially. Operations from two different streams can be executed in parallel.
[^426]: Line 35 is an MPI barrier synchronization, which is similar to the CUDA_syncthreads(). An MPI barrier forces all MPI processes specified by the parameter to wait for each other. None of the processes can con-tinue their execution beyond this point until everyone has reached this point. The reason why we want a barrier synchronization here is to make sure that all compute nodes have received their input data and are ready to perform the computation steps.

<!-- END -->