## MPI Collective Communication
### Introdução
Em continuidade ao que foi discutido sobre a comunicação *point-to-point* em MPI [^414], este capítulo se aprofundará no conceito de **MPI Collective Communication**, que envolve um grupo de processos MPI executando uma operação de comunicação em conjunto [^431]. Exploraremos o *MPI_Barrier* e outras funções coletivas comumente usadas, destacando sua importância e otimizações [^431].

### Conceitos Fundamentais

**Comunicação Coletiva** em MPI refere-se a operações que envolvem um grupo de processos MPI trabalhando juntos para realizar uma tarefa de comunicação [^431]. Ao contrário da comunicação *point-to-point*, que envolve apenas dois processos (um remetente e um receptor), a comunicação coletiva envolve múltiplos processos, tornando-a uma ferramenta poderosa para tarefas como distribuição de dados, agregação de resultados e sincronização [^414].

Um exemplo fundamental de comunicação coletiva é o **MPI_Barrier()** [^431]. Essa função atua como um ponto de sincronização. Todos os processos MPI que participam do comunicador especificado devem alcançar a barreira antes que qualquer um deles possa prosseguir [^431].  Como visto no exemplo de *stencil*, as barreiras são usadas para garantir que todos os processos MPI estejam prontos antes de interagirem uns com os outros [^431].

Outras formas comuns de comunicação coletiva incluem:

*   **Broadcast:** Envio de dados de um processo (o *root*) para todos os outros processos no comunicador.
*   **Reduction:** Combinação de dados de todos os processos em um comunicador em um único valor, que é então retornado a um processo (o *root*) ou a todos os processos.  Exemplos incluem soma, produto, máximo e mínimo.
*   **Gather:** Coleta de dados de todos os processos em um comunicador em um único processo (o *root*).
*   **Scatter:** Distribuição de dados de um processo (o *root*) para todos os outros processos no comunicador.

A função *MPI_Sendrecv()* [^428] é essencialmente uma combinação de *MPI_Send()* e *MPI_Recv()* [^428].

A linha 35 do compute process code inicia um loop que executa as etapas de cálculo. Em cada iteração, cada processo de computação executa um ciclo do processo de dois estágios na Figura 19.12 [^426].

A linha 36 chama uma função que executará quatro etapas de computação para gerar as quatro fatias dos pontos de limite esquerdo no estágio 1. Assumimos que existe um kernel que executa uma etapa de computação em uma região de pontos de aderência. A função launch_kernel() recebe vários parâmetros. O primeiro parâmetro é um ponteiro para a área de dados de saída para o kernel. O segundo parâmetro é um ponteiro para a área de dados de entrada. Em ambos os casos, adicionamos o left_stage1_offset aos dados de entrada e saída na memória do dispositivo. Os três parâmetros seguintes especificam as dimensões da porção da grade a ser processada, que é de 12 fatias neste caso [^426].

### Conclusão

As funções de comunicação coletiva são otimizadas pelos desenvolvedores do *runtime* MPI e fornecedores de sistemas [^431]. Usar essas funções geralmente leva a um melhor desempenho, bem como a uma melhor legibilidade e produtividade [^431].

### Referências
[^414]: MPI point-to-point communication types.
[^426]: Compute process code (part 3).
[^428]: The MPI_Sendrecv() function is essentially a combination of MPI_Send() and MPI_Recv().
[^431]: MPI collective communication involves a group of MPI processes performing a communication operation together.

<!-- END -->