## MPI Basics

### Introdução
Em ambientes de computação de alto desempenho (HPC), muitas aplicações exigem o poder computacional agregado de um *cluster* de nós computacionais [^1]. Historicamente, esses *clusters* têm sido programados predominantemente com a *Message Passing Interface* (MPI) [^1]. Este capítulo apresenta os conceitos-chave de MPI necessários para dimensionar aplicações heterogêneas para múltiplos nós em um ambiente de *cluster* [^1]. Em particular, o foco estará no particionamento de domínio, comunicação ponto a ponto e comunicação coletiva no contexto de dimensionamento de um *kernel* CUDA em múltiplos nós [^1].

### Conceitos Fundamentais

**MPI (Message Passing Interface)** é um conjunto de funções de API para comunicação entre processos em execução em um *cluster* de computação [^2]. MPI assume um modelo de memória distribuída onde os processos trocam informações enviando mensagens uns aos outros [^2]. Quando uma aplicação usa funções de comunicação da API, ela não precisa lidar com os detalhes da rede de interconexão [^2]. A implementação MPI permite que os processos se enderecem usando números lógicos, de maneira semelhante ao uso de números de telefone em um sistema telefônico [^2].

Em uma aplicação MPI típica, os dados e o trabalho são particionados entre os processos [^2]. Cada nó pode conter um ou mais processos [^2]. À medida que esses processos progridem, eles podem precisar de dados uns dos outros, o que é satisfeito pelo envio e recebimento de mensagens [^2]. Em alguns casos, os processos também precisam sincronizar entre si e gerar resultados coletivos ao colaborar em uma grande tarefa, o que é feito com funções de API de comunicação coletiva [^2].

Como CUDA, os programas MPI são baseados no modelo de execução paralela SPMD (single program, multiple data) [^3]. Todos os processos MPI executam o mesmo programa [^3]. O sistema MPI fornece um conjunto de funções de API para estabelecer sistemas de comunicação que permitem que os processos se comuniquem entre si [^3].

Cinco funções de API essenciais configuram e desmontam sistemas de comunicação para uma aplicação MPI [^3]:
*   `MPI_Init(int *argc, char ***argv)`: Inicializa o MPI [^6].
*   `MPI_Comm_rank(MPI_Comm comm, int *rank)`: Retorna a classificação do processo de chamada no grupo de `comm` [^6].
*   `MPI_Comm_size(MPI_Comm comm, int *size)`: Retorna o número de processos no grupo de `comm` [^6].
*   `MPI_Comm_abort(MPI_Comm comm, int errorcode)`: Termina a conexão de comunicação MPI com um código de erro [^6].
*   `MPI_Finalize()`: Termina uma aplicação MPI, fechando todos os recursos [^6].

Cada processo começa inicializando o *runtime* MPI com uma chamada `MPI_Init()` [^3]. Isso inicializa o sistema de comunicação para todos os processos em execução na aplicação [^3]. Uma vez que o *runtime* MPI é inicializado, cada processo chama duas funções para se preparar para a comunicação [^3]. A primeira função é `MPI_Comm_rank()`, que retorna um número único para cada processo de chamada, chamado de *rank* MPI ou ID de processo [^3]. Os números recebidos pelos processos variam de 0 ao número de processos menos 1 [^3]. O *rank* MPI para um processo é equivalente à expressão `blockIdx.x * blockDim.x + threadIdx.x` para um *thread* CUDA [^3]. Ele identifica exclusivamente o processo em uma comunicação, semelhante ao número de telefone em um sistema telefônico [^3].

A função `MPI_Comm_rank()` recebe dois parâmetros [^3]. O primeiro é um tipo MPI interno `MPI_Comm` que especifica o escopo da solicitação [^3]. Os valores de `MPI_Comm` são comumente referidos como um *communicator* [^3]. `MPI_Comm` e outros tipos internos do MPI são definidos em um arquivo de cabeçalho `mpi.h` que deve ser incluído em todos os arquivos de programa C que usam MPI [^3]. Isso é semelhante ao arquivo de cabeçalho `cuda.h` para programas CUDA [^3]. Uma aplicação MPI pode criar um ou mais *intracomunicadores* [^3]. Membros de cada *intracomunicador* são processos MPI [^3]. `MPI_Comm_rank()` atribui um ID exclusivo a cada processo em um *intracomunicador* [^3]. O valor do parâmetro passado é `MPI_COMM_WORLD`, o que significa que o *intracomunicador* inclui todos os processos MPI em execução na aplicação [^3].

O segundo parâmetro para a função `MPI_Comm_rank()` é um ponteiro para uma variável inteira na qual a função depositará o valor do *rank* retornado [^3]. Uma variável `pid` é declarada para esse propósito [^3]. Depois que `MPI_Comm_rank()` retorna, a variável `pid` conterá o ID exclusivo para o processo de chamada [^3].

A segunda função de API é `MPI_Comm_size()`, que retorna o número total de processos MPI em execução no *intracomunicador* [^6]. A função `MPI_Comm_size()` recebe dois parâmetros [^6]. O primeiro é um tipo MPI interno `MPI_Comm` que fornece o escopo da solicitação [^6]. O escopo é `MPI_COMM_WORLD`, e o valor retornado é o número de processos MPI em execução na aplicação [^7]. Isso é solicitado por um usuário quando a aplicação é submetida usando o comando `mpirun` ou o comando `mpiexec` [^7]. No entanto, o usuário pode não ter solicitado um número suficiente de processos, ou o sistema pode não ser capaz de criar todos os processos solicitados [^7]. Portanto, é uma boa prática para um programa de aplicação MPI verificar o número real de processos em execução [^7].

O segundo parâmetro é um ponteiro para uma variável inteira na qual a função `MPI_Comm_size()` depositará o valor de retorno [^7]. Uma variável `np` é declarada para esse propósito [^7]. Depois que a função retorna, a variável `np` contém o número de processos MPI em execução na aplicação [^7]. Se a aplicação exigir pelo menos três processos MPI, ela verifica se o número de processos é pelo menos três [^7]. Se não for, ela chama a função `MPI_Comm_abort()` para encerrar as conexões de comunicação e retornar com um valor de *flag* de erro de 1 [^7].

A função `MPI_Comm_abort()` recebe dois parâmetros [^7]. O primeiro é o escopo da solicitação [^7]. O escopo é todos os processos MPI em execução na aplicação [^7]. O segundo parâmetro é um código para o tipo de erro que causou a interrupção [^7]. Qualquer número diferente de 0 indica que ocorreu um erro [^7].

Se o número de processos atender ao requisito, o programa de aplicação prossegue para realizar o cálculo [^7]. A aplicação usa processos `np-1` (pid de 0 a `np-2`) para realizar o cálculo e um processo (o último dos quais o pid é `np-1`) para realizar um serviço de entrada/saída (I/O) para os outros processos [^7]. O processo que realiza os serviços de I/O é o *data server* e os processos que realizam o cálculo são os *compute processes* [^7]. Se o `pid` de um processo estiver dentro do intervalo de 0 a `np-2`, é um *compute process* e chama a função `compute_process()` [^7]. Se o `pid` do processo for `np-1`, é o *data server* e chama a função `data_server()` [^7].

Depois que a aplicação completa sua computação, ela notifica o *runtime* MPI com uma chamada para `MPI_Finalize()`, que libera todos os recursos de comunicação MPI alocados para a aplicação [^7]. A aplicação pode então sair com um valor de retorno 0, o que indica que nenhum erro ocorreu [^7].

### Conclusão
Este capítulo forneceu uma introdução aos conceitos básicos do MPI, essenciais para programar *clusters* heterogêneos e dimensionar aplicações para múltiplos nós [^1]. Os conceitos de inicialização do MPI, determinação do *rank* e tamanho, e finalização adequada foram discutidos [^3]. A estrutura básica de um programa MPI, incluindo a divisão de tarefas entre *data servers* e *compute processes*, foi elucidada [^7]. Estes fundamentos estabelecem a base para explorar tópicos mais avançados em programação MPI, como comunicação ponto a ponto, comunicação coletiva e técnicas de sobreposição [^1].

### Referências
[^1]: Capítulo 19, página 407
[^2]: Capítulo 19, página 408
[^3]: Capítulo 19, página 411
[^6]: Capítulo 19, página 412
[^7]: Capítulo 19, página 413
<!-- END -->