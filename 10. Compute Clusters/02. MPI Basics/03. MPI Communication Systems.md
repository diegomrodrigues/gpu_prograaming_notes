## Establishing Communication Systems with MPI

### Introdução

Em ambientes de *High-Performance Computing* (HPC), muitas aplicações requerem o poder computacional agregado de um cluster de nós computacionais [^1]. Historicamente, esses clusters têm sido programados predominantemente com a *Message Passing Interface* (MPI) [^1]. Este capítulo apresenta os conceitos chave de MPI necessários para programar aplicações heterogêneas em múltiplos nós em um ambiente de cluster [^1]. Especificamente, este capítulo foca no particionamento de domínio, comunicação *point-to-point* e comunicação coletiva no contexto de escalar um kernel CUDA para múltiplos nós [^1]. Esta seção detalha como o sistema MPI fornece um conjunto de funções API para estabelecer sistemas de comunicação que permitem que processos se comuniquem entre si.

### Conceitos Fundamentais

O sistema MPI oferece um conjunto de funções API para estabelecer sistemas de comunicação que permitem que os processos se comuniquem uns com os outros [^5]. A Figura 19.5 [^5, 6] mostra cinco funções API essenciais que configuram e desfazem os sistemas de comunicação para uma aplicação MPI. Um usuário precisa fornecer o arquivo executável do programa para o comando `mpirun` ou o comando `mpiexec` em um cluster [^5].

Cada processo começa inicializando o *runtime* MPI com uma chamada `MPI_Init()` [^5]. Isso inicializa o sistema de comunicação para todos os processos que executam a aplicação [^5]. Uma vez que o *runtime* MPI é inicializado, cada processo chama duas funções para se preparar para a comunicação [^5].

A primeira função é `MPI_Comm_rank()`, que retorna um número único para cada processo que chama, chamado de *MPI rank* ou *process ID* [^5]. Os números recebidos pelos processos variam de 0 ao número de processos menos 1 [^5]. O *MPI rank* para um processo é equivalente à expressão `blockIdx.x*blockDim.x + threadIdx.x` para um *thread* CUDA [^5]. Ele identifica exclusivamente o processo em uma comunicação, semelhante ao número de telefone em um sistema telefônico [^5].

A função `MPI_Comm_rank()` recebe dois parâmetros [^5]. O primeiro é um tipo MPI *built-in* `MPI_Comm` que especifica o escopo da requisição [^5]. Os valores de `MPI_Comm` são comumente referidos como um **comunicador** [^5]. `MPI_Comm` e outros tipos *built-in* MPI são definidos em um arquivo de *header* `mpi.h` que deve ser incluído em todos os arquivos de programa C que usam MPI [^5]. Isso é similar ao arquivo de *header* `cuda.h` para programas CUDA [^5]. Uma aplicação MPI pode criar um ou mais *intracommunicators* [^5]. Membros de cada *intracommunicator* são processos MPI [^5]. `MPI_Comm_rank()` atribui um ID único para cada processo em um *intracommunicator* [^5]. Na Figura 19.6 [^5, 6], o valor do parâmetro passado é `MPI_COMM_WORLD`, o que significa que o *intracommunicator* inclui todos os processos MPI que executam a aplicação [^5].

O segundo parâmetro para a função `MPI_Comm_rank()` é um ponteiro para uma variável inteira na qual a função depositará o valor de *rank* retornado [^5]. Na Figura 19.6 [^5, 6], uma variável `pid` é declarada para este propósito [^5]. Após o retorno de `MPI_Comm_rank()`, a variável `pid` conterá o ID único para o processo que chama [^5].

A segunda função API é `MPI_Comm_size()`, que retorna o número total de processos MPI em execução no *intracommunicador* [^6]. A função `MPI_Comm_size()` recebe dois parâmetros [^6]. O primeiro é um tipo *built-in* MPI `MPI_Comm` que fornece o escopo da requisição [^7]. Na Figura 19.6 [^5, 6], o escopo é `MPI_COMM_WORLD` [^7]. Como usamos `MPI_COMM_WORLD`, o valor retornado é o número de processos MPI executando a aplicação [^7]. Isso é solicitado por um usuário quando a aplicação é submetida usando o comando `mpirun` ou o comando `mpiexec` [^7]. No entanto, o usuário pode não ter solicitado um número suficiente de processos [^7]. Além disso, o sistema pode ou não ser capaz de criar todos os processos solicitados [^7]. Portanto, é uma boa prática para um programa de aplicação MPI verificar o número real de processos em execução [^7].

O segundo parâmetro é um ponteiro para uma variável inteira na qual a função `MPI_Comm_size()` depositará o valor de retorno [^7]. Na Figura 19.6 [^5, 6], uma variável `np` é declarada para este propósito [^7]. Após o retorno da função, a variável `np` contém o número de processos MPI executando a aplicação [^7]. Na Figura 19.6 [^5, 6], assumimos que a aplicação requer pelo menos três processos MPI [^7]. Portanto, ela verifica se o número de processos é pelo menos três [^7]. Caso contrário, ela chama a função `MPI_Comm_abort()` para encerrar as conexões de comunicação e retornar com um valor de *flag* de erro de 1 [^7].

A Figura 19.6 [^5, 6] também mostra um padrão comum para reportar erros ou outras tarefas [^7]. Existem múltiplos processos MPI, mas precisamos reportar o erro apenas uma vez [^7]. O código da aplicação designa o processo com `pid = 0` para fazer o relatório [^7].

Como mostrado na Figura 19.5 [^5, 6], a função `MPI_Comm_abort()` recebe dois parâmetros [^7]. O primeiro é o escopo da requisição [^7]. Na Figura 19.6 [^5, 6], o escopo é todos os processos MPI executando a aplicação [^7]. O segundo parâmetro é um código para o tipo de erro que causou o aborto [^7]. Qualquer número diferente de 0 indica que ocorreu um erro [^7].

Se o número de processos satisfaz o requisito, o programa da aplicação continua a executar o cálculo [^7]. Na Figura 19.6 [^5, 6], a aplicação usa `np-1` processos (`pid` de 0 a `np-2`) para executar o cálculo e um processo (o último dos quais o `pid` é `np-1`) para executar um serviço de entrada/saída (I/O) para os outros processos [^7]. Nos referiremos ao processo que executa os serviços de I/O como o *data server* e os processos que executam o cálculo como *compute processes* [^7]. Na Figura 19.6 [^5, 6], se o `pid` de um processo estiver dentro do intervalo de 0 a `np-2`, ele é um *compute process* e chama a função `compute_process()` [^7]. Se o processo `pid` for `np-1`, ele é o *data server* e chama a função `data_server()` [^7].

Após a aplicação completar sua computação, ela notifica o *runtime* MPI com uma chamada para `MPI_Finalize()`, que libera todos os recursos de comunicação MPI alocados para a aplicação [^7]. A aplicação pode então sair com um valor de retorno 0, que indica que nenhum erro ocorreu [^7].

### Conclusão

O sistema MPI fornece um conjunto robusto de funções API que permite a criação de sistemas de comunicação complexos entre processos. Através da inicialização adequada, determinação do *rank* e tamanho do comunicador, e finalização do ambiente MPI, os desenvolvedores podem estruturar aplicações paralelas eficientes em *clusters* de computação [^5, 6, 7].

### Referências
[^1]: Capítulo 19, página 407
[^2]: Capítulo 19, página 408
[^3]: Capítulo 19, página 409
[^4]: Capítulo 19, página 410
[^5]: Capítulo 19, página 411
[^6]: Capítulo 19, página 412
[^7]: Capítulo 19, página 413
<!-- END -->