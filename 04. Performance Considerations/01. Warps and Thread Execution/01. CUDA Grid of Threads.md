## CUDA Kernels: Hierarquia de Threads e Execução em Warps

### Introdução

Em CUDA, a execução de um kernel é fundamentalmente paralela, orquestrada através de uma estrutura hierárquica de threads [^1]. Esta hierarquia é dividida em dois níveis: a *grid* de blocos e os blocos contendo *arrays* de threads. Compreender esta organização é crucial para otimizar o desempenho em aplicações CUDA. Este capítulo explora em detalhes a estrutura dos kernels CUDA, a organização das threads em blocos e grids, e as implicações para a escalabilidade e execução paralela.

### Conceitos Fundamentais

#### Hierarquia de Threads

Um **kernel CUDA** é executado como uma *grid* de blocos [^1]. Cada bloco, por sua vez, contém um *array* de threads. Essa hierarquia bidimensional (grid de blocos e blocos de threads) permite uma organização flexível e escalável da computação paralela. A dimensionalidade da grid e dos blocos pode ser configurada como 1D, 2D ou 3D, adaptando-se às características do problema a ser resolvido [^1].

**Grid:** Representa o conjunto total de blocos que serão executados pelo kernel. Os blocos dentro da grid são independentes entre si, o que permite que sejam executados em qualquer ordem. Essa independência é fundamental para a escalabilidade, pois o *scheduler* do GPU pode alocar os blocos para diferentes núcleos de processamento conforme a disponibilidade [^1].

**Bloco:** Contém um conjunto de threads que podem cooperar entre si, compartilhando memória e sincronizando a execução. Os threads dentro de um bloco são executados no mesmo multiprocessador (SM - Streaming Multiprocessor) do GPU. O tamanho máximo de um bloco é limitado pelo hardware e varia entre diferentes arquiteturas CUDA. A escolha do tamanho do bloco é um fator crítico para o desempenho, pois afeta a ocupação do SM e a capacidade de ocultar a latência de memória.

**Thread:** É a unidade básica de execução em CUDA. Cada thread executa o mesmo código do kernel, mas opera sobre diferentes dados. Cada thread possui um ID único dentro do bloco, o `threadIdx`, e um ID único dentro da grid, que pode ser calculado a partir do `blockIdx`, `blockDim` e `threadIdx`.

![CUDA grid structure illustrating blocks, threads, and memory hierarchy.](./../images/image10.jpg)

#### Execução Independente dos Blocos

A capacidade de executar blocos em qualquer ordem [^1] é uma característica chave da arquitetura CUDA. Isso significa que o programador não precisa se preocupar com a ordem em que os blocos são executados, simplificando a programação paralela. O *scheduler* do GPU gerencia a alocação de blocos para os SMs disponíveis, garantindo que todos os blocos sejam eventualmente executados.

Essa independência também permite a **escalabilidade transparente** [^1]. À medida que mais núcleos de processamento ficam disponíveis (em GPUs mais poderosos), o *scheduler* pode simplesmente alocar mais blocos para execução simultânea. O código do kernel permanece o mesmo, sem necessidade de modificações para tirar proveito do hardware adicional.

#### Cálculo dos Índices de Threads e Blocos

Para que cada thread possa acessar os dados corretos, é essencial calcular o índice global da thread dentro da grid. Os seguintes identificadores são usados para este cálculo:

*   `threadIdx.x`, `threadIdx.y`, `threadIdx.z`: Índices da thread dentro do bloco nas dimensões x, y e z.
*   `blockIdx.x`, `blockIdx.y`, `blockIdx.z`: Índices do bloco dentro da grid nas dimensões x, y e z.
*   `blockDim.x`, `blockDim.y`, `blockDim.z`: Dimensões do bloco nas dimensões x, y e z.
*   `gridDim.x`, `gridDim.y`, `gridDim.z`: Dimensões da grid nas dimensões x, y e z.

O índice global da thread pode ser calculado da seguinte forma para o caso 1D, 2D e 3D:

*   **1D:** $$globalThreadIdx = blockIdx.x * blockDim.x + threadIdx.x$$
*   **2D:** $$globalThreadIdx = blockIdx.x * blockDim.x + threadIdx.x + (blockIdx.y * gridDim.x * blockDim.y) + (threadIdx.y * blockDim.x)$$
*   **3D:** $$globalThreadIdx = blockIdx.x * blockDim.x + threadIdx.x + (blockIdx.y * gridDim.x * blockDim.y) + (threadIdx.y * blockDim.x) + (blockIdx.z * gridDim.x * gridDim.y * gridDim.z) + (threadIdx.z * blockDim.x * blockDim.y)$$

#### Exemplo Prático

Considere um kernel CUDA que realiza a soma de dois vetores `A` e `B`, armazenando o resultado no vetor `C`. Cada thread será responsável por somar um elemento correspondente dos vetores `A` e `B`. O código do kernel poderia ser similar a este:

```c++
__global__ void vectorAdd(float *A, float *B, float *C, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) {
        C[i] = A[i] + B[i];
    }
}
```

Neste exemplo, `n` é o tamanho dos vetores. O índice `i` é calculado para cada thread, e a verificação `if (i < n)` garante que as threads não acessem posições fora dos limites dos vetores.  A configuração da grid e dos blocos, juntamente com o valor de `n`, determina quantos threads serão lançados para executar o kernel.

### Conclusão

A hierarquia de threads em CUDA, composta por grids de blocos e blocos de threads, oferece uma estrutura poderosa e flexível para a programação paralela. A execução independente dos blocos permite a escalabilidade transparente e simplifica o desenvolvimento de aplicações CUDA. A correta compreensão e utilização dos identificadores de threads e blocos (`threadIdx`, `blockIdx`, `blockDim`, `gridDim`) são essenciais para garantir o acesso correto aos dados e otimizar o desempenho. Ao compreender esses conceitos, é possível criar kernels CUDA eficientes e escaláveis que aproveitam ao máximo o poder de processamento do GPU.

### Referências
[^1]: Informação provida no contexto: "CUDA kernels are executed as a grid of threads, organized in a two-level hierarchy: a grid of blocks (1D, 2D, or 3D), and blocks containing an array of threads (1D, 2D, or 3D). Blocks can execute in any order, enabling transparent scalability."
<!-- END -->