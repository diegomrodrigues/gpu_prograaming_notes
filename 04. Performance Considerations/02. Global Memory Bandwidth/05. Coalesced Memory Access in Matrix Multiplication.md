## Otimização de Acesso à Memória Global em Multiplicação de Matrizes via Memória Compartilhada

### Introdução
A otimização do acesso à memória global é crucial para alcançar alto desempenho em aplicações CUDA. A memória global, embora vasta, possui uma latência significativamente maior em comparação com outros tipos de memória, como a memória compartilhada. Em particular, o acesso *coalescido* à memória global é fundamental para maximizar a largura de banda efetiva. Este capítulo explora a técnica de uso da memória compartilhada para possibilitar o acesso coalescido à memória global em cenários onde o algoritmo inerentemente exige iteração ao longo de linhas, como na multiplicação de matrizes.

### Conceitos Fundamentais
A largura de banda da memória global é um fator limitante no desempenho de muitas aplicações CUDA. A *coalescência* de memória ocorre quando threads em um warp acessam posições de memória global contíguas. Quando isso acontece, a GPU combina esses acessos em uma única transação de memória maior, reduzindo a sobrecarga e aumentando a eficiência. No entanto, quando os acessos não são coalescidos, a GPU pode precisar realizar múltiplas transações menores, o que leva a uma utilização ineficiente da largura de banda disponível.

Em multiplicação de matrizes, o padrão de acesso ideal para coalescência depende da forma como os dados são dispostos na memória. O contexto [^5] destaca que o acesso coalescido é obtido quando os threads em um warp leem colunas adjacentes, e não linhas adjacentes. Isso ocorre porque a memória é disposta linearmente, e elementos em colunas adjacentes estão fisicamente mais próximos na memória do que elementos em linhas adjacentes.

Quando o algoritmo requer iteração ao longo de linhas, como é comum em certas implementações de multiplicação de matrizes, o acesso direto à memória global resulta em *não-coalescência*. Para contornar essa limitação, a memória compartilhada pode ser utilizada como um buffer intermediário.

![Simplified memory hierarchy illustrating the relationship between main memory, caches, and the processor.](./../images/image5.jpg)

### Uso da Memória Compartilhada para Coalescência

A técnica envolve duas etapas principais:

1.  **Carregamento Coalescido na Memória Compartilhada:** Os dados necessários são primeiro carregados da memória global para a memória compartilhada usando um padrão de acesso coalescido. Isso significa que os threads em um warp leem blocos contíguos de dados da memória global e os armazenam na memória compartilhada.
2.  **Acesso Não-Coalescido na Memória Compartilhada:** Uma vez que os dados estejam na memória compartilhada, eles podem ser acessados em qualquer padrão necessário pelo algoritmo, incluindo iteração ao longo de linhas. Como a memória compartilhada possui uma latência muito menor e uma largura de banda muito maior do que a memória global, o impacto do acesso não-coalescido neste estágio é significativamente reduzido.

**Exemplo:**

Considere a multiplicação de duas matrizes $A$ e $B$ para produzir a matriz $C$, onde $C_{ij} = \sum_{k=1}^{N} A_{ik} B_{kj}$. Para calcular uma linha da matriz $C$, é necessário acessar elementos em uma linha de $A$ e elementos em colunas de $B$. Se $A$ estiver armazenada em *row-major order* e $B$ em *column-major order*, ou se ambas estiverem em *row-major order*, o acesso a $B$ resultará em acessos não-coalescidos à memória global.

Para otimizar, podemos carregar blocos de $A$ e $B$ na memória compartilhada de forma coalescida. Por exemplo, podemos carregar um bloco de $B$ na memória compartilhada onde os threads em um warp leem colunas adjacentes de $B$ da memória global. Uma vez que esse bloco esteja na memória compartilhada, os threads podem acessar os elementos necessários para calcular a linha correspondente de $C$, mesmo que isso envolva um padrão de acesso não-coalescido dentro da memória compartilhada.

**Pseudo-código:**

```c++
__global__ void matrixMul(float* A, float* B, float* C, int widthA, int widthB) {
  // Identificação do thread e bloco
  int bx = blockIdx.x;
  int by = blockIdx.y;
  int tx = threadIdx.x;
  int ty = threadIdx.y;

  // Dimensões dos blocos
  int blockWidthA = widthA / gridDim.x;
  int blockWidthB = widthB / gridDim.y;

  // Memória compartilhada para blocos de A e B
  __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];
  __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];

  // Loop sobre os blocos de A e B
  for (int k = 0; k < widthA / blockWidthA; ++k) {
    // Carregamento coalescido de blocos de A e B na memória compartilhada
    As[ty][tx] = A[by * blockWidthA * widthA + k * blockWidthA + ty * widthA + tx];
    Bs[ty][tx] = B[k * blockWidthB * widthB + bx * blockWidthB + ty * widthB + tx];

    // Sincronização para garantir que todos os dados estejam carregados
    __syncthreads();

    // Computação da multiplicação usando os dados na memória compartilhada
    // (Acesso aos dados na memória compartilhada pode ser não-coalescido,
    // mas o impacto é minimizado devido à alta largura de banda da memória compartilhada)

    // Sincronização antes de carregar o próximo bloco
    __syncthreads();
  }

  // Escrita do resultado na memória global
  C[by * blockWidthA * widthA + bx * blockWidthB + ty * widthA + tx] = ...;
}
```

### Análise e Considerações

O uso da memória compartilhada para otimizar o acesso à memória global introduz uma sobrecarga adicional devido à necessidade de carregar os dados na memória compartilhada e sincronizar os threads. No entanto, essa sobrecarga é geralmente superada pelos ganhos obtidos com o acesso coalescido à memória global e pela redução da latência ao acessar os dados na memória compartilhada.

A escolha do tamanho do bloco (BLOCK_SIZE no pseudo-código) é um parâmetro importante que afeta o desempenho. Um bloco muito pequeno pode não aproveitar totalmente a largura de banda da memória global, enquanto um bloco muito grande pode exceder a capacidade da memória compartilhada ou levar a uma utilização ineficiente dos recursos da GPU.

A memória compartilhada é limitada em tamanho. Portanto, essa técnica é mais eficaz quando os blocos de dados que precisam ser acessados cabem na memória compartilhada disponível.

### Conclusão
A utilização da memória compartilhada como um buffer intermediário é uma técnica eficaz para otimizar o acesso à memória global em aplicações CUDA, especialmente em algoritmos como a multiplicação de matrizes, onde o padrão de acesso natural pode levar a acessos não-coalescidos. Ao carregar dados da memória global para a memória compartilhada usando um padrão coalescido, e então acessar os dados na memória compartilhada no padrão necessário pelo algoritmo, é possível minimizar a sobrecarga de latência da memória global e maximizar a largura de banda efetiva. Essa otimização é crucial para alcançar alto desempenho em GPUs.

### Referências
[^5]: Contexto fornecido: "In matrix multiplication, coalesced memory access is achieved when threads in a warp read adjacent *columns*, not adjacent *rows*. If an algorithm inherently requires iteration along rows, shared memory can be used to enable coalescing. Data is first loaded into shared memory in a coalesced pattern and then accessed in the required pattern."
<!-- END -->