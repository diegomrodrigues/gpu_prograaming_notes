Okay, I will add Mermaid diagrams to enhance the text as requested.

## Global Memory Access in CUDA: Importance, Challenges, and Optimization Techniques

```mermaid
flowchart LR
    subgraph GPU
        GlobalMemory["Global Memory (DRAM)"]
        SharedMemory["Shared Memory"]
        Registers["Registers"]
    end
    
    Host["Host CPU"] --> GlobalMemory
    GlobalMemory --> SharedMemory
    SharedMemory --> Registers
    Registers --> SharedMemory
    SharedMemory --> GlobalMemory
    
    style GlobalMemory fill:#f9f,stroke:#333,stroke-width:2px
    style SharedMemory fill:#ccf,stroke:#333,stroke-width:2px
    style Registers fill:#cfc,stroke:#333,stroke-width:2px
    
    
    linkStyle 0,1,2,3,4,5 stroke:#333,stroke-width:1px;
    
    
    classDef slow fill:#f9f,stroke:#333,stroke-width:2px
    class GlobalMemory slow
    classDef medium fill:#ccf,stroke:#333,stroke-width:2px
    class SharedMemory medium
    classDef fast fill:#cfc,stroke:#333,stroke-width:2px
    class Registers fast
    
    
    
    
    
    
    
    
    
    direction LR
    
    
    
    
```

### Introdução

O acesso à **memória global** é um fator crítico para o desempenho de aplicações CUDA. A memória global é o principal espaço de armazenamento de dados na GPU, e a maioria das aplicações CUDA requer a transferência de grandes volumes de dados entre a memória global e os núcleos de processamento. No entanto, o acesso à memória global é uma operação relativamente lenta, se comparada a outros tipos de memória (como a memória compartilhada e os registradores), o que pode se tornar um gargalo de desempenho, caso não seja otimizado. Este capítulo irá explorar a importância da memória global em CUDA, os desafios associados ao seu acesso, e as estratégias para otimizar a transferência de dados para maximizar o desempenho. Analisaremos técnicas para evitar o desperdício da largura de banda da memória global, o que resulta em melhor desempenho, e também em menor consumo de energia. O entendimento dos mecanismos da memória global e de como acessá-la corretamente é fundamental para desenvolver aplicações CUDA eficientes.

### Conceitos Fundamentais

Para otimizar o desempenho de aplicações CUDA que utilizam a memória global, é essencial entender como ela funciona, os desafios de se utilizar e como fazer acessos eficientes a ela.

**Conceito 1: A Memória Global na GPU**

A **memória global** é um tipo de memória disponível na GPU que é acessível por todos os threads, tanto no host quanto no device [^14]. Ela é implementada com chips de memória DRAM (Dynamic Random Access Memory) e é utilizada para armazenar dados de entrada e de saída das aplicações CUDA, bem como dados intermediários que precisam ser acessados por múltiplos blocos de threads. A memória global é a principal área de armazenamento de dados na GPU e tem uma alta capacidade, mas em contrapartida, seu acesso é relativamente lento quando comparado a outros tipos de memória (como a memória compartilhada e os registradores).

**Lemma 1:** *A memória global é a principal área de armazenamento de dados nas GPUs, sendo acessível tanto pelo host quanto pelo device, e seu uso é essencial para a execução de kernels CUDA.*

*Prova:* A memória global é o único espaço de memória onde o host e o device podem trocar informações e dados. $\blacksquare$

**Conceito 2: Acesso à Memória Global e seu Impacto no Desempenho**

O acesso à memória global é uma operação relativamente lenta, devido à alta latência das DRAMs, de forma que, quando comparada com outras formas de memória da GPU, é a mais lenta.  Portanto, o desempenho das aplicações CUDA é altamente dependente da eficiência com que os dados são acessados da memória global. A otimização dos acessos à memória global é uma etapa essencial para otimizar o desempenho do seu código CUDA. A largura de banda da memória global, que representa a taxa de transferência de dados por unidade de tempo, pode se tornar um gargalo, se os acessos não forem feitos da forma mais eficiente possível.

**Corolário 1:** *O acesso ineficiente à memória global pode se tornar um gargalo de desempenho para aplicações CUDA, devido à sua latência comparativamente alta e à necessidade de garantir acessos coalescidos para utilização máxima da largura de banda.*

*Derivação:* A baixa largura de banda da memória global, combinada com a latência da DRAM, faz com que acessos ineficientes à memória global tornem-se um gargalo de desempenho para a aplicação.

**Conceito 3: Necessidade de Transferências Massivas de Dados**

As aplicações CUDA normalmente envolvem o processamento paralelo de grandes quantidades de dados, o que requer a transferência de grandes quantidades de dados da memória global para os núcleos de processamento da GPU, e vice-versa. O *overhead* dessas transferências de dados pode se tornar significativo e afetar o desempenho geral da aplicação, o que obriga que o programador faça uso de técnicas que evitem o tráfego excessivo de dados, tanto para a memória global, quanto para a memória compartilhada e os registradores.

> ⚠️ **Nota Importante:** A maioria das operações em CUDA envolve a transferência de grandes quantidades de dados da memória global, e esses acessos devem ser otimizados para evitar que o acesso à memória se torne o gargalo da aplicação.

### Desafios e Complexidades do Acesso à Memória Global

```mermaid
flowchart LR
    subgraph Global Memory Access Challenges
        LatenciaDRAM["Latency of DRAM"]
        AcessosCoalescidos["Need for Coalesced Access"]
        Concorrencia["Concurrency Issues"]
        LarguraBanda["Bandwidth Limitations"]
    end
    
    LatenciaDRAM --> AcessosCoalescidos
    AcessosCoalescidos --> Concorrencia
    Concorrencia --> LarguraBanda
    
    subgraph Memory Hierarchy
        SharedMemory["Shared Memory (Fast)"]
        Registers["Registers (Fastest)"]
    end
    
    LarguraBanda --> SharedMemory
    LarguraBanda --> Registers
    
    style LatenciaDRAM fill:#fdd,stroke:#333,stroke-width:2px
    style AcessosCoalescidos fill:#fdd,stroke:#333,stroke-width:2px
    style Concorrencia fill:#fdd,stroke:#333,stroke-width:2px
    style LarguraBanda fill:#fdd,stroke:#333,stroke-width:2px
    style SharedMemory fill:#ccf,stroke:#333,stroke-width:2px
    style Registers fill:#cfc,stroke:#333,stroke-width:2px
    
    linkStyle 0,1,2,3,4,5 stroke:#333,stroke-width:1px;
    
    
    
    direction TB
```

O acesso à memória global apresenta vários desafios e complexidades que precisam ser levados em conta:

**Latência da DRAM:**
A memória global é implementada utilizando chips de memória DRAM, que possuem alta latência. Isso significa que leva um tempo significativo para o hardware acessar os dados na memória global. A otimização dos acessos é fundamental para minimizar a influência da latência da DRAM.

**Acessos Coalescidos:**
Para maximizar a largura de banda da memória global, os acessos precisam ser **coalescidos**, ou seja, os threads de um mesmo warp precisam acessar posições contíguas de memória, em uma única transação [^8]. O acesso a memória não contígua exige que a GPU realize múltiplas transações para processar o mesmo warp, o que reduz a largura de banda de forma drástica.

**Concorrência:**
Múltiplos blocos de threads podem estar tentando acessar a memória global ao mesmo tempo, e isso pode levar a contenção e gargalos, já que as operações de leitura e escrita precisam ser sincronizadas.

**Dependência da Largura de Banda:**
O desempenho da aplicação pode ser limitado pela largura de banda da memória global. Se o programa tenta acessar mais dados do que a capacidade da memória consegue transferir, o desempenho da aplicação é limitado pela velocidade do acesso à memória, e não pela capacidade de processamento.

**Lemma 2:** *O acesso à memória global é desafiador devido à alta latência da DRAM, à necessidade de acessos coalescidos, à concorrência e às limitações da largura de banda, o que torna a otimização da memória fundamental para obter alto desempenho.*

*Prova:*  Os acessos à memória global são mais lentos e sujeitos a contenção, de forma que é preciso minimizar a quantidade de acessos e maximizar a largura de banda de cada acesso, para não sobrecarregar o sistema. $\blacksquare$

**Corolário 2:** *A implementação de algoritmos em CUDA precisa considerar esses desafios para obter o máximo desempenho. A arquitetura da GPU, com sua hierarquia de memórias, obriga que o programador otimize o uso de memória, tanto global, quanto compartilhada e registradores.*

*Derivação:* O baixo desempenho resultante de um uso inadequado da memória global é causado pelos seus gargalos, e a forma de como a memória é acessada, e esses problemas precisam ser mitigados para que o desempenho seja alto.

### Técnicas para Otimizar o Acesso à Memória Global

```mermaid
flowchart LR
    subgraph Optimization Techniques
        CoalescedAccess["Coalesced Memory Access"]
        SharedMemoryCache["Shared Memory as Cache"]
        Tiling["Data Tiling"]
        MinimizeTraffic["Minimize Global Memory Traffic"]
        AsyncTransfers["Asynchronous Transfers"]
    end
    
    CoalescedAccess --> SharedMemoryCache
    SharedMemoryCache --> Tiling
    Tiling --> MinimizeTraffic
    MinimizeTraffic --> AsyncTransfers

    
    style CoalescedAccess fill:#ccf,stroke:#333,stroke-width:2px
    style SharedMemoryCache fill:#ccf,stroke:#333,stroke-width:2px
    style Tiling fill:#ccf,stroke:#333,stroke-width:2px
    style MinimizeTraffic fill:#ccf,stroke:#333,stroke-width:2px
    style AsyncTransfers fill:#ccf,stroke:#333,stroke-width:2px
        
        
    linkStyle 0,1,2,3,4 stroke:#333,stroke-width:1px;
    
    direction TB

```

Para minimizar o impacto negativo do acesso à memória global, várias técnicas de otimização podem ser aplicadas.

**1. Acessos Coalescidos:**
   *  **Organização dos Dados:** Organizar os dados na memória de forma que os threads de um mesmo warp acessem posições de memória contíguas, utilizando a propriedade de que as threads de um warp possuem índices sequenciais.
   *  **Cálculo de Índices:** Utilizar o `threadIdx` para calcular os índices de memória que garantam acessos contíguos, minimizando a quantidade de transações de memória.

**2. Uso da Memória Compartilhada:**
   *  **Cache de Dados:** Utilizar a memória compartilhada para armazenar cópias dos dados da memória global que serão utilizados pelo bloco, para evitar múltiplos acessos à memória global, que é muito mais lenta que a memória compartilhada.
    *  **Acessos Otimizados:**  Realizar o carregamento dos dados da memória global para a memória compartilhada de forma coalescida, e então realizar os acessos à memória compartilhada, que tem menor latência e maior largura de banda.

**3. Tiling e Blocos de Dados:**
   *   **Dividir Dados em Blocos:** Dividir os dados em blocos menores e realizar o processamento por blocos, e não no conjunto total de dados, minimizando a quantidade de dados a serem acessados da memória global em uma dada etapa do algoritmo.
    *   **Utilizar Memória Compartilhada:** A estratégia de divisão de dados em blocos possibilita o uso da memória compartilhada para o processamento local de dados, antes de retornar os resultados para a memória global.

**4. Minimização do Tráfego da Memória Global:**
   *   **Reutilização de Dados:** Utilizar os dados na memória compartilhada o máximo possível, reutilizando os dados previamente carregados, para minimizar os acessos à memória global.
   * **Evitar Acessos Desnecessários:** Evitar acessos desnecessários à memória global, lendo e escrevendo dados apenas quando necessário.

**5. Uso de Transferências Assíncronas:**
   *   **Transferências Paralelas:** Utilizar transferências assíncronas para que o processamento na GPU possa ocorrer em paralelo com a transferência de dados, maximizando o uso do tempo de processamento, e minimizando o *overhead* da transferência de dados.

**Lemma 3:** *A otimização dos acessos à memória global envolve uma combinação de técnicas como o uso de acessos coalescidos, a utilização da memória compartilhada para o armazenamento de dados, o *tiling* dos dados em blocos menores, a minimização da quantidade de acessos, e o uso de transferências assíncronas.*

*Prova:* A aplicação combinada dessas técnicas maximiza a largura de banda da memória global, e também minimiza a latência e a quantidade de dados trafegados entre a CPU e a GPU. $\blacksquare$

**Corolário 3:** *A otimização dos acessos à memória global leva a um aumento significativo no desempenho das aplicações CUDA, devido à redução da latência e maximização da largura de banda da memória.*

*Derivação:* Ao otimizar a forma como a memória global é utilizada, a quantidade de dados transferidos em um dado período de tempo é maximizada, resultando em um aumento do desempenho.

### Dedução Teórica Complexa: Modelagem Matemática do Impacto da Coalescência na Largura de Banda da Memória Global

```mermaid
    flowchart LR
    A["Uncoalesced Access"] -->|Multiple Transactions| B("Lower Bandwidth")
    C["Coalesced Access"] --> |Single Transaction| D("Higher Bandwidth")
    
    style A fill:#fdd,stroke:#333,stroke-width:2px
    style B fill:#fdd,stroke:#333,stroke-width:2px
    style C fill:#ccf,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
    
    linkStyle 0,1 stroke:#333,stroke-width:1px,color:red
    linkStyle 2,3 stroke:#333,stroke-width:1px,color:green
    
    direction LR
```

Para entender como a coalescência afeta o desempenho do acesso à memória global, vamos desenvolver um modelo matemático que quantifica o ganho em termos de largura de banda.

**Modelo Teórico da Largura de Banda:**

Seja:
* $B_{max}$ a largura de banda máxima teórica da memória global.
*   $W$ o tamanho do warp (32 threads).
*  $T_{trans,coalesced}$ o tempo necessário para uma transação coalescida, onde múltiplos threads da mesma warp acessam posições de memória contíguas.
* $T_{trans,uncoalesced}$ o tempo necessário para uma transação não coalescida, onde cada thread acessa uma posição de memória diferente.
* $N_{threads}$ o número de threads que realizam acessos à memória.
*  $B_{efetiva,coalesced}$ a largura de banda efetiva do acesso coalescido.
* $B_{efetiva,uncoalesced}$ a largura de banda efetiva do acesso não coalescido.

Em acessos coalescidos, uma única transação de memória atende a todos os threads do warp, de forma que o tempo por transação é igual a $T_{trans,coalesced}$. Em acessos não coalescidos, cada thread necessita de uma transação separada, o que resulta em um tempo por transação igual a $T_{trans,uncoalesced}$.

A largura de banda efetiva é dada por:
$$B_{efetiva} = \frac{W * tamanho\_da\_transacao}{T_{trans}}$$
Em acessos coalescidos, temos:
$$B_{efetiva,coalesced} = \frac{W * tamanho\_da\_transacao}{T_{trans,coalesced}}$$
Em acessos não coalescidos, temos:
$$B_{efetiva,uncoalesced} = \frac{tamanho\_da\_transacao}{T_{trans,uncoalesced}}$$
onde $T_{trans,uncoalesced} > T_{trans,coalesced}$, e $W * B_{efetiva,uncoalesced} < B_{efetiva,coalesced} $

O acesso coalescido permite que a largura de banda máxima da memória global seja utilizada. No caso de acessos não coalescidos, a largura de banda utilizada é menor devido ao *overhead* das transações adicionais.

**Lemma 4:** *A utilização de acessos coalescidos à memória global maximiza a largura de banda, ao passo que a utilização de acessos não coalescidos diminui a largura de banda da memória.*

*Prova:* Ao utilizar acessos coalescidos, o hardware consegue realizar uma única transação de leitura e escrita na memória para um conjunto de threads, enquanto em acessos não coalescidos é necessário realizar múltiplas transações, o que resulta em um menor tempo de transferência por unidade de tempo. $\blacksquare$

**Corolário 4:** *Para obter alto desempenho em aplicações CUDA, é essencial utilizar acessos coalescidos à memória global, e minimizar a necessidade de acesso a dados que não sejam contíguos.*

*Derivação:* O acesso coalescido permite que o hardware transfira dados da memória global de forma mais eficiente, aumentando a taxa de transferência de dados, e diminuindo a latência.

### Prova ou Demonstração Matemática Avançada: Modelagem do Impacto do *Overhead* de Memória na Escalabilidade de Algoritmos CUDA

```mermaid
    flowchart LR
    A["Increase Problem Size"] --> B("Increase Memory Overhead")
    B --> C["Limited Speedup"]
    D["Optimized Memory Access"] --> E("Improved Scalability")
    
    style A fill:#ccf,stroke:#333,stroke-width:2px
    style B fill:#fdd,stroke:#333,stroke-width:2px
    style C fill:#fdd,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
    style E fill:#ccf,stroke:#333,stroke-width:2px
    
    linkStyle 0,1,2 stroke:#333,stroke-width:1px,color:red
    linkStyle 3,4 stroke:#333,stroke-width:1px,color:green
    
    direction LR
```

Para entender as limitações causadas pelo acesso à memória global, vamos modelar matematicamente o impacto do *overhead* da memória na escalabilidade de algoritmos CUDA.

**Modelo Teórico de *Overhead* de Memória:**
Seja:
*  $N$ o número total de elementos a serem processados.
* $P$ o número de processadores paralelos (threads, warps).
*  $T_{comp}$ o tempo de computação por elemento.
*  $T_{mem}$ o tempo de acesso à memória por elemento.
*  $T_{overhead,mem}$ o *overhead* total de memória, que é composto de dois componentes: $T_{trans}$ o tempo de transferência de dados e $T_{contencao}$ o tempo gasto devido a contenção da memória.
*  $T_{par}$ o tempo de execução paralela.
*  $T_{seq}$ o tempo de execução sequencial.

Em um algoritmo sequencial, o tempo de execução é dado por $T_{seq} = N * (T_{comp} + T_{mem})$.

Em um algoritmo paralelo, o tempo de execução é dado por:
$$T_{par} = \frac{N}{P} * T_{comp} +  T_{overhead,mem}$$
Onde o tempo de *overhead* da memória é dado por:
$$T_{overhead,mem} =  \frac{N}{P} * T_{mem} + T_{contencao} + T_{trans}$$

O termo $ \frac{N}{P} * T_{mem}$ representa o acesso à memória feita por cada thread. O termo $T_{contencao}$ representa a contenção da memória global, onde múltiplas threads tentam acessar a memória ao mesmo tempo, o que causa um gargalo. O termo $T_{trans}$ representa o tempo necessário para transferir dados entre a CPU e a GPU, e o tempo para realizar os acessos à memória de forma não contígua.

**Análise da Escalabilidade:**
O *speedup* do algoritmo paralelo é dado por:
$$ S = \frac{T_{seq}}{T_{par}} = \frac{N * (T_{comp} + T_{mem})}{\frac{N}{P} * T_{comp} +  \frac{N}{P} * T_{mem} + T_{contencao} + T_{trans}}$$

Quando $P$ aumenta, o termo $\frac{N}{P} * (T_{comp} + T_{mem})$ diminui, mas os termos $T_{contencao}$ e $T_{trans}$ aumentam, o que limita a escalabilidade e o *speedup* que pode ser alcançado. O objetivo da otimização é minimizar os termos $T_{contencao}$ e $T_{trans}$ para que o desempenho seja melhor, mesmo em um número maior de processadores.

**Lemma 5:** *O tempo de execução de algoritmos paralelos em GPUs é limitado pelo *overhead* de memória, e o aumento do número de threads não resulta em ganho linear no desempenho, quando o acesso à memória não é otimizado.*

*Prova:* A quantidade de tempo gasto para realizar acessos à memória se torna um gargalo em um dado ponto, o que faz com que o tempo de execução do algoritmo pare de diminuir e se torne linear com o aumento do número de threads. $\blacksquare$

**Corolário 5:** *A otimização dos acessos à memória, através da coalescência, do uso da memória compartilhada e da minimização da transferência de dados, resulta em algoritmos paralelos mais escaláveis e com menor tempo de execução.*

*Derivação:* Ao diminuir o *overhead* da memória, e também o tempo gasto para realizar as operações na memória global, a latência e o tempo total de execução são diminuídos, e a escalabilidade é aumentada.

### Pergunta Teórica Avançada: **Como a escolha do tamanho do bloco de threads afeta a latência e a largura de banda de acesso à memória global em um kernel CUDA?**

**Resposta:**

A escolha do tamanho do bloco de threads tem um impacto significativo na latência e na largura de banda de acesso à memória global em um kernel CUDA. O tamanho do bloco afeta como os threads são agrupados em warps, como esses warps acessam a memória global, e como os acessos são coalescidos. A utilização adequada do tamanho do bloco é fundamental para garantir uma largura de banda alta e minimizar a latência.

**Tamanho do Bloco e Acessos Coalescidos:**
O tamanho do bloco influencia a forma como os acessos à memória são realizados. Idealmente, para o acesso coalescido, todos os threads de um warp devem acessar posições contíguas da memória global. Uma escolha inadequada do tamanho do bloco pode levar a acessos não coalescidos, que utilizam o potencial de largura de banda da memória global de forma ineficiente.

**Tamanho do Bloco e Ocupação:**
Um tamanho de bloco menor pode resultar em baixa ocupação do SM (Streaming Multiprocessor), ou seja, poucos threads em execução por unidade de tempo, resultando em menor utilização do hardware. Um tamanho de bloco muito grande pode sobrecarregar o SM e levar à contenção de recursos, como registradores e memória compartilhada, diminuindo a quantidade de warps ativos e levando a um gargalo no desempenho.

**Tamanho do Bloco e Latência:**
A latência é o tempo necessário para realizar um acesso à memória. Blocos maiores podem levar a uma latência menor, uma vez que os acessos são feitos por mais threads simultaneamente, e que é possível utilizar memória compartilhada para fazer um acesso mais eficiente aos dados.

**Tamanho do Bloco e Largura de Banda:**
A escolha do tamanho do bloco afeta a largura de banda da memória, pois um tamanho adequado permite utilizar acessos coalescidos, o que aumenta a largura de banda, enquanto um tamanho inadequado pode gerar acessos não coalescidos, diminuindo a largura de banda da memória.

**Otimização:**
A escolha ideal do tamanho do bloco é aquela que equilibra a ocupação do SM, a coalescência de acessos à memória e a minimização da latência. Para garantir que o hardware esteja sendo usado de forma otimizada, e que a transferência de dados entre a memória global e o hardware ocorra da forma mais rápida, o desenvolvedor precisa balancear o tamanho do bloco com as outras características do hardware.

**Lemma 6:** *A escolha do tamanho do bloco de threads afeta diretamente a latência e a largura de banda de acesso à memória global, e uma escolha inadequada pode resultar em baixa utilização do hardware e baixo desempenho.*

*Prova:* O tamanho do bloco define como os threads são agrupados em warps, o que influencia como os acessos à memória são realizados. $\blacksquare$

**Corolário 6:** *Para maximizar a eficiência do acesso à memória global, o tamanho do bloco deve ser escolhido de forma a garantir a coalescência de acessos, a minimizar a latência e maximizar a largura de banda, além de garantir a ocupação do SM.*

*Derivação:* A escolha do tamanho do bloco afeta diretamente o desempenho, e o desenvolvedor precisa analisar como essa escolha afeta o acesso à memória e o uso dos outros recursos do hardware.

### Conclusão

Neste capítulo, exploramos a importância do acesso à **memória global** em aplicações CUDA, destacando que a maioria das operações necessitam de uma transferência de grandes quantidades de dados da memória global, que pode se tornar um gargalo de desempenho, caso não seja feito de forma otimizada. Vimos como a latência da DRAM, a necessidade de acessos coalescidos e a limitação da largura de banda podem afetar o desempenho das aplicações CUDA.  Apresentamos técnicas de otimização que envolvem a utilização de acessos coalescidos, o uso da memória compartilhada, a estratégia de *tiling*, e também o uso de transferências assíncronas. O entendimento desses conceitos é fundamental para o desenvolvimento de aplicações CUDA de alto desempenho. As ideias principais abordadas neste capítulo foram:

*   **Memória Global:** A memória global é a principal área de armazenamento de dados na GPU e deve ser utilizada com cuidado.
*   **Latência:** O acesso à memória global tem alta latência, o que pode levar a gargalos no desempenho da aplicação.
*   **Coalescência:** A coalescência de acessos à memória é essencial para maximizar a largura de banda da memória.
*  **Tiling e Memória Compartilhada:** Utilizar a memória compartilhada como um cache, e dividir os dados em blocos menores são formas de diminuir o tráfego da memória global.
*  **Otimização:** A otimização do acesso à memória global envolve a utilização de acessos coalescidos, o uso da memória compartilhada e a transferência de dados de forma assíncrona.

A otimização dos acessos à memória global é uma etapa fundamental no desenvolvimento de aplicações CUDA de alto desempenho, pois ela garante o máximo aproveitamento do poder de computação paralela da GPU.

### Referências

[^7]: "The SIMD hardware executes all threads of a warp as a bundle. An instruction is run for all threads in the same warp. It works well when all threads within a warp follow the same execution path, or more formally referred to as control flow, when working their data. For example, for an if-else construct, the execution works well when either all threads execute the if part or all execute the else part. When threads within a warp take different control flow paths, the SIMD hardware will take multiple passes through these divergent paths." *(Trecho de <Performance Considerations>)*
[^8]: "When all threads in a warp execute a load instruction, the hardware detects whether they access consecutive global memory locations. That is, the most favorable access pattern is achieved when all threads in a warp access consecutive global memory locations. In this case, the hardware combines, or coalesces, all these accesses into a consolidated access to consecutive DRAM locations." *(Trecho de <Performance Considerations>)*
[^10]: "Fortunately, a tiled algorithm can be used to enable coalescing. As we discussed in Chapter 5, threads of a block can first cooperatively load the tiles into the shared memory." *(Trecho de <Performance Considerations>)*
[^11]: "The execution resources in a streaming multiprocessor (SM) include registers, shared memory, thread block slots, and thread slots." *(Trecho de <Performance Considerations>)*
[^14]: "The global memory of a CUDA device is implemented with DRAMs." *(Trecho de <Performance Considerations>)*

**Deseja que eu continue com as próximas seções?**
