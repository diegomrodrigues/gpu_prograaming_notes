## Tiled Matrix Multiplication: Collaborative Data Loading

### Introdução

Como discutido anteriormente [^1, 95], alcançar alta eficiência no acesso à memória é crucial para o desempenho dos kernels CUDA. A memória global, embora vasta, possui alta latência e largura de banda limitada [^1, 95, 98]. Para mitigar essas limitações, o CUDA oferece memórias *on-chip* como registradores e memória compartilhada [^3, 97]. Este capítulo aprofunda-se no uso de *tiles* e memória compartilhada para otimizar o kernel de multiplicação de matrizes, reduzindo o tráfego para a memória global. Este método permite que os threads colaborem no carregamento de dados na memória compartilhada antes de utilizá-los individualmente, melhorando significativamente o desempenho [^15, 109].

### Conceitos Fundamentais

#### A Estratégia de *Tiling*

A estratégia de *tiling* envolve particionar os dados em subconjuntos menores, chamados *tiles*, que cabem na memória compartilhada [^11, 105]. A analogia é cobrir uma grande parede (dados na memória global) com *tiles* (subconjuntos que cabem na memória compartilhada) [^11, 105]. O critério fundamental é que a computação do kernel nesses *tiles* possa ser realizada independentemente [^11, 105].

Para ilustrar, considere a multiplicação de matrizes [^11, 105]. Em vez de cada thread acessar diretamente a memória global para cada elemento necessário, dividimos as matrizes em *tiles*.  Os threads dentro de um bloco colaboram para carregar esses *tiles* na memória compartilhada e, em seguida, cada thread acessa os elementos do *tile* na memória compartilhada para seus cálculos [^15, 109]. Isso reduz drasticamente o número de acessos à memória global, pois os dados são carregados uma vez na memória compartilhada e reutilizados por vários threads [^17, 111].

#### Implementação do Kernel *Tiled*

O kernel *tiled* mostrado na Figura 5.12 [^18] demonstra como implementar a estratégia de *tiling* para multiplicação de matrizes.  O kernel divide a computação em fases [^16, 110]. Em cada fase, todos os threads em um bloco colaboram para carregar um *tile* de elementos de $M$ e um *tile* de elementos de $N$ na memória compartilhada [^16, 110]. Isso é feito fazendo com que cada thread em um bloco carregue um elemento de $M$ e um elemento de $N$ na memória compartilhada [^16, 110].

As linhas 1 e 2 da Figura 5.12 [^18] declaram `Mds` e `Nds` como variáveis de memória compartilhada. O escopo das variáveis de memória compartilhada é um bloco [^18, 103]. Portanto, um par de `Mds` e `Nds` será criado para cada bloco e todos os threads de um bloco têm acesso ao mesmo `Mds` e `Nds` [^18, 103]. Isso é importante, pois todos os threads em um bloco devem ter acesso aos valores de $M$ e $N$ carregados em `Mds` e `Nds` por seus pares para que possam usar esses valores para satisfazer suas necessidades de entrada [^18, 103].

As linhas 3 e 4 salvam os valores de `threadIdx` e `blockIdx` em variáveis automáticas e, portanto, em registradores para acesso rápido [^18]. Lembre-se de que as variáveis automáticas têm escopo individual [^19, 103]. Ou seja, uma versão privada de `tx`, `ty`, `bx` e `by` é criada pelo sistema de tempo de execução para cada thread [^19, 103]. Elas residirão em registradores acessíveis por um thread [^19, 103]. Elas são inicializadas com os valores de `threadIdx` e `blockIdx` e usadas muitas vezes durante o tempo de vida do thread [^19, 103]. Uma vez que o thread termina, os valores dessas variáveis também deixam de existir [^19, 103].

As linhas 5 e 6 determinam o índice de linha e o índice de coluna do elemento `d_P` que o thread deve produzir [^19, 103]. Conforme mostrado na linha 6, a posição horizontal ($x$), ou o índice de coluna do elemento `d_P` a ser produzido por um thread, pode ser calculado como:

$$bx \\cdot TILE\\_WIDTH + tx$$

Isso ocorre porque cada bloco cobre `TILE_WIDTH` elementos na dimensão horizontal [^19, 103]. Um thread no bloco `bx` teria `bx` blocos de threads, ou ($bx \\cdot TILE\\_WIDTH$) threads antes dele; eles cobrem $bx \\cdot TILE\\_WIDTH$ elementos de `d_P` [^19, 103]. Outro thread `tx` dentro do mesmo bloco cobriria outro elemento `tx` de `d_P` [^19, 103]. Assim, o thread com `bx` e `tx` deve ser responsável por calcular o elemento `d_P` cujo índice $x$ é $bx \\cdot TILE\\_WIDTH + tx$ [^19, 103]. Este índice horizontal é salvo na variável `Col` (para coluna) para o thread e também é ilustrado na Figura 5.13 [^19, 20].

A linha 8 da Figura 5.12 [^18] marca o início do *loop* que itera através de todas as fases de cálculo do elemento final `d_P` [^19, 111]. Cada iteração do *loop* corresponde a uma fase do cálculo mostrada na Figura 5.11 [^16, 19, 110]. A variável `m` indica o número de fases que já foram feitas para o produto escalar [^19, 111]. Lembre-se de que cada fase usa um *tile* de elementos `d_M` e um *tile* de elementos `d_N` [^19, 111]. Portanto, no início de cada fase, $m \\cdot TILE\\_WIDTH$ pares de elementos `d_M` e `d_N` foram processados por fases anteriores [^19, 111].

Em cada fase, a linha 9 carrega o elemento `d_M` apropriado na memória compartilhada [^19, 111]. Como já conhecemos a linha de `d_M` e a coluna de `d_N` a serem processadas pelo thread, nos concentraremos no índice de coluna de `d_M` e no índice de linha de `d_N` [^19, 111]. Conforme mostrado na Figura 5.11 [^16, 110], cada bloco tem $TILE\\_WIDTH^2$ threads que colaborarão para carregar $TILE\\_WIDTH^2$ elementos `d_M` na memória compartilhada [^19, 113]. Assim, tudo o que precisamos fazer é atribuir a cada thread para carregar um elemento `d_M` [^19, 113]. Isso é convenientemente feito usando `blockIdx` e `threadIdx` [^19, 113].

#### Sincronização e Localidade

A função `__syncthreads()` [^18, 114] é crucial para garantir que todos os threads terminem de carregar os *tiles* de `d_M` e `d_N` em `Mds` e `Nds` antes que qualquer um deles possa avançar [^20, 114].  O *loop* na linha 12 então executa uma fase do produto escalar baseado nesses elementos do *tile* [^20, 114].

A reutilização dos dados na memória compartilhada promove a **localidade** dos acessos [^17, 111, 112]. A localidade se refere à tendência de um programa acessar os mesmos locais de memória repetidamente em um curto período de tempo [^17, 111, 112]. Ao carregar os dados na memória compartilhada e reutilizá-los, reduzimos a necessidade de acessar a memória global, que tem maior latência [^3, 97].

#### Impacto no Desempenho

O benefício do algoritmo *tiled* é substancial [^21, 115]. Para a multiplicação de matrizes, os acessos à memória global são reduzidos por um fator de $TILE\\_WIDTH$ [^21, 115]. Se usarmos *tiles* de 16x16, podemos reduzir os acessos à memória global em um fator de 16 [^21, 115]. Isso aumenta o CGMA de 1 para 16 [^21, 115]. Essa melhoria permite que a largura de banda da memória de um dispositivo CUDA suporte uma taxa de computação próxima ao seu desempenho de pico [^21, 115]. Por exemplo, essa melhoria permite que uma largura de banda de memória global de 150 GB/s suporte $(150/4) \\cdot 16 = 600$ GFLOPS! [^21, 115].

### Conclusão

A utilização de *tiles* e memória compartilhada é uma técnica poderosa para otimizar kernels CUDA, particularmente aqueles limitados pela largura de banda da memória [^21, 115]. Ao permitir que os threads colaborem no carregamento de dados e promovam a localidade, podemos reduzir drasticamente o tráfego para a memória global e alcançar ganhos significativos de desempenho [^21, 115]. No entanto, é importante considerar as limitações de capacidade da memória compartilhada e o uso de registradores [^21, 115, 22, 116], que podem se tornar fatores limitantes se não forem gerenciados cuidadosamente. O exemplo da multiplicação de matrizes *tiled* demonstra uma aplicação prática desses conceitos e fornece um modelo para otimizar outros kernels com padrões de acesso à memória semelhantes [^24, 118].

### Referências
[^1]: Capítulo 5, p. 95
[^2]: Capítulo 5, p. 96
[^3]: Capítulo 5, p. 97
[^4]: Capítulo 5, p. 98
[^5]: Capítulo 5, p. 99
[^6]: Capítulo 5, p. 100
[^7]: Capítulo 5, p. 101
[^8]: Capítulo 5, p. 102
[^9]: Capítulo 5, p. 103
[^10]: Capítulo 5, p. 104
[^11]: Capítulo 5, p. 105
[^12]: Capítulo 5, p. 106
[^13]: Capítulo 5, p. 107
[^14]: Capítulo 5, p. 108
[^15]: Capítulo 5, p. 109
[^16]: Capítulo 5, p. 110
[^17]: Capítulo 5, p. 111
[^18]: Capítulo 5, p. 112
[^19]: Capítulo 5, p. 113
[^20]: Capítulo 5, p. 114
[^21]: Capítulo 5, p. 115
[^22]: Capítulo 5, p. 116
[^23]: Capítulo 5, p. 117
[^24]: Capítulo 5, p. 118
<!-- END -->