## Locality and Shared Memory in CUDA: Enhancing Performance

### Introdução
Como discutido anteriormente [^95], os kernels CUDA são executados por um grande número de threads, acessando dados que são transferidos da memória do host para a memória global do dispositivo. No entanto, a memória global, tipicamente implementada com DRAM, possui latências elevadas e largura de banda finita, limitando o desempenho dos kernels [^95]. Este capítulo explora a importância da localidade dos dados e o uso de memórias rápidas e pequenas, como a memória compartilhada, para mitigar essas limitações e melhorar significativamente a eficiência dos acessos à memória em GPUs multi-core.

### Conceitos Fundamentais

A eficiência dos acessos à memória é crucial para o desempenho dos kernels CUDA [^96]. Para ilustrar isso, podemos analisar o kernel de multiplicação de matrizes [^96]. A parte mais importante do kernel, em termos de tempo de execução, é o loop `for` que realiza o cálculo do produto interno [^96]:

```c++
for (int k = 0; k < Width; ++k)
    Pvalue += d_M[Row*Width + k] * d_N[k*Width + Col];
```

Em cada iteração deste loop, dois acessos à memória global são realizados para uma multiplicação e uma adição de ponto flutuante. Um acesso busca um elemento `d_M[]` e o outro busca um elemento `d_N[]` [^96]. A razão entre o número de cálculos de ponto flutuante e o número de acessos à memória global (CGMA - Compute to Global Memory Access) é de 1:1, ou 1.0 [^96].

A CGMA tem implicações importantes no desempenho de um kernel CUDA [^96]. Em dispositivos modernos, a largura de banda da memória global é de cerca de 200 GB/s. Com 4 bytes por valor de ponto flutuante de precisão simples, pode-se esperar carregar no máximo 50 Giga operandos por segundo [^96]. Com uma CGMA de 1.0, o kernel de multiplicação de matrizes executará no máximo 50 GFLOPS, o que representa uma pequena fração do desempenho máximo de 1.500 GFLOPS ou mais para dispositivos de ponta [^97]. Para alcançar o desempenho máximo, é necessário aumentar a CGMA [^97].

Uma estratégia comum para aumentar a CGMA é particionar os dados em subconjuntos chamados *tiles*, de forma que cada *tile* caiba na memória compartilhada [^105]. O termo *tile* se refere à analogia de que uma grande parede (os dados na memória global) pode ser coberta por *tiles* (subconjuntos que cabem na memória compartilhada) [^105]. Uma vez que a memória compartilhada está *on-chip*, ela pode ser acessada com latência muito menor e largura de banda muito maior do que a memória global [^101].

**A localidade** é o comportamento onde cada fase se concentra em um pequeno subconjunto dos elementos da matriz de entrada. Quando um algoritmo exibe localidade, há uma oportunidade de usar memórias pequenas e de alta velocidade para atender à maioria dos acessos e remover esses acessos da memória global [^112]. A localidade é importante para alcançar alto desempenho em CPUs multi-core e GPUs multi-thread [^112].

A Figura 5.5 [^105] ilustra o conceito de *tiling* para a multiplicação de matrizes. O exemplo assume que usamos quatro blocos 2x2 para computar a matriz P.  Os acessos aos elementos M e N pelo *thread*(0,0) e *thread*(0,1) do *block*(0,0) são destacados com setas pretas [^105]. Por exemplo, o *thread*(0,0) lê M0,0 e N0,0, seguido por M0,1 e N1,0, seguido por M0,2 e N2,0, seguido por M0,3 e N3,0 [^105].

A Figura 5.6 [^106] mostra os acessos à memória global realizados por todos os *threads* no *block*0,0. Os *threads* são listados na direção vertical, com o tempo de acesso aumentando para a direita na direção horizontal [^106]. Cada *thread* acessa quatro elementos de M e quatro elementos de N durante sua execução [^106]. Entre os quatro *threads* destacados, há uma sobreposição significativa em termos dos elementos M e N que eles acessam [^106]. Por exemplo, o *thread*0,0 e o *thread*0,1 acessam M0,0, bem como o restante da linha 0 de M. Da mesma forma, o *thread*0,1 e o *thread*1,1 acessam N0,1, bem como o restante da coluna 1 de N [^106].

O kernel na Figura 5.1 [^106] é escrito de forma que tanto o *thread*0,0 quanto o *thread*0,1 acessem os elementos da linha 0 de M da memória global. Se pudermos de alguma forma fazer com que o *thread*0,0 e o *thread*1,0 colaborem para que esses elementos M sejam carregados da memória global apenas uma vez, podemos reduzir o número total de acessos à memória global pela metade [^106]. Em geral, podemos ver que cada elemento M e N é acessado exatamente duas vezes durante a execução do *block*0,0 [^106]. Portanto, se pudermos fazer com que todos os quatro *threads* colaborem em seus acessos à memória global, podemos reduzir o tráfego para a memória global pela metade [^106].

### Conclusão

O uso eficiente da memória compartilhada e a exploração da localidade dos dados são essenciais para otimizar o desempenho de kernels CUDA. Ao particionar os dados em *tiles* que se encaixam na memória compartilhada e ao fazer com que os *threads* colaborem no carregamento e reutilização desses *tiles*, podemos reduzir significativamente o tráfego para a memória global e aumentar a CGMA, aproximando o desempenho dos kernels do potencial máximo do hardware. A técnica de *tiling* e o uso da memória compartilhada são estratégias eficazes para melhorar o desempenho em diversas aplicações de computação paralela [^118].

### Referências
[^95]: Capítulo introdutório sobre CUDA Memories, enfatizando a necessidade de métodos adicionais para acessar a memória e remover a maioria das solicitações de dados da memória global.
[^96]: Seção 5.1, "Importance of Memory Access Efficiency," detalhando o impacto da eficiência do acesso à memória no desempenho do kernel de multiplicação de matrizes.
[^97]: Seção 5.2, "CUDA Device Memory Types," introduzindo tipos de memória que podem ser usados para alcançar uma alta razão CGMA.
[^101]: Seção 5.2, "CUDA Device Memory Types," explicando as características e diferenças entre registradores e memória compartilhada.
[^105]: Seção 5.3, "A Strategy for Reducing Global Memory Traffic," introduzindo o conceito de *tiling* como uma estratégia para reduzir o tráfego de memória global.
[^106]: Seção 5.3, "A Strategy for Reducing Global Memory Traffic," ilustrando os acessos à memória global realizados por *threads* em um bloco.
[^112]: Seção 5.4, "A Tiled Matrix-Matrix Multiplication Kernel," destacando a importância da localidade e do uso de memórias de alta velocidade.
[^118]: Seção 5.6, "Summary," resumindo o uso de algoritmos *tiled* para atingir alto desempenho.
<!-- END -->