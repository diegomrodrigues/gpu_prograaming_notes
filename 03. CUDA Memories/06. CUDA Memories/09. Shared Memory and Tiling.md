## Capítulo 5.3: Estratégias Avançadas para Reduzir o Tráfego na Memória Global com Memória Compartilhada e "Tiling"

### Introdução
Em CUDA, a escolha entre memória global e memória compartilhada apresenta um *trade-off* intrínseco: a memória global é vasta, porém lenta, enquanto a memória compartilhada é limitada em tamanho, mas oferece acesso rápido [^105]. Este capítulo explora estratégias para mitigar o gargalo da memória global, com foco no uso eficiente da memória compartilhada. Especificamente, detalharemos a técnica de "tiling" (ou divisão em blocos), que se mostra uma abordagem eficaz para otimizar o acesso à memória em aplicações CUDA [^105].

### Conceitos Fundamentais
A técnica de **tiling** consiste em particionar os dados em subconjuntos menores, denominados *tiles*, que possam ser acomodados na memória compartilhada [^105]. A analogia com a cobertura de uma parede (os dados na memória global) com azulejos (os *tiles* na memória compartilhada) ilustra bem o conceito. O critério essencial é que a computação do kernel sobre esses *tiles* possa ser realizada de forma independente [^105]. É importante notar que nem todas as estruturas de dados se prestam a esta divisão, dependendo da função do kernel [^105].

O uso da memória compartilhada permite que *threads* colaborem, compartilhando dados e resultados intermediários, reduzindo o tráfego na memória global e, consequentemente, melhorando o desempenho [^98]. A **divisão em blocos** envolve a partição de dados em subconjuntos menores que se encaixam na memória compartilhada, permitindo que os *threads* trabalhem colaborativamente em cada bloco para reduzir o tráfego na memória global [^98].

Para ilustrar o conceito, consideremos a multiplicação de matrizes, um exemplo clássico de aplicação que se beneficia do *tiling* [^105]. A Figura 5.5 [^105] apresenta um exemplo simplificado, onde a matriz P é computada utilizando quatro blocos 2x2. Para brevidade, as seguintes abreviações são utilizadas: $P_{y,x} = d\\_P[y*Width + x]$, $M_{y,x} = d\\_M[y*Width + x]$, e $N_{y,x} = d\\_N[y*Width + x]$. A figura destaca a computação realizada pelos quatro *threads* do bloco(0,0), que computam $P_{0,0}$, $P_{0,1}$, $P_{1,0}$ e $P_{1,1}$ [^105].

A Figura 5.6 [^106] ilustra os acessos à memória global realizados por todos os *threads* no bloco(0,0). Os *threads* são listados verticalmente, com o tempo de acesso aumentando horizontalmente. Cada *thread* acessa quatro elementos de M e quatro elementos de N durante sua execução. Observa-se uma sobreposição significativa nos elementos de M e N acessados pelos *threads*. Por exemplo, thread0,0 e thread0,1 acessam $M_{0,0}$ e o restante da linha 0 de M [^106].

O kernel na Figura 5.1 [^106] é implementado de forma que thread0,0 e thread0,1 acessem os elementos da linha 0 de M da memória global. Se thread0,0 e thread1,0 pudessem colaborar para carregar esses elementos de M uma única vez, o número total de acessos à memória global seria reduzido pela metade. Em geral, cada elemento de M e N é acessado duas vezes durante a execução do bloco(0,0). Portanto, se os quatro *threads* colaborarem, o tráfego na memória global pode ser reduzido pela metade [^106].

A redução potencial no tráfego da memória global é proporcional à dimensão dos blocos utilizados. Com blocos $N \\times N$, a redução potencial é de N. Por exemplo, com blocos 16x16, o tráfego pode ser reduzido em 1/16 [^106].

A técnica de *tiling* também pode ser comparada a um sistema de caronas (*carpooling*) [^108]. Os dados acessados por cada *thread* são análogos aos passageiros, e as requisições de DRAM aos veículos. Quando a taxa de requisições de DRAM excede a largura de banda do sistema DRAM, ocorre congestionamento e as unidades aritméticas ficam ociosas [^108]. Se múltiplos *threads* acessarem dados da mesma localização DRAM, eles podem formar um "carpool" e combinar seus acessos em uma única requisição DRAM [^108]. Isso requer que os *threads* tenham um cronograma de execução similar, permitindo que seus acessos de dados sejam combinados [^108].

A Figura 5.9 [^109] ilustra dois *threads* acessando os mesmos elementos de dados com tempos similares (arranjo bom) e com tempos diferentes (arranjo ruim). No segundo caso, os elementos precisam ser mantidos na memória *on-chip* por mais tempo, aumentando os requisitos de memória [^109]. Sendo assim, a sincronização é fundamental para manter os *threads* que formam o "carpool" seguindo a mesma execução [^109].

A Figura 5.10 [^109] ilustra a divisão das matrizes M e N em *tiles* 2x2. Cada *thread* é dividido em fases. Em cada fase, todos os *threads* em um bloco colaboram para carregar um *tile* de elementos M e um *tile* de elementos N na memória compartilhada. Isso é feito fazendo com que cada *thread* em um bloco carregue um elemento M e um elemento N na memória compartilhada, como ilustrado na Figura 5.11 [^110].

### Kernel Tiled Matrix-Matrix Multiplication
A Figura 5.12 [^112] apresenta o kernel *tiled* que usa memória compartilhada para reduzir o tráfego da memória global. Nas linhas 1 e 2, Mds e Nds são declaradas como variáveis de memória compartilhada. O escopo das variáveis de memória compartilhada é um bloco. Assim, um par de Mds e Nds será criado para cada bloco, e todos os *threads* de um bloco terão acesso ao mesmo Mds e Nds [^112]. Isso é importante, pois todos os *threads* em um bloco devem ter acesso aos valores de M e N carregados em Mds e Nds por seus pares para que possam usar esses valores para satisfazer suas necessidades de entrada [^112].

As linhas 3 e 4 armazenam os valores de *threadIdx* e *blockIdx* em variáveis automáticas e, portanto, em registradores para acesso rápido [^112].

A linha 8 da Figura 5.12 [^113] marca o início do *loop* que itera por todas as fases do cálculo do elemento final d_P. Cada iteração do *loop* corresponde a uma fase do cálculo mostrado na Figura 5.11 [^113]. A variável m indica o número de fases já realizadas para o produto escalar. Lembre-se de que cada fase usa um *tile* de d_M e um *tile* de elementos d_N. Portanto, no início de cada fase, m*TILE_WIDTH pares de elementos d_M e d_N foram processados pelas fases anteriores [^113].

Em cada fase, a linha 9 carrega o elemento d_M apropriado na memória compartilhada [^113]. Como já conhecemos a linha de d_M e a coluna de d_N a serem processadas pelo *thread*, focaremos no índice da coluna de d_M e no índice da linha de d_N. Como mostrado na Figura 5.11 [^113], cada bloco tem $TILE\\_WIDTH^2$ *threads* que colaborarão para carregar $TILE\\_WIDTH^2$ elementos d_M na memória compartilhada. Assim, tudo o que precisamos fazer é atribuir a cada *thread* o carregamento de um elemento d_M [^113]. Isso é convenientemente feito usando *blockIdx* e *threadIdx* [^113].

A barreira _syncthreads() na linha 11 garante que todos os *threads* tenham terminado de carregar os *tiles* de d_M e d_N em Mds e Nds antes que qualquer um deles possa seguir em frente [^114]. O *loop* na linha 12 então executa uma fase do produto escalar baseado nesses elementos do *tile*. A progressão do *loop* para *thread(ty,tx)* é mostrada na Figura 5.13 [^114], com a direção de d_M e d_N elementos de uso ao longo da seta marcada com k, a variável de *loop* na linha 12 [^115].

### Conclusão
O uso estratégico da memória compartilhada, combinado com a técnica de *tiling*, representa uma ferramenta poderosa para otimizar o desempenho de aplicações CUDA [^118]. Ao dividir os dados em subconjuntos menores e promover a colaboração entre *threads*, é possível reduzir significativamente o tráfego na memória global, mitigando o gargalo de desempenho associado à sua latência [^118]. A escolha do tamanho do *tile* deve considerar as limitações de capacidade da memória compartilhada e o número de *threads* em cada bloco [^118].

### Referências
[^98]: Overview of the CUDA device memory model.
[^105]: A Strategy for Reducing Global Memory Traffic.
[^106]: Global memory accesses performed by threads in blocko,o.
[^108]: Tiled algorithms are very similar to carpooling arrangements.
[^109]: Tiled algorithms require synchronization among threads.
[^110]: Execution phases of a tiled matrix multiplication.
[^112]: Tiled matrix multiplication kernel using shared memory.
[^113]: Scalar variables are placed into registers.
[^114]: Calculation of the matrix indices in tiled multiplication.
[^115]: Memory as a Limiting Factor to Parallelism
[^118]: In summary, CUDA defines registers, shared memory, and constant memory that can be accessed at a higher speed and in a more parallel manner than the global memory.

<!-- END -->