## CUDA On-Chip Memories: Registers and Shared Memory

### Introdução
Como discutido anteriormente, a eficiência do acesso à memória é crucial para o desempenho de kernels CUDA [^95, ^96]. Para mitigar as limitações da global memory, a CUDA oferece diversas opções de memória, incluindo registers e shared memory, que são on-chip memories [^95, ^97]. Este capítulo explora em detalhes o uso e as características dessas memórias, com foco em como elas podem ser utilizadas para otimizar o CGMA (compute to global memory access) ratio.

### Conceitos Fundamentais

#### Tipos de Memória CUDA e CGMA Ratio
A CUDA fornece vários tipos de memória para atingir um alto CGMA ratio [^97]. Estes incluem:
*   **Global Memory:** Implementada com DRAM, possui alta latência e largura de banda limitada [^95].
*   **Constant Memory:** Suporta acesso somente leitura de alta largura de banda, especialmente quando todos os threads acessam o mesmo local simultaneamente [^97].
*   **Registers:** Alocados a threads individuais, permitindo acesso exclusivo [^97].
*   **Shared Memory:** Alocada a thread blocks, permitindo que os threads dentro do mesmo bloco compartilhem dados de forma eficiente [^97].

O CGMA ratio é definido como o número de floating-point calculations realizados para cada acesso à global memory dentro de uma região de um programa CUDA [^96]. Aumentar o CGMA ratio é fundamental para melhorar o desempenho do kernel [^97].

#### Registers: Acesso Privado e Rápido
*Registers* são memórias on-chip alocadas a threads individuais [^97]. Cada thread tem acesso exclusivo aos seus próprios registers, o que elimina a necessidade de sincronização para evitar conflitos de acesso [^97]. Uma kernel function normalmente usa registers para armazenar variáveis frequentemente acessadas [^97].

*   **Escopo:** Cada register é privado para um thread individual [^98].
*   **Acesso:** Acesso extremamente rápido e paralelo [^103].
*   **Localização:** On-chip, resultando em baixa latência e alta largura de banda [^98].
*   **Uso:** Variáveis automáticas escalares em kernel e device functions são colocadas em registers [^102].

A utilização de registers para armazenar operandos de instruções aritméticas elimina a necessidade de instruções adicionais para carregar valores da memória global para a ALU (Arithmetic Logic Unit) [^99]. Por exemplo, uma instrução de adição de ponto flutuante típica tem a forma:
$$ fadd \\ r1, r2, r3 $$
onde *r2* e *r3* especificam os registers que contêm os operandos de entrada, e *r1* especifica o register para armazenar o resultado [^99].

Em contraste, se os operandos estiverem na global memory, seriam necessárias instruções de load adicionais, como:
$$\
load \\ r2, r4, offset \\\\\
fadd \\ r1, r2, r3
$$
onde *r4* contém o endereço base, *offset* é o deslocamento, e *r2* recebe o valor carregado da global memory [^100].

#### Shared Memory: Colaboração entre Threads
A *shared memory* é alocada a thread blocks, permitindo que todos os threads dentro de um bloco acessem variáveis na shared memory [^98]. Isso possibilita a colaboração eficiente entre threads, que podem compartilhar dados de entrada e resultados intermediários [^98].

*   **Escopo:** Variáveis na shared memory são visíveis para todos os threads dentro de um bloco [^103].
*   **Acesso:** Mais rápida que a global memory, mas mais lenta que registers [^101].
*   **Localização:** On-chip, mas parte do espaço de memória no chip [^101].
*   **Uso:** Compartilhamento de dados entre threads no mesmo bloco [^103].

Para declarar uma variável na shared memory, usa-se a palavra-chave `__shared__` [^103]. Por exemplo:
```c++
__shared__ int SharedVar;
```
Isso declara uma variável inteira chamada `SharedVar` na shared memory, acessível por todos os threads no bloco [^103].

#### Tiling: Redução do Tráfego na Global Memory
Uma estratégia comum para otimizar o acesso à memória é o *tiling* [^105]. A ideia é dividir os dados em subconjuntos menores (tiles) que possam caber na shared memory [^105]. Os threads colaboram para carregar os tiles na shared memory e realizar os cálculos localmente, reduzindo a necessidade de acessar a global memory repetidamente [^109].

#### Sincronização de Threads

Ao usar shared memory, é fundamental sincronizar os threads para evitar condições de corrida e garantir a consistência dos dados [^111]. A função `__syncthreads()` serve como uma barreira de sincronização, garantindo que todos os threads no bloco alcancem um determinado ponto no código antes que qualquer um deles possa prosseguir [^114].

#### Limitações de Capacidade

Embora registers e shared memory ofereçam acesso rápido e paralelo, suas capacidades são limitadas [^115]. Exceder essas capacidades pode reduzir o número de threads que podem residir simultaneamente em um Streaming Multiprocessor (SM), impactando o desempenho [^115].

### Conclusão
A escolha adequada do tipo de memória é crucial para otimizar o desempenho de kernels CUDA [^97]. Registers fornecem acesso rápido e privado para cada thread, enquanto a shared memory permite a colaboração eficiente entre threads no mesmo bloco [^97]. Ao usar técnicas como tiling e sincronização, é possível reduzir significativamente o tráfego na global memory e melhorar o CGMA ratio [^105, ^111]. No entanto, é fundamental estar ciente das limitações de capacidade dessas memórias para evitar gargalos de desempenho [^115].

### Referências
[^95]: Página 95 do documento fonte.
[^96]: Página 96 do documento fonte.
[^97]: Página 97 do documento fonte.
[^98]: Página 98 do documento fonte.
[^99]: Página 99 do documento fonte.
[^100]: Página 100 do documento fonte.
[^101]: Página 101 do documento fonte.
[^102]: Página 102 do documento fonte.
[^103]: Página 103 do documento fonte.
[^105]: Página 105 do documento fonte.
[^109]: Página 109 do documento fonte.
[^111]: Página 111 do documento fonte.
[^114]: Página 114 do documento fonte.
[^115]: Página 115 do documento fonte.
<!-- END -->