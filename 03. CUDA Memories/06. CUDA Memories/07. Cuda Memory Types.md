## 5.2 CUDA Device Memory Types

### Introdução
O desempenho de kernels CUDA é fortemente influenciado pela eficiência com que os dados são acessados. A arquitetura CUDA oferece diversas opções de memória, cada uma com características distintas de escopo, tempo de vida, latência e largura de banda [^95]. A escolha apropriada do tipo de memória pode reduzir a necessidade de acessos à memória global, que é tipicamente implementada com DRAM, apresentando alta latência e largura de banda limitada [^95]. Este capítulo explora os diferentes tipos de memória disponíveis em CUDA e suas implicações no desempenho.

### Conceitos Fundamentais
CUDA suporta vários tipos de memória que podem ser usados pelos programadores para alcançar uma alta taxa CGMA (*compute to global memory access*) e, portanto, uma alta velocidade de execução em seus kernels [^97]. A Figura 5.2 [^97, 98] ilustra esses tipos de memória, incluindo memória global, memória constante, registros e memória compartilhada.

**Memória Global e Constante:**
Localizadas na parte inferior da Figura 5.2 [^97, 98], a memória global e a memória constante podem ser escritas (W) e lidas (R) pelo host através de chamadas de API [^97]. A memória global foi introduzida no Capítulo 3 [^97]. A memória constante oferece acesso somente leitura de alta largura de banda quando todos os threads acessam simultaneamente a mesma localização [^97]. A memória constante é armazenada na memória global, mas é cached para acesso eficiente [^104]. O tamanho total da memória constante em uma aplicação é limitado a 65.536 bytes [^104].

**Registros e Memória Compartilhada:**
Registros e memória compartilhada são memórias *on-chip* [^97]. Variáveis que residem nesses tipos de memória podem ser acessadas em alta velocidade e de forma paralela [^97].
*   **Registros:** São alocados a threads individuais; cada thread só pode acessar seus próprios registros [^97]. Uma função kernel normalmente usa registros para armazenar variáveis acessadas frequentemente [^97]. A Figura 5.1 [^96, 97] mostra um exemplo onde as variáveis `Row`, `Col` e `Pvalue` são variáveis automáticas e, portanto, alocadas em registros [^103].
*   **Memória Compartilhada:** É alocada a blocos de threads; todos os threads em um bloco podem acessar variáveis nas localizações de memória compartilhada alocadas ao bloco [^98]. A memória compartilhada é um meio eficiente para os threads cooperarem, compartilhando seus dados de entrada e os resultados intermediários de seu trabalho [^98].

**Escopo e Tempo de Vida das Variáveis:**
A declaração de uma variável CUDA determina seu tipo de memória, escopo e tempo de vida [^102].

*   **Escopo:** Identifica o alcance dos threads que podem acessar a variável: por um único thread, por todos os threads de um bloco ou por todos os threads de todos os grids [^102]. Se o escopo de uma variável é um único thread, uma versão privada da variável será criada para cada thread; cada thread só pode acessar sua versão privada da variável [^102].
*   **Tempo de Vida:** Indica a porção da duração da execução do programa quando a variável está disponível para uso: dentro da execução de um kernel ou durante toda a aplicação [^102]. Se o tempo de vida de uma variável é dentro da execução de um kernel, ela deve ser declarada dentro do corpo da função kernel e estará disponível para uso apenas pelo código do kernel [^102].

A Tabela 5.1 [^102] resume os qualificadores de tipo de variável CUDA, sua memória, escopo e tempo de vida.

**Acesso a Registros vs. Memória Global:**
O acesso a registros envolve menos instruções do que o acesso à memória global [^99]. Em um acesso à memória global, o processador usa o valor do PC (*program counter*) para buscar instruções da memória para o IR (*instruction register*) [^99]. As instruções buscadas controlam as atividades dos componentes do computador, o que é conhecido como execução de instrução [^99]. Quando um operando de uma instrução aritmética está em um registro, nenhuma instrução adicional é necessária para tornar o valor do operando disponível para a unidade lógica e aritmética (ALU) [^99]. Por outro lado, se um valor de operando está na memória global, é necessário realizar uma operação de carregamento da memória para tornar o valor do operando disponível para a ALU [^100].

**Memória Local:**
Além dos registros, cada thread também tem acesso à memória local. Essa memória é privada para cada thread e tem um tempo de vida que corresponde à execução do kernel [^98]. A memória local é usada para armazenar variáveis que não cabem nos registros, como arrays grandes ou variáveis que são spilladas dos registros devido à alta pressão de registro [^103].

### Conclusão
A arquitetura CUDA oferece uma hierarquia de memória rica e diversificada, permitindo aos programadores otimizar o acesso aos dados e maximizar o desempenho de seus kernels. Compreender as características de cada tipo de memória, como escopo, tempo de vida, latência e largura de banda, é crucial para escrever código CUDA eficiente. A escolha adequada do tipo de memória, juntamente com técnicas como tiling e colaboração entre threads, pode reduzir significativamente o tráfego de memória global e melhorar o desempenho geral da aplicação.

### Referências
[^95]: Capítulo 5, página 95.
[^97]: Capítulo 5, página 97.
[^98]: Capítulo 5, páginas 97-98.
[^99]: Capítulo 5, página 99.
[^100]: Capítulo 5, página 100.
[^102]: Capítulo 5, página 102.
[^103]: Capítulo 5, página 103.
[^104]: Capítulo 5, página 104.

<!-- END -->