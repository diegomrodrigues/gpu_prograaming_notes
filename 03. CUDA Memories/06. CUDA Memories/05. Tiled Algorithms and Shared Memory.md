## 5.4 A Tiled Matrix-Matrix Multiplication Kernel

### Introdução
Como vimos anteriormente, a eficiência do acesso à memória é crucial para o desempenho dos kernels CUDA [^96]. A memória global, embora grande, possui alta latência e largura de banda limitada [^95]. Uma estratégia comum para mitigar esses problemas é a utilização de **tiling**, onde os dados são particionados em subconjuntos menores, chamados *tiles*, que cabem na memória compartilhada, que é menor, mas de acesso muito mais rápido [^105]. Este capítulo se aprofunda na implementação de um kernel de multiplicação de matrizes que emprega tiling para reduzir o tráfego na memória global [^109].

### Conceitos Fundamentais

A ideia central do algoritmo de tiling é fazer com que as threads colaborem para carregar elementos das matrizes $M$ e $N$ na memória compartilhada antes de utilizá-los individualmente no cálculo do produto escalar [^109]. É fundamental considerar que a memória compartilhada é limitada e deve-se evitar exceder sua capacidade ao carregar os elementos [^109]. Isso é alcançado dividindo as matrizes $M$ e $N$ em *tiles* menores, cujo tamanho é escolhido de forma que caibam na memória compartilhada [^109]. Em sua forma mais simples, as dimensões dos *tiles* são iguais às dimensões do bloco [^109].

Em algoritmos tiled, as threads se organizam para carregar coletivamente os elementos $M$ e $N$ na memória compartilhada antes de usá-los individualmente para o cálculo do produto escalar [^109]. As matrizes $M$ e $N$ são divididas em *tiles* menores para caber na memória compartilhada, e cada thread carrega um elemento de $M$ e um elemento de $N$ [^109]. O processo é dividido em fases, onde todas as threads em um bloco colaboram para carregar um *tile* de elementos de $M$ e um *tile* de elementos de $N$ na memória compartilhada [^110]. Cada thread no bloco é responsável por carregar um elemento de $M$ e um elemento de $N$ [^110].

Após o carregamento dos *tiles* de $M$ e $N$ na memória compartilhada, esses valores são utilizados no cálculo do produto escalar [^111]. É importante notar que cada valor na memória compartilhada é usado múltiplas vezes, reduzindo o número de acessos à memória global [^111]. Especificamente, cada elemento de $M$ e $N$ é acessado duas vezes durante a execução do bloco [^106]. Ao carregar cada valor da memória global na memória compartilhada para uso múltiplo, reduz-se o número de acessos à memória global [^111]. No caso ilustrado, o número de acessos à memória global é reduzido pela metade [^111]. Em geral, essa redução é de um fator de $N$ se os *tiles* forem de tamanho $N \times N$ [^111].

A colaboração entre as threads para carregar os dados na memória compartilhada é crucial para o desempenho. Para garantir a correta execução do kernel tiled, é necessário utilizar a função `__syncthreads()` para sincronizar as threads após o carregamento dos *tiles* e antes de iniciar o cálculo do produto escalar [^114]. Isso garante que todas as threads tenham carregado seus respectivos elementos na memória compartilhada antes de iniciar a computação, evitando condições de corrida e resultados incorretos [^114].

O kernel tiled mostrado na Figura 5.12 [^112] implementa as fases ilustradas na Figura 5.11 [^112]. As linhas 1 e 2 declaram `Mds` e `Nds` como variáveis de memória compartilhada [^112]. O escopo das variáveis de memória compartilhada é um bloco [^112]. Assim, um par de `Mds` e `Nds` será criado para cada bloco e todas as threads de um bloco terão acesso aos mesmos `Mds` e `Nds` [^112]. Isso é importante, pois todas as threads em um bloco devem ter acesso aos valores de $M$ e $N$ carregados em `Mds` e `Nds` por seus pares para que possam usar esses valores para satisfazer suas necessidades de entrada [^112].

### Conclusão

A técnica de tiling é uma estratégia eficaz para reduzir o tráfego na memória global, explorando a localidade dos dados e a colaboração entre as threads [^111]. Ao dividir o problema em *tiles* menores e utilizar a memória compartilhada para armazenar os dados acessados frequentemente, é possível aumentar significativamente o desempenho dos kernels CUDA [^115]. No entanto, é crucial considerar as limitações de capacidade da memória compartilhada e garantir a correta sincronização das threads para evitar erros e maximizar a eficiência [^115].

### Referências
[^95]: CUDA Memories: So far, we have learned to write a CUDA kernel function that is executed by a massive number of threads. The data to be processed by these threads is first transferred from the host memory to the device global memory. The threads then access their portion of the data from the global memory using their block IDs and thread IDs. We have also learned more details of the assignment and scheduling of threads for execution. Although this is a very good start, these simple CUDA kernels will likely achieve only a small frac- tion of the potential speed of the underlying hardware. The poor perfor- mance is due to the fact that global memory, which is typically implemented with dynamic random access memory (DRAM), tends to have long access latencies (hundreds of clock cycles) and finite access band- width.
[^96]: 5.1 IMPORTANCE OF MEMORY ACCESS EFFICIENCY
[^105]: 5.3 A STRATEGY FOR REDUCING GLOBAL MEMORY TRAFFIC: We have an intrinsic trade-off in the use of device memories in CUDA: global memory is large but slow, whereas the shared memory is small but fast. A common strategy is partition the data into subsets called tiles so that each tile fits into the shared memory.
[^106]: Figure 5.6 shows the global memory accesses done by all threads in blocko.0. The threads are listed in the vertical direction, with time of access increasing to the right in the horizontal direction. Note that each thread accesses four elements of M and four elements of N during its execution. Among the four threads highlighted, there is a significant overlap in terms of the M and N elements they access. For example, thread0.0 and thread0,1 both access M0,0 as well as the rest of row 0 of M. Similarly, thread0,1 and thread1,1 both access No,1 as well as the rest of column 1 of N.
[^109]: 5.4 A TILED MATRIX—MATRIX MULTIPLICATION KERNEL: We now present an algorithm where threads collaborate to reduce the traf- fic to the global memory. The basic idea is to have the threads to collabo- ratively load M and N elements into the shared memory before they individually use these elements in their dot product calculation. Keep in mind that the size of the shared memory is quite small and one must be careful not to exceed the capacity of the shared memory when loading these M and N elements into the shared memory. This can be accomplished by dividing the M and N matrices into smaller tiles. The size of these tiles is chosen so that they can fit into the shared memory. In the simplest form, the tile dimensions equal those of the block, as illustrated in Figure 5.10.
[^110]: 5.4 A Tiled Matrix-Matrix Multiplication Kernel: each thread are now divided into phases. In each phase, all threads in a block collaborate to load a tile of M elements and a tile of N elements into the shared memory. This is done by having every thread in a block to load one M element and one N element into the shared memory, as illus- trated in Figure 5.11. Each row of Figure 5.11 shows the execution
[^111]: 5.4 A Tiled Matrix-Matrix Multiplication Kernel: After the two tiles of M and N elements are loaded into the shared mem- ory, these values are used in the calculation of the dot product. Note that each value in the shared memory is used twice. For example, the M1,1 value, loaded by thread1,1 into Mds1,1, is used twice, once by threado,1 and once by thread1,1. By loading each global memory value into shared mem- ory so that it can be used multiple times, we reduce the number of accesses to the global memory. In this case, we reduce the number of accesses to the global memory by half. Readers should verify that the reduction is by a factor of N if the tiles are N× N elements.
[^112]: Figure 5.12 implements the phases illustrated in Figure 5.11. In Figure 5.12, lines 1 and 2 declare Mds and Nds as shared memory variables. Recall that the scope of shared memory variables is a block. Thus, one pair of Mds and Nds will be created for each block and all threads of a block have access to the same Mds and Nds.
[^114]: 5.4 A Tiled Matrix-Matrix Multiplication Kernel: The barrier _syncthreads() in line 11 ensures that all threads have finished loading the tiles of d_M and d_N into Mds and Nds before any of them can move forward.
[^115]: 5.5 MEMORY AS A LIMITING FACTOR TO PARALLELISM: While CUDA registers and shared memory can be extremely effective in reducing the number of accesses to global memory, one must be careful not to exceed the capacity of these memories. These memories are forms of resources that are needed for thread execution.
<!-- END -->