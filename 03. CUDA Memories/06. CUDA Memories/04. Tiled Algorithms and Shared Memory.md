## Tiling: A Strategy for Reducing Global Memory Traffic in CUDA

### Introdução

Em CUDA, existe um *trade-off* intrínseco no uso de diferentes tipos de memória: a memória global é grande, mas lenta, enquanto a memória compartilhada é pequena, mas rápida [^11]. Para otimizar o desempenho, uma estratégia comum é particionar os dados em subconjuntos chamados **tiles**, que se encaixam na memória compartilhada [^11]. Esta abordagem, conhecida como **tiling**, permite que a computação do kernel seja realizada independentemente em cada tile, reduzindo o número de acessos à memória global [^11].

### Conceitos Fundamentais

O conceito de tiling se assemelha ao *carpooling*, onde threads que acessam os mesmos dados da DRAM combinam seus acessos em uma única requisição [^11]. Isso exige que os threads tenham um padrão de execução semelhante [^11]. O uso de **barreiras de sincronização** garante que os threads sigam aproximadamente o mesmo tempo de execução [^11].

#### Analogia com Carpooling
A analogia com o carpooling é útil para entender o conceito de tiling. Imagine que cada dado acessado por um thread é um passageiro e cada requisição à DRAM é um veículo. Se muitos threads (passageiros) precisam acessar os mesmos dados (ir para o mesmo local), eles podem formar um "carpool" e combinar seus acessos em uma única requisição [^11]. Isso reduz o "tráfego" na memória global e melhora o desempenho [^11]. No entanto, para que o carpooling seja eficiente, os threads (passageiros) precisam ter horários semelhantes, ou seja, acessar os dados aproximadamente ao mesmo tempo [^11].

#### Tiled Matrix Multiplication
O conceito de tiling pode ser ilustrado com o exemplo da multiplicação de matrizes [^11]. Considere a multiplicação de duas matrizes $M$ e $N$ para gerar a matriz $P$. Em vez de cada thread acessar diretamente os elementos de $M$ e $N$ na memória global, podemos dividir as matrizes em tiles menores que se encaixam na memória compartilhada. Os threads colaboram para carregar esses tiles na memória compartilhada e, em seguida, realizam os cálculos necessários.

##### Exemplo
A Figura 5.5 [^11] apresenta um exemplo simplificado de multiplicação de matrizes com tiling. As matrizes são divididas em blocos de 2x2. Os threads dentro de um bloco colaboram para carregar os elementos dos tiles correspondentes de $M$ e $N$ na memória compartilhada. Por exemplo, thread(0,0) lê $M_{0,0}$ e $N_{0,0}$, seguido por $M_{0,1}$ e $N_{1,0}$, e assim por diante [^11]. A Figura 5.6 [^12] mostra os acessos à memória global realizados por todos os threads no bloco(0,0). Observe que cada thread acessa quatro elementos de $M$ e quatro elementos de $N$ [^12].

##### Redução de Acessos à Memória Global
A chave para reduzir o tráfego na memória global é a colaboração entre os threads. Se pudermos garantir que os threads dentro de um bloco colaborem para carregar os tiles de $M$ e $N$ na memória compartilhada, podemos reduzir o número total de acessos à memória global. No exemplo da Figura 5.1 [^12], thread0,0 e thread0,1 acessam os elementos da linha 0 de $M$ da memória global. Se pudermos fazer com que thread0,0 e thread1,0 colaborem para carregar esses elementos apenas uma vez, podemos reduzir o número total de acessos à memória global pela metade [^12].

##### Dimensão dos Blocos
A redução potencial no tráfego da memória global é proporcional à dimensão dos blocos utilizados. Com blocos $N \\times N$, a redução potencial do tráfego da memória global seria $N$ [^12]. Por exemplo, se usarmos blocos de 16x16, podemos potencialmente reduzir o tráfego da memória global para 1/16 através da colaboração entre threads [^12].

#### Sincronização de Threads
As threads que formam o "carpool" precisam seguir aproximadamente o mesmo tempo de execução [^15]. Isto é conseguido através da utilização de barreiras de sincronização, como `__syncthreads()` [^15]. Esta função garante que todas as threads no bloco tenham concluído o carregamento dos tiles de $M$ e $N$ na memória compartilhada antes que qualquer uma delas prossiga para a próxima fase de computação [^18].

#### Código Tiled
O código para um kernel de multiplicação de matrizes com tiling é apresentado na Figura 5.12 [^18]. As variáveis `Mds` e `Nds` são declaradas como variáveis de memória compartilhada. As linhas 3 e 4 salvam os valores `threadIdx` e `blockIdx` em variáveis automáticas para acesso rápido [^18]. As linhas 5 e 6 determinam o índice de linha e o índice de coluna do elemento `d_P` que o thread deve produzir [^19]. A linha 8 marca o início do loop que itera através de todas as fases de cálculo do elemento final `d_P` [^19]. A linha 9 carrega o elemento `d_M` apropriado na memória compartilhada [^19]. A linha 10 carrega o elemento `d_N` apropriado na memória compartilhada. A barreira `_syncthreads()` na linha 11 garante que todos os threads tenham terminado de carregar os tiles de `d_M` e `d_N` em `Mds` e `Nds` antes que qualquer um deles possa prosseguir [^20]. O loop na linha 12 então executa uma fase do produto escalar com base nesses elementos de tile [^20]. A barreira `_syncthreads()` na linha 14 garante que todos os threads tenham terminado de usar os elementos `d_M` e `d_N` na memória compartilhada antes que qualquer um deles avance para a próxima iteração e carregue os elementos nos próximos tiles [^21].

### Conclusão

O tiling é uma estratégia eficaz para reduzir o tráfego na memória global e melhorar o desempenho em CUDA [^24]. Ao dividir os dados em tiles menores que se encaixam na memória compartilhada e ao garantir que os threads colaborem para carregar e processar esses tiles, podemos reduzir significativamente o número de acessos à memória global [^24]. No entanto, é importante estar ciente das limitações de tamanho da memória compartilhada e dos registros, bem como das limitações de hardware de threading, ao projetar kernels com tiling [^24].

### Referências
[^11]: Seção 5.3, página 105.
[^12]: Seção 5.3, página 106.
[^15]: Seção 5.4, página 109.
[^18]: Seção 5.4, página 112.
[^19]: Seção 5.4, página 113.
[^20]: Seção 5.4, página 114.
[^21]: Seção 5.5, página 115.
[^24]: Seção 5.6, página 118.
<!-- END -->