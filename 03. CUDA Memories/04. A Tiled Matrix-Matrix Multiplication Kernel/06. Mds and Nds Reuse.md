## Reutilização de Memória Compartilhada para Localidade em Multiplicação de Matrizes Tiled

### Introdução
A multiplicação de matrizes utilizando o conceito de *tiling* [^111] apresenta uma estratégia eficaz para otimizar o uso da memória em arquiteturas CUDA. Como discutido no capítulo anterior, a memória global possui alta latência e baixa largura de banda, enquanto a memória compartilhada oferece o oposto [^105]. O *tiling* permite particionar os dados em subconjuntos menores, chamados *tiles*, que podem ser carregados na memória compartilhada, explorando a localidade dos dados e reduzindo o tráfego para a memória global. Este capítulo se aprofundará na técnica de reutilização de `Mds` e `Nds` para armazenar os valores de entrada, maximizando a localidade e minimizando o uso de memória compartilhada.

### Conceitos Fundamentais

No contexto da multiplicação de matrizes tiled, `Mds` e `Nds` representam *arrays* de memória compartilhada utilizados para armazenar os *tiles* das matrizes de entrada M e N, respectivamente [^111]. A reutilização desses *arrays* é uma otimização crucial para reduzir a quantidade total de memória compartilhada necessária.

A ideia central é que, em cada fase da computação, apenas um subconjunto dos elementos de M e N é necessário [^111]. Em vez de alocar memória compartilhada suficiente para conter toda a matriz, `Mds` e `Nds` são reutilizados para armazenar apenas os *tiles* relevantes para a fase atual [^111]. Isso é possível devido à natureza local da operação de multiplicação de matrizes tiled.

*Localidade:* A localidade refere-se à propriedade de um algoritmo onde os acessos à memória se concentram em um pequeno subconjunto de dados em um determinado período [^111]. Na multiplicação de matrizes tiled, cada fase da computação se concentra em um pequeno *tile* das matrizes de entrada, permitindo que os dados relevantes sejam carregados na memória compartilhada e reutilizados intensivamente antes de passar para a próxima fase [^111].

Para ilustrar, considere o kernel de multiplicação de matrizes tiled apresentado na Figura 5.12 [^112]. As linhas 1 e 2 declaram `Mds` e `Nds` como variáveis de memória compartilhada:

```c++
_shared_ float Mds[TILE_WIDTH][TILE_WIDTH];
_shared_ float Nds[TILE_WIDTH][TILE_WIDTH];
```

Como mencionado, o escopo das variáveis de memória compartilhada é o bloco [^112]. Isso significa que um par de `Mds` e `Nds` é criado para cada bloco, e todos os *threads* dentro do bloco têm acesso a esses *arrays*. A colaboração entre os *threads* é essencial para carregar os valores de M e N em `Mds` e `Nds`, garantindo que todos os *threads* tenham acesso aos dados necessários [^112].

No loop principal (linha 8 da Figura 5.12), os *tiles* de M e N são carregados colaborativamente na memória compartilhada [^112]:

```c++
for (int m = 0; m < Width/TILE_WIDTH; ++m) {
    Mds[ty][tx] = d_M[Row*Width + m*TILE_WIDTH + tx];
    Nds[ty][tx] = d_N[(m*TILE_WIDTH + ty) *Width + Col];
    __syncthreads();
    ...
}
```

A função `__syncthreads()` [^112] atua como uma barreira, garantindo que todos os *threads* no bloco tenham completado o carregamento dos dados antes de prosseguir para a próxima fase da computação.

Após o carregamento, os valores em `Mds` e `Nds` são utilizados para calcular o produto escalar:

```c++
for (int k = 0; k < TILE_WIDTH; ++k) {
    Pvalue += Mds[ty][k] * Nds[k][tx];
}
__syncthreads();
```

A reutilização de `Mds` e `Nds` se manifesta no fato de que, em cada iteração do loop externo (variável `m`), os mesmos *arrays* de memória compartilhada são usados para armazenar um novo *tile* de M e N [^111]. Essa estratégia minimiza a quantidade total de memória compartilhada necessária, permitindo que mais blocos residam simultaneamente em cada Multiprocessador de *Streaming* (SM), aumentando o paralelismo e a eficiência [^115].

### Conclusão

A reutilização de `Mds` e `Nds` em conjunto com o *tiling* é uma técnica poderosa para otimizar a multiplicação de matrizes em CUDA [^111]. Ao explorar a localidade dos dados e reutilizar a memória compartilhada, é possível reduzir significativamente o tráfego para a memória global, aumentar o paralelismo e alcançar um desempenho próximo ao pico teórico do dispositivo [^115]. No entanto, é crucial considerar as limitações de capacidade da memória compartilhada e o número de *threads* que podem residir simultaneamente em cada SM [^115], [^116] para garantir que a otimização seja efetiva e não se torne um gargalo.

### Referências
[^111]: Seção 5.4, "A Tiled Matrix-Matrix Multiplication Kernel".
[^112]: Figura 5.12, "Tiled matrix multiplication kernel using shared memory".
[^105]: Seção 5.3, "A Strategy for Reducing Global Memory Traffic".
[^115]: Seção 5.5, "Memory as a Limiting Factor to Parallelism".
[^116]: Capítulo 5, CUDA Memories.
<!-- END -->