## Redução do Tráfego de Memória Global Através do Uso de Memória Compartilhada em Multiplicação de Matrizes com Tiles

### Introdução
Em kernels CUDA, a eficiência no acesso à memória é crucial para o desempenho. A memória global, embora grande, possui alta latência e largura de banda limitada [^95]. Para mitigar esse gargalo, uma estratégia comum é particionar os dados em *tiles*, que são então carregados na memória compartilhada, uma memória on-chip de baixa latência e alta largura de banda [^105]. Este capítulo detalha como essa técnica é aplicada no contexto da multiplicação de matrizes, especificamente no kernel *Tiled Matrix-Matrix Multiplication*.

### Conceitos Fundamentais

#### Acesso à Memória e CGMA
Como vimos anteriormente [^96], a razão entre o cálculo de ponto flutuante e o acesso à memória global (CGMA - Compute to Global Memory Access ratio) impacta significativamente o desempenho. Em uma implementação direta da multiplicação de matrizes, cada iteração do loop interno requer dois acessos à memória global: um para um elemento de cada matriz [^96]. Isso resulta em um CGMA baixo, limitando o desempenho do kernel.

#### A Estratégia de Tiling
A técnica de *tiling* envolve a divisão das matrizes de entrada em subconjuntos menores, chamados *tiles*, que podem ser acomodados na memória compartilhada [^105]. A computação do kernel é então realizada nesses *tiles* de forma independente [^105]. Essa abordagem permite que os dados sejam carregados uma vez na memória compartilhada e reutilizados várias vezes pelos threads dentro de um bloco, reduzindo o número total de acessos à memória global.

#### Implementação do Kernel Tiled
No kernel *Tiled Matrix-Matrix Multiplication*, os threads colaboram para carregar os elementos das matrizes M e N na memória compartilhada antes de realizar seus cálculos de produto escalar individuais [^109]. O tamanho dos *tiles* é escolhido de forma que caibam na memória compartilhada, o que requer atenção ao tamanho limitado dessa memória [^109].

#### Fases de Execução e Sincronização
A execução do kernel é dividida em *fases*. Em cada fase, todos os threads em um bloco colaboram para carregar um *tile* de elementos de M e um *tile* de elementos de N na memória compartilhada [^110]. Isso é feito atribuindo a cada thread em um bloco o carregamento de um elemento de M e um elemento de N na memória compartilhada [^110]. A sincronização através de `__syncthreads()` [^114] garante que todos os threads tenham concluído o carregamento dos *tiles* antes de prosseguir com os cálculos. Após o carregamento, os valores são usados no cálculo do produto escalar, com cada valor na memória compartilhada sendo usado várias vezes [^111].

#### Redução de Acessos à Memória Global
Ao carregar cada valor da memória global para a memória compartilhada para que possa ser usado várias vezes, o número de acessos à memória global é reduzido. Neste caso, o número de acessos à memória global é reduzido pela metade [^106]. Se os *tiles* são $N \\times N$ elementos, a redução potencial no tráfego de memória global seria $N$ [^111]. No kernel apresentado na Figura 5.12 [^112], se utilizarmos *tiles* de 16x16, podemos reduzir os acessos à memória global por um fator de 16 [^115], aumentando significativamente o CGMA.

#### Código do Kernel Tiled
O código do kernel tiled é apresentado na Figura 5.12 [^112]. As linhas 1 e 2 declaram `Mds` e `Nds` como variáveis de memória compartilhada [^112]. As linhas 3 e 4 armazenam os valores `threadIdx` e `blockIdx` em variáveis automáticas para acesso rápido [^112]. As linhas 5 e 6 determinam o índice de linha e coluna do elemento $d\\_P$ que o thread irá produzir [^113]. A linha 8 marca o início do loop que itera através de todas as fases de cálculo do elemento $d\\_P$ final [^113]. A linha 9 carrega o elemento $d\\_M$ apropriado na memória compartilhada [^113]. A função barreira `__syncthreads()` na linha 11 garante que todos os threads tenham terminado de carregar os *tiles* de $d\\_M$ e $d\\_N$ em `Mds` e `Nds` antes que qualquer um deles possa seguir em frente [^114]. O loop na linha 12 então realiza uma fase do produto escalar baseado nesses elementos do *tile* [^114]. A função barreira `__syncthreads()` na linha 14 garante que todos os threads tenham terminado de usar os elementos $d\\_M$ e $d\\_N$ na memória compartilhada antes que qualquer um deles avance para a próxima iteração e carregue os elementos nos próximos *tiles* [^115].

#### Localidade
A reutilização de dados na memória compartilhada explora o princípio da *localidade* [^111]. Como cada fase se concentra em um pequeno subconjunto dos dados da matriz de entrada, os threads podem carregar colaborativamente o subconjunto na memória compartilhada e usar os valores na memória compartilhada para satisfazer suas necessidades de entrada sobrepostas na fase [^111].

### Conclusão
Ao empregar a técnica de *tiling* e utilizar a memória compartilhada para armazenar *tiles* de dados, o kernel *Tiled Matrix-Matrix Multiplication* reduz significativamente o número de acessos à memória global. Essa redução resulta em um aumento no CGMA, permitindo que o kernel atinja um desempenho muito maior em comparação com implementações mais simples. A sincronização adequada entre os threads é crucial para garantir a consistência dos dados e a execução correta do kernel. A consideração cuidadosa do tamanho dos *tiles* e da capacidade da memória compartilhada é essencial para otimizar o desempenho e evitar gargalos de memória.
### Referências
[^95]: Capítulo 5, página 95
[^96]: Capítulo 5, página 96
[^105]: Capítulo 5, página 105
[^106]: Capítulo 5, página 106
[^109]: Capítulo 5, página 109
[^110]: Capítulo 5, página 110
[^111]: Capítulo 5, página 111
[^112]: Capítulo 5, página 112
[^113]: Capítulo 5, página 113
[^114]: Capítulo 5, página 114
[^115]: Capítulo 5, página 115
$\blacksquare$
<!-- END -->