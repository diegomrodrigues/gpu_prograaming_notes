## Collaborative Loading of Tiles into Shared Memory in Tiled Matrix Multiplication

### Introdução
Este capítulo explora a otimização da multiplicação de matrizes por meio do uso eficiente da memória compartilhada em CUDA. Como discutido anteriormente, a memória global possui alta latência e largura de banda limitada, o que pode restringir o desempenho do kernel [^95]. Para mitigar esse problema, emprega-se uma estratégia de *tiling* (divisão em blocos) para particionar os dados em subconjuntos menores que caibam na memória compartilhada, que é mais rápida [^105]. Este capítulo se concentrará no processo de carregamento colaborativo desses tiles na memória compartilhada.

### Conceitos Fundamentais
A estratégia de *tiling* envolve dividir as matrizes de entrada M e N em *tiles* menores, de tamanho apropriado para caber na memória compartilhada [^109]. O objetivo é que os *threads* em um bloco cooperem para carregar esses *tiles* na memória compartilhada antes de realizar os cálculos do produto escalar [^109]. Isso reduz o número de acessos à memória global, resultando em melhor desempenho.

**Carregamento Colaborativo:**
Em cada fase do algoritmo de multiplicação de matrizes *tiled*, todos os *threads* em um bloco colaboram para carregar um *tile* de elementos M e um *tile* de elementos N na memória compartilhada [^110]. Cada *thread* no bloco é responsável por carregar um elemento M e um elemento N na memória compartilhada [^110].

Para ilustrar, considere uma configuração onde as matrizes M e N são divididas em *tiles* de 2x2 [^109]. Na Figura 5.11 [^110], cada linha representa a execução de um *thread*. No início da fase 1, os quatro *threads* do bloco (0,0) carregam colaborativamente um *tile* de elementos M na memória compartilhada (Mds):

*   thread(0,0) carrega M0,0 em Mds0,0
*   thread(0,1) carrega M0,1 em Mds0,1
*   thread(1,0) carrega M1,0 em Mds1,0
*   thread(1,1) carrega M1,1 em Mds1,1

Um processo similar ocorre para carregar um *tile* de elementos N na memória compartilhada (Nds) [^111]. Após os *tiles* de M e N serem carregados na memória compartilhada, esses valores são usados para o cálculo do produto escalar. Cada valor na memória compartilhada é usado múltiplas vezes, reduzindo o número de acessos à memória global [^111]. No exemplo, o valor M1,1 carregado pelo *thread* (1,1) em Mds1,1 é usado duas vezes: uma pelo *thread* (0,1) e outra pelo *thread* (1,1) [^111].

**Implementação no Kernel CUDA:**
O kernel CUDA para multiplicação de matrizes *tiled* (Figura 5.12) implementa as fases ilustradas na Figura 5.11 [^112]. As linhas 1 e 2 declaram Mds e Nds como variáveis de memória compartilhada [^112]. O escopo dessas variáveis é o bloco, o que significa que um par de Mds e Nds é criado para cada bloco, e todos os *threads* dentro desse bloco têm acesso a ele [^112].

```c++
__global__ void MatrixMulKernel(float* d_M, float* d_N, float* d_P, int Width) {
    __shared__ float Mds[TILE_WIDTH][TILE_WIDTH];
    __shared__ float Nds[TILE_WIDTH][TILE_WIDTH];

    int bx = blockIdx.x; int by = blockIdx.y;
    int tx = threadIdx.x; int ty = threadIdx.y;

    int Row = by * TILE_WIDTH + ty;
    int Col = bx * TILE_WIDTH + tx;

    float Pvalue = 0;

    for (int m = 0; m < Width / TILE_WIDTH; ++m) {
        Mds[ty][tx] = d_M[Row * Width + m * TILE_WIDTH + tx];
        Nds[ty][tx] = d_N[(m * TILE_WIDTH + ty) * Width + Col];
        __syncthreads();

        for (int k = 0; k < TILE_WIDTH; ++k) {
            Pvalue += Mds[ty][k] * Nds[k][tx];
        }
        __syncthreads();
    }
    d_P[Row * Width + Col] = Pvalue;
}
```

As linhas 9 e 10 realizam o carregamento colaborativo dos *tiles* de d_M e d_N na memória compartilhada:

```c++
Mds[ty][tx] = d_M[Row * Width + m * TILE_WIDTH + tx];
Nds[ty][tx] = d_N[(m * TILE_WIDTH + ty) * Width + Col];
__syncthreads();
```

A função `__syncthreads()` garante que todos os *threads* tenham terminado de carregar os *tiles* de d_M e d_N em Mds e Nds antes que qualquer um deles prossiga [^114]. O *loop* na linha 12 realiza uma fase do produto escalar com base nesses elementos do *tile* [^114].

**Sincronização:**
A sincronização é crucial para o correto funcionamento do carregamento colaborativo. A função `__syncthreads()` garante que todos os *threads* em um bloco tenham concluído o carregamento dos dados na memória compartilhada antes que qualquer *thread* comece a ler esses dados [^114]. Isso evita condições de corrida e garante que os cálculos sejam realizados com os dados corretos. Sem a sincronização adequada, alguns *threads* podem tentar ler dados da memória compartilhada antes que outros *threads* os tenham escrito, levando a resultados incorretos.

**Localidade:**
O uso de memória compartilhada e o carregamento colaborativo promovem a localidade dos dados [^111]. Ao carregar os *tiles* na memória compartilhada, os *threads* podem acessar os mesmos dados várias vezes sem precisar acessar a memória global, que é mais lenta [^111]. Isso melhora significativamente o desempenho do kernel.

### Conclusão
O carregamento colaborativo de *tiles* na memória compartilhada é uma técnica fundamental para otimizar a multiplicação de matrizes em CUDA [^111]. Essa técnica reduz o número de acessos à memória global, promove a localidade dos dados e permite que os *threads* trabalhem em conjunto de forma eficiente [^111]. A sincronização adequada é essencial para garantir a correção dos resultados [^114]. Ao compreender e aplicar esses conceitos, é possível obter ganhos significativos de desempenho em aplicações CUDA.

### Referências
[^95]: Capítulo 5, página 95
[^105]: Capítulo 5, página 105
[^109]: Capítulo 5, página 109
[^110]: Capítulo 5, página 110
[^111]: Capítulo 5, página 111
[^112]: Capítulo 5, página 112
[^114]: Capítulo 5, página 114
<!-- END -->