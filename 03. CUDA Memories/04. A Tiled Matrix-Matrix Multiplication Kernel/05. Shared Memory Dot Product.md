## Uso Eficiente da Memória Compartilhada na Multiplicação de Matrizes com Tiling

### Introdução
No capítulo anterior, exploramos a importância da eficiência no acesso à memória e como a memória global, apesar de sua capacidade, pode se tornar um gargalo devido à sua alta latência [^95]. Para mitigar esse problema, CUDA oferece a memória compartilhada, uma memória on-chip de acesso rápido que pode ser utilizada para reduzir o tráfego na memória global [^97]. Neste capítulo, aprofundaremos o conceito de tiling na multiplicação de matrizes, com foco em como os dados carregados na memória compartilhada são reutilizados para otimizar o desempenho [^109].

### Conceitos Fundamentais

A técnica de **tiling** consiste em dividir os dados em subconjuntos menores, denominados *tiles*, que podem ser acomodados na memória compartilhada [^105]. No contexto da multiplicação de matrizes, isso significa dividir as matrizes de entrada $M$ e $N$ em tiles menores.

Após o carregamento dos tiles de $M$ e $N$ na memória compartilhada, esses valores são utilizados no cálculo do produto escalar [^109]. Cada valor na memória compartilhada é usado duas vezes [^109]. Para ilustrar esse ponto, considere a Figura 5.11 [^110], que demonstra as fases de execução de uma multiplicação de matrizes com tiling.

Na fase 1, os threads do bloco 0,0 carregam colaborativamente um tile de elementos de $M$ e um tile de elementos de $N$ na memória compartilhada [^111]. Especificamente, o thread 0,0 carrega $M_{0,0}$ em $Mds_{0,0}$, o thread 0,1 carrega $M_{0,1}$ em $Mds_{0,1}$, e assim por diante [^111]. Um tile de elementos $N$ também é carregado de maneira similar [^111].

Após o carregamento dos dois tiles de elementos $M$ e $N$ na memória compartilhada, esses valores são utilizados no cálculo do produto escalar [^111]. Cada valor na memória compartilhada é usado duas vezes [^109]. Por exemplo, o valor $M_{1,1}$, carregado pelo thread 1,1 em $Mds_{1,1}$, é usado duas vezes: uma vez pelo thread 0,1 e outra pelo thread 1,1 [^111]. Ao carregar cada valor da memória global na memória compartilhada para que possa ser usado várias vezes, reduzimos o número de acessos à memória global [^111]. Nesse caso, reduzimos o número de acessos à memória global pela metade [^111]. Os leitores devem verificar que a redução é por um fator de $N$ se os tiles forem elementos $N \times N$ [^111].

É importante observar que o cálculo de cada produto escalar na Figura 5.6 [^106] agora é realizado em duas fases, como mostrado nas fases 1 e 2 na Figura 5.11 [^110]. Em cada fase, os produtos de dois pares dos elementos da matriz de entrada são acumulados na variável $Pvalue$ [^111]. Observe que $Pvalue$ é uma variável automática, de modo que uma versão privada é gerada para cada thread [^111]. Adicionamos subscritos apenas para esclarecer que essas são instâncias diferentes da variável $Pvalue$ criada para cada thread [^111]. O cálculo da primeira fase é mostrado na quarta coluna da Figura 5.11 [^110]; o cálculo da segunda fase está na sétima coluna [^111]. Em geral, se uma matriz de entrada tiver dimensão $N$ e o tamanho do tile for $TILE\_WIDTH$, o produto escalar seria realizado em fases $N/TILE\_WIDTH$ [^111]. A criação dessas fases é fundamental para a redução dos acessos à memória global [^111].

Com cada fase focada em um pequeno subconjunto dos valores da matriz de entrada, os threads podem carregar colaborativamente o subconjunto na memória compartilhada e usar os valores na memória compartilhada para satisfazer suas necessidades de entrada sobrepostas na fase [^111].

Observe também que $Mds$ e $Nds$ são reutilizados para armazenar os valores de entrada [^111]. Em cada fase, os mesmos $Mds$ e $Nds$ são usados para armazenar o subconjunto de elementos $M$ e $N$ usados na fase [^111]. Isso permite que uma memória compartilhada muito menor atenda à maioria dos acessos à memória global [^111]. Isso ocorre porque cada fase se concentra em um pequeno subconjunto dos elementos da matriz de entrada [^111]. Esse comportamento de acesso focado é chamado de *localidade* [^111]. Quando um algoritmo exibe localidade, há uma oportunidade de usar memórias pequenas e de alta velocidade para atender à maioria dos acessos e remover esses acessos da memória global [^112].

### Conclusão

A reutilização de dados na memória compartilhada é um componente crucial da otimização da multiplicação de matrizes com tiling [^111]. Ao garantir que cada valor carregado na memória compartilhada seja utilizado múltiplas vezes, o número de acessos à memória global é significativamente reduzido, levando a um aumento substancial no desempenho [^111]. A escolha do tamanho do tile, a sincronização entre os threads e o gerenciamento da capacidade da memória compartilhada são aspectos críticos para o sucesso dessa técnica [^111].

### Referências
[^95]: Capítulo 5, página 95
[^97]: Capítulo 5, página 97
[^105]: Capítulo 5, página 105
[^106]: Capítulo 5, página 106
[^109]: Capítulo 5, página 109
[^110]: Capítulo 5, página 110
[^111]: Capítulo 5, página 111
[^112]: Capítulo 5, página 112
<!-- END -->