## Tiled Algorithms as Carpooling for Memory Access

### Introdução
Este capítulo explora uma estratégia crucial para otimizar o acesso à memória global em CUDA, concentrando-se na técnica de *tiling* e sua analogia com o conceito de *carpooling* [^1]. A eficiência do acesso à memória é fundamental para o desempenho de kernels CUDA, e o *tiling* surge como uma abordagem eficaz para mitigar a latência e o gargalo da largura de banda da memória global [^1]. Conforme discutido no capítulo anterior, a memória global, implementada tipicamente com DRAM, apresenta latências elevadas e largura de banda limitada [^1]. Para contornar essas limitações, o *tiling* visa reduzir o tráfego para a memória global, explorando a localidade dos dados e a colaboração entre *threads* [^1].

### Conceitos Fundamentais

O *tiling* é uma técnica que particiona os dados em subconjuntos menores, denominados *tiles*, que podem ser acomodados na memória compartilhada, que é menor e mais rápida [^1, 105]. A analogia com o *carpooling* surge da observação de que, se vários *threads* acessarem os mesmos dados na memória global, eles podem "compartilhar a carona" e combinar seus acessos em uma única requisição à DRAM [^1, 108].

[^108] define que *algoritmos tiled são muito similares a arranjos de carona. Podemos pensar nos valores de dados acessados por cada thread como passageiros e DRAM requisitada como veículos.*

A eficiência do *carpooling* depende da similaridade nos padrões de acesso dos *threads*. Se os *threads* acessarem os mesmos dados em momentos próximos, a combinação dos acessos é straightforward. No entanto, se os acessos forem distribuídos no tempo, a técnica perde sua eficácia [^1, 108].

**Congestionamento e Largura de Banda:**
Quando a taxa de requisições à DRAM excede a largura de banda disponível, ocorre congestionamento, resultando em ociosidade das unidades aritméticas [^1, 108]. O *tiling*, ao reduzir o número total de acessos à memória global, pode aliviar esse congestionamento e melhorar o desempenho geral [^1, 108].

**Implementação do Tiling:**
A implementação do *tiling* envolve a divisão do kernel em fases, onde os *threads* colaboram para carregar *tiles* de dados na memória compartilhada antes de realizar seus cálculos individuais [^1, 109]. Isso requer sincronização entre os *threads* para garantir que todos os dados necessários estejam disponíveis na memória compartilhada antes do início dos cálculos [^1, 111].

**Exemplo: Multiplicação de Matrizes:**
O capítulo utiliza a multiplicação de matrizes como um exemplo para ilustrar os benefícios do *tiling* [^1, 105, 109]. Ao dividir as matrizes em *tiles* menores, os *threads* podem carregar os *tiles* relevantes na memória compartilhada e realizar as multiplicações, reduzindo significativamente o número de acessos à memória global [^1, 105, 109].

**Localidade dos Dados:**
O *tiling* explora a localidade dos dados, que é a tendência de um *thread* acessar os mesmos dados repetidamente em um curto período de tempo [^1, 111, 112]. Ao carregar os dados na memória compartilhada, os *threads* podem acessar os dados localmente, evitando o acesso frequente à memória global [^1, 111, 112].

**Considerações sobre a Memória Compartilhada:**
É crucial considerar a capacidade limitada da memória compartilhada ao implementar o *tiling* [^1, 109, 115]. O tamanho dos *tiles* deve ser escolhido de forma que caibam na memória compartilhada, e é importante evitar exceder a capacidade da memória compartilhada, o que pode limitar o número de *threads* que podem ser executados simultaneamente [^1, 115].

**Código de Exemplo:**
O capítulo apresenta um código de exemplo de um kernel de multiplicação de matrizes com *tiling* [^1, 112]. O código demonstra como os *threads* colaboram para carregar os *tiles* na memória compartilhada, como os cálculos são realizados usando os dados na memória compartilhada e como a sincronização é usada para garantir a correção [^1, 112].

```c++
__global__ void MatrixMulKernel(float* d_M, float* d_N, float* d_P, int Width) {
    __shared__ float Mds[TILE_WIDTH][TILE_WIDTH];
    __shared__ float Nds[TILE_WIDTH][TILE_WIDTH];

    int bx = blockIdx.x; int by = blockIdx.y;
    int tx = threadIdx.x; int ty = threadIdx.y;

    int Row = by * TILE_WIDTH + ty;
    int Col = bx * TILE_WIDTH + tx;

    float Pvalue = 0;

    for (int m = 0; m < Width/TILE_WIDTH; ++m) {
        Mds[ty][tx] = d_M[Row*Width + m*TILE_WIDTH + tx];
        Nds[ty][tx] = d_N[(m*TILE_WIDTH + ty)*Width + Col];
        __syncthreads();

        for (int k = 0; k < TILE_WIDTH; ++k) {
            Pvalue += Mds[ty][k] * Nds[k][tx];
        }
        __syncthreads();
    }
    d_P[Row*Width + Col] = Pvalue;
}
```

### Conclusão

O *tiling* é uma técnica essencial para otimizar o acesso à memória global em CUDA, permitindo a redução do tráfego para a memória global e o aproveitamento da localidade dos dados [^1, 105, 111]. Ao particionar os dados em *tiles* menores e utilizar a memória compartilhada, é possível melhorar significativamente o desempenho dos kernels CUDA [^1, 105, 111]. A analogia com o *carpooling* ilustra a importância da colaboração entre *threads* para combinar acessos à memória e reduzir o congestionamento [^1, 108].  A utilização eficaz do *tiling* é fundamental para alcançar o máximo desempenho em aplicações CUDA [^1, 118].

### Referências
[^1]: Capítulo 5 do texto fornecido.

<!-- END -->