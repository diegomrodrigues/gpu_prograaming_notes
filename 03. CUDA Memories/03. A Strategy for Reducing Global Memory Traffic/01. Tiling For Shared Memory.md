## Tiled Algorithms for Reducing Global Memory Traffic

### Introdução
Em CUDA, existe um *trade-off* inerente entre a memória global, que é grande mas lenta, e a memória compartilhada, que é pequena mas rápida [^11]. Uma estratégia comum para mitigar o problema da latência da memória global é particionar os dados em subconjuntos chamados **tiles**, de forma que cada *tile* caiba na memória compartilhada [^11]. O termo "*tile*" deriva da analogia de que uma grande parede (ou seja, os dados na memória global) pode ser coberta por "*tiles*" (ou seja, subconjuntos que cabem na memória compartilhada) [^11]. Este capítulo explora em detalhes essa estratégia, demonstrando como ela pode ser aplicada para otimizar o acesso à memória em *kernels* CUDA.

### Conceitos Fundamentais

A eficiência do acesso à memória é crucial para o desempenho de *kernels* CUDA. A memória global, implementada com DRAM, possui longas latências de acesso e largura de banda finita [^1]. Para ilustrar o impacto da eficiência do acesso à memória, considere o *kernel* de multiplicação de matrizes na Figura 5.1 [^2]. Cada iteração do loop interno realiza duas operações de acesso à memória global para cada operação de ponto flutuante [^2].  Uma métrica importante é a razão **compute to global memory access (CGMA)**, definida como o número de cálculos de ponto flutuante realizados para cada acesso à memória global [^2].

Em dispositivos de ponta, a largura de banda da memória global é de aproximadamente 200 GB/s [^2]. Com valores de ponto flutuante de precisão simples ocupando 4 bytes, pode-se esperar carregar no máximo 50 GFLOPS [^2]. Para atingir o desempenho máximo de 1.500 GFLOPS, seria necessário um valor CGMA de 30 [^3].

CUDA oferece vários tipos de memória, incluindo registradores e memória compartilhada (*on-chip memories*), que permitem acesso em alta velocidade e de forma altamente paralela [^3].  A memória compartilhada é alocada para blocos de *threads*, permitindo que todos os *threads* em um bloco acessem as variáveis alocadas nesses locais [^4].

#### Tiling e Memória Compartilhada

O conceito de *tiling* envolve particionar os dados em subconjuntos (tiles) que se encaixam na memória compartilhada [^11]. O *kernel* é então projetado para realizar computações nesses *tiles* de forma independente [^11]. Nem todas as estruturas de dados podem ser particionadas em *tiles*, dependendo da função do *kernel* [^11].

Para ilustrar, considere o exemplo de multiplicação de matrizes na Figura 5.5 [^11].  O exemplo assume o uso de quatro blocos 2x2 para calcular a matriz P. A figura destaca a computação realizada pelos quatro *threads* do bloco(0,0). Os acessos aos elementos M e N pelos *threads* (0,0) e (0,1) do bloco(0,0) são destacados com setas pretas [^11].

No *kernel* original (Figura 5.1) [^2], cada *thread* acessa elementos de M e N da memória global. Se os *threads* puderem colaborar para carregar esses elementos na memória compartilhada apenas uma vez, o número total de acessos à memória global pode ser reduzido pela metade [^12]. Em geral, cada elemento M e N é acessado duas vezes durante a execução do bloco(0,0) [^12]. Se todos os quatro *threads* colaborarem, o tráfego para a memória global pode ser reduzido pela metade [^12].

A redução potencial no tráfego de memória global é proporcional à dimensão dos blocos usados [^12]. Com blocos $N \\times N$, a redução potencial seria N. Por exemplo, com blocos 16x16, o tráfego pode ser reduzido para 1/16 [^12].

#### Sincronização e Localidade

Os algoritmos de *tiling* são análogos a acordos de *carpooling* [^14]. Os valores de dados acessados por cada *thread* são como passageiros, e os acessos DRAM são como veículos. Se vários *threads* acessarem dados do mesmo local DRAM, eles podem formar um "*carpool*" e combinar seus acessos em uma única solicitação DRAM [^14]. Isso requer que os *threads* tenham um cronograma de execução semelhante [^14].

A Figura 5.9 [^15] ilustra como os *threads* devem acessar os mesmos elementos de dados com *timing* semelhante para permitir o *carpooling* eficaz. A seção superior mostra dois *threads* acessando os mesmos elementos de dados com *timing* semelhante, enquanto a seção inferior mostra *threads* acessando dados comuns em momentos muito diferentes [^15].

O *kernel tiled* divide a computação em fases [^15]. Em cada fase, todos os *threads* em um bloco colaboram para carregar um *tile* de elementos M e um *tile* de elementos N na memória compartilhada [^15]. Cada *thread* carrega um elemento M e um elemento N na memória compartilhada [^15]. Após os *tiles* serem carregados, os valores são usados no cálculo do produto escalar [^15]. Cada valor na memória compartilhada é usado várias vezes, reduzindo o número de acessos à memória global [^15].

Os *kernels tiled* exploram a **localidade** dos dados, permitindo que uma memória compartilhada menor atenda à maioria dos acessos à memória global [^17].

#### Kernel Tiled para Multiplicação de Matrizes

A Figura 5.12 [^18] mostra um *kernel tiled* para multiplicação de matrizes. As linhas 1 e 2 declaram `Mds` e `Nds` como variáveis de memória compartilhada [^18]. O escopo das variáveis de memória compartilhada é um bloco [^18]. Todas as *threads* em um bloco devem ter acesso aos valores M e N carregados em `Mds` e `Nds` por seus pares [^18].

As linhas 3 e 4 salvam os valores `threadIdx` e `blockIdx` em variáveis automáticas para acesso rápido [^18]. As linhas 5 e 6 determinam o índice da linha e o índice da coluna do elemento `d_P` que o *thread* deve produzir [^19]. A linha 8 marca o início do *loop* que itera por todas as fases do cálculo final do elemento `d_P` [^19]. A variável `m` indica o número de fases já concluídas [^19].

A linha 9 carrega o elemento `d_M` apropriado na memória compartilhada [^19]. Cada bloco possui `TILE_WIDTH` *threads* que colaboram para carregar `TILE_WIDTH` elementos `d_M` na memória compartilhada [^19]. Cada *thread* é atribuído para carregar um elemento `d_M`, o que é feito usando `blockIdx` e `threadIdx` [^19].

A barreira `__syncthreads()` na linha 11 garante que todos os *threads* tenham terminado de carregar os *tiles* de `d_M` e `d_N` em `Mds` e `Nds` antes que qualquer um deles possa avançar [^20]. O *loop* na linha 12 então executa uma fase do produto escalar com base nesses elementos do *tile* [^20]. A barreira `__syncthreads()` na linha 14 garante que todos os *threads* tenham terminado de usar os elementos `d_M` e `d_N` na memória compartilhada antes que qualquer um deles avance para a próxima iteração e carregue os elementos nos próximos *tiles* [^21].

O benefício do algoritmo *tiled* é substancial [^21]. Para multiplicação de matrizes, os acessos à memória global são reduzidos por um fator de `TILE_WIDTH` [^21]. Com *tiles* 16x16, é possível reduzir os acessos à memória global por um fator de 16 [^21]. Isso aumenta o CGMA de 1 para 16 [^21]. Essa melhoria permite que a largura de banda da memória de um dispositivo CUDA suporte uma taxa de computação próxima ao seu desempenho de pico [^21].

### Conclusão
A estratégia de *tiling* representa uma abordagem eficaz para reduzir o tráfego de memória global em *kernels* CUDA [^11]. Ao particionar os dados em *tiles* menores que cabem na memória compartilhada e otimizar o acesso aos dados dentro desses *tiles*, é possível mitigar a latência da memória global e melhorar significativamente o desempenho [^11]. O uso de *tiling* requer uma compreensão cuidadosa das características da arquitetura CUDA, como a hierarquia de memória e a sincronização de *threads* [^1, 4]. Ao aplicar esses princípios, os desenvolvedores podem criar *kernels* CUDA altamente otimizados para uma variedade de aplicações computacionalmente intensivas [^17].

### Referências
[^1]: Capítulo 5, página 95.
[^2]: Capítulo 5, página 96.
[^3]: Capítulo 5, página 97.
[^4]: Capítulo 5, página 98.
[^11]: Capítulo 5, página 105.
[^12]: Capítulo 5, página 106.
[^14]: Capítulo 5, página 108.
[^15]: Capítulo 5, página 109.
[^17]: Capítulo 5, página 111.
[^18]: Capítulo 5, página 112.
[^19]: Capítulo 5, página 113.
[^20]: Capítulo 5, página 114.
[^21]: Capítulo 5, página 115.
<!-- END -->