## Global Memory in CUDA

### Introdução
No contexto da programação CUDA, a eficiência do acesso à memória é crucial para o desempenho das aplicações. Conforme introduzido no início deste capítulo [^1], kernels CUDA simples podem alcançar apenas uma pequena fração da velocidade potencial do hardware subjacente devido às características da **global memory**. A global memory, sendo tipicamente implementada com DRAM, apresenta longas latências de acesso e largura de banda finita [^1]. Este capítulo visa detalhar as características da global memory e como ela se compara com outros tipos de memória no modelo de programação CUDA, como os **registros**, com o objetivo de otimizar o desempenho.

### Conceitos Fundamentais
A **global memory** no modelo de programação CUDA corresponde à memória do modelo de von Neumann e é implementada com tecnologia DRAM off-chip [^4]. Esta implementação resulta em latências de acesso mais longas e largura de banda relativamente baixa [^4]. Para ilustrar a importância do acesso eficiente à memória, considere o kernel de multiplicação de matrizes apresentado na Figura 5.1 [^2]. A parte mais importante deste kernel, em termos de tempo de execução, é o loop `for` que realiza o cálculo do produto interno [^2].

```c++
for (int k = 0; k < Width; ++k)
    Pvalue += d_M[Row*Width + k] * d_N[k*Width + Col];
```

Em cada iteração deste loop, dois acessos à global memory são realizados para uma multiplicação e uma adição de ponto flutuante [^2]. Um acesso busca um elemento `d_M[]` e o outro busca um elemento `d_N[]` [^2].  A razão entre o cálculo de ponto flutuante e a operação de acesso à global memory é de 1:1, ou 1.0 [^2]. Esta razão é referida como a **compute to global memory access (CGMA) ratio**, definida como o número de cálculos de ponto flutuante realizados para cada acesso à global memory dentro de uma região de um programa CUDA [^2].

A CGMA tem implicações significativas no desempenho de um kernel CUDA [^2]. Em um dispositivo de ponta, a largura de banda da global memory é de cerca de 200 GB/s [^2]. Com 4 bytes em cada valor de ponto flutuante de precisão simples, pode-se esperar carregar não mais que 50 (200/4) giga operandos de precisão simples por segundo [^2]. Com uma CGMA de 1.0, o kernel de multiplicação de matrizes executará não mais que 50 giga operações de ponto flutuante por segundo (GFLOPS) [^3]. Embora 50 GFLOPS seja um número respeitável, é apenas uma pequena fração do desempenho de pico de precisão simples de 1.500 GFLOPS ou superior para dispositivos de ponta [^3]. Para aumentar o desempenho do kernel, é necessário aumentar a CGMA [^3].

Em contraste, a aritmética com operandos em **registros** não requer instruções adicionais para tornar o valor do operando disponível para a unidade lógica aritmética (ALU), economizando tempo de processamento [^4]. Em um exemplo típico de instrução de adição de ponto flutuante, a instrução é da forma `fadd r1, r2, r3`, onde `r2` e `r3` são os números dos registros que especificam a localização no arquivo de registros onde os valores dos operandos de entrada podem ser encontrados [^5]. A localização para armazenar o valor do resultado da adição de ponto flutuante é especificada por `r1` [^5]. Portanto, quando um operando de uma instrução aritmética está em um registro, não há necessidade de uma instrução adicional para tornar o valor do operando disponível para a ALU [^5].

Por outro lado, um valor de operando na global memory requer uma operação de *memory load* para estar disponível para a ALU, envolvendo mais instruções e demorando mais para processar [^4]. Por exemplo, se o primeiro operando de uma instrução de adição de ponto flutuante está na global memory, as instruções envolvidas provavelmente serão [^6]:

```
load r2, r4, offset
fadd r1, r2, r3
```

A instrução `load` adiciona um valor de *offset* ao conteúdo de `r4` para formar um endereço para o valor do operando [^6]. Em seguida, acessa a global memory e coloca o valor no registro `r2` [^6]. A instrução `fadd` então realiza a adição de ponto flutuante usando os valores em `r2` e `r3` e coloca o resultado em `r1` [^6]. Como o processador só pode buscar e executar um número limitado de instruções por ciclo de clock, a versão com um *load* adicional provavelmente levará mais tempo para processar do que a sem. Esta é outra razão pela qual colocar os operandos em registros pode melhorar a velocidade de execução [^6].

Além disso, a energia consumida para acessar um valor do arquivo de registros é pelo menos uma ordem de magnitude menor do que para acessar um valor da global memory [^6]. No entanto, o número de registros disponíveis para cada thread é bastante limitado nas GPUs de hoje [^6]. Portanto, é preciso ter cuidado para não superinscrever esse recurso limitado [^6].

### Conclusão
A global memory é essencial no modelo de programação CUDA, mas suas características de alta latência e baixa largura de banda podem limitar o desempenho. O uso eficiente dos registros, em comparação, oferece um acesso mais rápido e menor consumo de energia, mas com recursos limitados [^6]. Entender essas diferenças e otimizar o uso da memória é crucial para alcançar o máximo desempenho em aplicações CUDA. As técnicas de tiling, que serão exploradas em seções posteriores [^15, 109], representam uma estratégia eficaz para mitigar os efeitos da global memory, através da colaboração entre threads para carregar dados na shared memory [^110], reduzindo o tráfego de dados para a global memory e, consequentemente, melhorando o desempenho geral [^111].

### Referências
[^1]: Página 95: "So far, we have learned to write a CUDA kernel function that is executed by a massive number of threads...In this chapter, you will learn to use these memories to boost the execution efficiency of CUDA kernels."
[^2]: Página 96: "In every iteration of this loop, two global memory accesses are per-formed for one floating-point multiplication and one floating-point addi-tion...We will refer to this ratio as the compute to global memory access (CGMA) ratio, defined as the number of floating-point calculations performed for each access to the global memory within a region of a CUDA program."
[^3]: Página 97: "will execute no more than 50 giga floating-point operations per second (GFLOPS)...We need to increase the CGMA ratio to achieve a higher level of performance for the kernel."
[^4]: Página 98: "The global memory in the CUDA programming model maps to the mem-ory of the von Neumann model (see “The von Neumann Model” sidebar)...The registers correspond to the "register file” of the von Neumann model."
[^5]: Página 99: "Arithmetic instructions in most modern processors have “built-in" reg-ister operands...Therefore, when an operand of an arithmetic instruction is in a register, there is no additional instruction required to make the operand value avail-able to the arithmetic and logic unit (ALU) where the arithmetic calcula-tion is done."
[^6]: Página 100: "On the other hand, if an operand value is in global memory, one needs to perform a memory load operation to make the operand value available to the ALU...Finally, there is another subtle reason why placing an operand value in registers is preferable."
[^15]: Página 109: "We now present an algorithm where threads collaborate to reduce the traf-fic to the global memory."
[^109]: Página 109: "5.4 A TILED MATRIX—MATRIX MULTIPLICATION KERNEL"
[^110]: Página 110: "In each phase, all threads in a block collaborate to load a tile of M elements and a tile of N elements into the shared memory."
[^111]: Página 111: "By loading each global memory value into shared mem-ory so that it can be used multiple times, we reduce the number of accesses to the global memory."

<!-- END -->