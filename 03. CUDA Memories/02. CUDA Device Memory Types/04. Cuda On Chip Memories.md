## Registers and Shared Memory in CUDA

### Introdução
Em CUDA, a eficiência no acesso à memória é crucial para alcançar alto desempenho em kernels. Uma das formas de otimizar o acesso à memória é através do uso estratégico de **registers** e **shared memory**, que são memórias *on-chip* com alta velocidade e capacidade de acesso paralelo [^3]. Este capítulo explora em profundidade as características e o uso dessas memórias, bem como as implicações para o design de algoritmos paralelos eficientes.

### Conceitos Fundamentais

**Registers** são alocados a *threads* individuais, enquanto a **shared memory** é alocada a *thread blocks*, permitindo que as *threads* cooperem compartilhando dados [^3]. A escolha do tipo de memória influencia a visibilidade da variável e sua velocidade de acesso [^3].

#### Registers
*   **Alocação:** Cada *thread* recebe seus próprios registers, tornando-os privados e acessíveis apenas por essa *thread* [^3].
*   **Acesso:** O acesso a registers é extremamente rápido e paralelo [^3, 103]. Em um dispositivo típico, a largura de banda agregada dos *register files* é cerca de duas ordens de magnitude maior do que a da *global memory* [^4]. Além disso, quando uma variável é armazenada em um register, seus acessos não consomem mais largura de banda da *off-chip global memory*, aumentando a *CGMA ratio* (Compute to Global Memory Access ratio) [^4].
*   **Escopo e Lifetime:** Variáveis escalares automáticas (não arrays) declaradas em funções de kernel e device são alocadas em registers. O escopo dessas variáveis é dentro de *threads* individuais, e uma cópia privada é gerada para cada *thread* [^102]. A *lifetime* dessas variáveis é limitada à execução do kernel [^102]. Por exemplo, as variáveis `Row`, `Col` e `Pvalue` no código da Figura 5.1 [^2] são variáveis automáticas e, portanto, residem em registers [^103].
*   **Considerações:** Apesar do acesso rápido, a capacidade de armazenamento de registers é limitada, e é preciso ter cuidado para não exceder essa capacidade [^103].

#### Shared Memory
*   **Alocação:** A *shared memory* é alocada a *thread blocks*, permitindo que todas as *threads* em um bloco acessem as variáveis alocadas nessa memória [^4].
*   **Acesso:** O acesso à *shared memory* é mais rápido do que o acesso à *global memory*, mas mais lento do que o acesso a registers [^101]. Embora a *shared memory* resida *on-chip*, o acesso ainda requer uma operação de carregamento (load), similar ao acesso à *global memory*, resultando em maior latência e menor largura de banda em comparação com registers [^101].
*   **Escopo e Lifetime:** Variáveis declaradas com o qualificador `__shared__` (ou `__device__ __shared__`) residem na *shared memory* [^103]. O escopo de uma variável *shared* é dentro de um *thread block*, e uma versão privada é criada para cada *thread block* durante a execução do kernel [^103]. A *lifetime* de uma variável *shared* é a duração do kernel [^103].
*   **Colaboração:** A *shared memory* é projetada para suportar o compartilhamento eficiente de dados entre *threads* em um bloco [^101]. As implementações de hardware da *shared memory* em dispositivos CUDA são projetadas para permitir que múltiplas unidades de processamento (SPs) acessem simultaneamente seu conteúdo, facilitando o compartilhamento eficiente de dados entre *threads* [^101].
*   **Uso:** Programadores CUDA frequentemente utilizam variáveis *shared* para armazenar porções de dados da *global memory* que são intensamente usadas em uma fase de execução de um kernel [^103]. Isso pode exigir o ajuste dos algoritmos para criar fases de execução que se concentrem em pequenas porções dos dados da *global memory* [^103].
*   **Tiling:** Uma estratégia comum para utilizar a *shared memory* é particionar os dados em subconjuntos chamados *tiles*, de forma que cada *tile* caiba na *shared memory* [^105]. O conceito de *tiling* se baseia na analogia de cobrir uma grande parede (dados na *global memory*) com *tiles* (subconjuntos que cabem na *shared memory*) [^105].

#### Considerações sobre a escolha entre Registers e Shared Memory
*   **Privacidade vs. Compartilhamento:** Se os dados são usados apenas por uma *thread*, registers são preferíveis devido à sua velocidade. Se os dados precisam ser compartilhados entre *threads* em um bloco, a *shared memory* é a escolha apropriada [^4, 101].
*   **Capacidade:** A capacidade de registers é limitada, enquanto a *shared memory* oferece mais espaço, mas ainda é limitada em comparação com a *global memory* [^103, 115].
*   **Sincronização:** Ao usar *shared memory*, é importante sincronizar as *threads* para evitar condições de corrida (race conditions) [^104]. A função `__syncthreads()` é usada para garantir que todas as *threads* em um bloco cheguem a um certo ponto no código antes de prosseguir [^114].

### Conclusão
Registers e shared memory são componentes essenciais para otimizar kernels CUDA. A escolha entre eles depende das necessidades de privacidade, compartilhamento e capacidade de armazenamento. A utilização eficiente dessas memórias requer uma compreensão profunda de suas características e limitações, bem como o uso de técnicas como *tiling* para maximizar o desempenho. Ao usar *shared memory*, a sincronização adequada das *threads* é crucial para garantir a correção e a eficiência do código. Em resumo, o domínio de registers e shared memory é fundamental para qualquer programador CUDA que busca alcançar o máximo desempenho em aplicações paralelas.

### Referências
[^3]: "Registers and shared memory in Figure 5.2 are on-chip memories. Variables that reside in these types of memory can be accessed at very high speed in a highly parallel manner. Registers are allocated to individual threads; each thread can only access its own registers. A kernel function typically uses registers to hold frequently accessed variables that are"
[^4]: "Shared memory is allocated to thread blocks; all threads in a block can access variables in the shared memory locations allocated to the block. Shared memory is an efficient means for threads to cooperate by sharing their input data and the intermediate results of their work."
[^101]: "Figure 5.4 shows shared memory and registers in a CUDA device. Although both are on-chip memories, they differ significantly in functionality and cost of access. Shared memory is designed as part of the memory space that resides on the processor chip (see Section 4.2). When the pro- cessor accesses data that resides in the shared memory, it needs to perform a memory load operation, just like accessing data in the global memory."
[^102]: "As shown in Table 5.1, all automatic scalar variables declared in kernel and device functions are placed into registers. We refer to variables that are not arrays as scalar variables. The scopes of these automatic variables are within individual threads. When a kernel function declares an auto- matic variable, a private copy of that variable is generated for every thread"
[^103]: "Registers and shared memory in Figure 5.2 are on-chip memories. Variables that reside in these types of memory can be accessed at very high speed in a highly parallel manner. Registers are allocated to individ- ual threads; each thread can only access its own registers. A kernel func- tion typically uses registers to hold frequently accessed variables that are"
[^104]: "In CUDA, pointers are used to point to data objects in global memory. There are two typical ways in which pointer usage arises in kernel and device functions. First, if an object is allocated by a host function, the pointer to the object is initialized by cudaMalloc() and can be passed to the kernel function as a parameter."
[^105]: "We have an intrinsic trade-off in the use of device memories in CUDA: global memory is large but slow, whereas the shared memory is small but fast. A common strategy is partition the data into subsets called tiles so that each tile fits into the shared memory."
[^114]: "The barrier _syncthreads() in line 11 ensures that all threads have finished loading the tiles of d_M and d_N into Mds and Nds before any of them can move forward."
[^115]: "While CUDA registers and shared memory can be extremely effective in reducing the number of accesses to global memory, one must be careful not to exceed the capacity of these memories."
[^2]: "FIGURE 5.1 A simple matrix-matrix multiplication kernel using one thread to compute each d_P element (copied from Figure 4.7)."
[^4]: "In a typical device, the aggregated access bandwidth of the register files is about two orders of magnitude of that of the global memory."
<!-- END -->