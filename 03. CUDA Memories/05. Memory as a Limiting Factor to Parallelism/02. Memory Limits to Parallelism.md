## Memória como Fator Limitante ao Paralelismo

### Introdução
Como vimos anteriormente, a eficiência do acesso à memória é crucial para o desempenho das aplicações CUDA [^95]. A utilização de diferentes tipos de memória, como registros e memória compartilhada, pode reduzir significativamente o tráfego na memória global, melhorando o desempenho [^97]. No entanto, é essencial considerar a capacidade limitada dessas memórias, pois exceder essa capacidade pode restringir o nível de paralelismo alcançável [^115]. Este capítulo explora como a memória pode atuar como um fator limitante ao paralelismo em CUDA.

### Conceitos Fundamentais

A arquitetura CUDA oferece diferentes tipos de memória, cada um com características distintas em termos de latência, largura de banda e escopo [^102]. **Registros** são alocados a threads individuais e oferecem acesso rápido e paralelo [^97]. A **memória compartilhada** permite que threads dentro de um bloco colaborem e compartilhem dados de forma eficiente [^97]. No entanto, tanto os registros quanto a memória compartilhada têm capacidade limitada [^115].

[^115] afirma que *cada dispositivo CUDA oferece uma quantidade limitada de recursos, o que limita o número de threads que podem residir simultaneamente no SM para uma determinada aplicação. Em geral, quanto mais recursos cada thread requer, menos threads podem residir em cada SM e, portanto, menos threads podem residir em todo o dispositivo*.

Para ilustrar essa limitação, considere um dispositivo CUDA onde cada SM pode acomodar até 1.536 threads e possui 16.384 registros [^115]. Se cada thread utilizar um número relativamente pequeno de registros (por exemplo, 10), o SM poderá suportar o número máximo de threads. No entanto, se cada thread necessitar de mais registros (por exemplo, 11), o número de threads que podem ser executados concorrentemente no SM será reduzido [^115]. Essa redução é feita na granularidade do bloco.

Por exemplo, se cada bloco contém 512 threads, a redução no número de threads será feita reduzindo 512 threads por vez [^116]. Assim, o próximo número mais baixo de threads a partir de 1.536 seria 512, uma redução de um terço dos threads que podem residir simultaneamente em cada SM [^116]. Isso pode reduzir significativamente o número de warps disponíveis para escalonamento, diminuindo a capacidade do processador de encontrar trabalho útil na presença de operações de longa latência [^116].

Da mesma forma, o uso da memória compartilhada também pode limitar o número de threads atribuídos a cada SM [^116]. Suponha que o dispositivo tenha 16 KB de memória compartilhada em cada SM e que cada SM possa acomodar até oito blocos [^116]. Para atingir esse máximo, cada bloco não deve usar mais de 2 KB de memória compartilhada [^116]. Se cada bloco usar mais de 2 KB de memória, o número de blocos que podem residir em cada SM é tal que a quantidade total de memória compartilhada usada por esses blocos não exceda 16 KB [^116].

Para o exemplo de multiplicação de matrizes, a memória compartilhada pode se tornar um fator limitante [^116]. Para um tamanho de tile de 16x16, cada bloco precisa de 16 * 16 * 4 = 1 KB de armazenamento para `Mds` [^116]. Outro 1 KB é necessário para `Nds` [^116]. Assim, cada bloco usa 2 KB de memória compartilhada. A memória compartilhada de 16 KB permite que oito blocos residam simultaneamente em um SM [^116]. Como isso é o mesmo que o máximo permitido pelo hardware de threading, a memória compartilhada não é um fator limitante para este tamanho de tile [^116]. Neste caso, a limitação real é a limitação do hardware de threading que apenas 768 threads são permitidos em cada SM. Isso limita o número de blocos em cada SM a três [^116]. Como resultado, apenas 3 * 2 KB = 6 KB da memória compartilhada serão usados [^116].

É importante notar que o número de registros disponíveis para cada SM varia de dispositivo para dispositivo [^116]. Um aplicativo pode determinar dinamicamente o número de registros disponíveis em cada SM do dispositivo usado e escolher uma versão do kernel que use o número de registros apropriado para o dispositivo [^116]. Isso pode ser feito chamando a função `cudaGetDeviceProperties()`, cujo uso foi discutido na Seção 4.6.

### Conclusão

Embora o uso eficiente de registros e memória compartilhada possa melhorar significativamente o desempenho das aplicações CUDA, é crucial considerar as limitações de capacidade dessas memórias [^115]. Exceder essas capacidades pode restringir o nível de paralelismo alcançável, diminuindo o desempenho [^115]. Portanto, os programadores CUDA devem estar cientes das limitações de tamanho desses tipos de memória [^118]. Suas capacidades são dependentes da implementação [^118]. Depois que suas capacidades são excedidas, elas se tornam fatores limitantes para o número de threads que podem ser executados simultaneamente em cada SM [^118]. A capacidade de raciocinar sobre as limitações do hardware ao desenvolver um aplicativo é um aspecto fundamental do pensamento computacional [^118].

### Referências
[^95]: Capítulo 5, página 95.
[^97]: Capítulo 5, página 97.
[^102]: Capítulo 5, página 102.
[^115]: Capítulo 5, página 115.
[^116]: Capítulo 5, página 116.
[^118]: Capítulo 5, página 118.
<!-- END -->