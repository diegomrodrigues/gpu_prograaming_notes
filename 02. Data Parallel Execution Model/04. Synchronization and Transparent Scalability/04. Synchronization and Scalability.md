## Sincronização e Escalabilidade Transparente em CUDA

### Introdução
O modelo de execução de dados paralelos do CUDA permite o lançamento de kernels para execução por uma grade de *threads* [^81]. Já foi discutido como mapear *threads* para partes da estrutura de dados. No entanto, ainda não foram apresentados meios para coordenar a execução de múltiplas *threads*. Este capítulo explora um mecanismo básico de coordenação e a escalabilidade transparente resultante.

### Conceitos Fundamentais
CUDA permite que *threads* no mesmo bloco coordenem suas atividades usando uma função de sincronização de barreira, `__syncthreads()` [^81]. É crucial notar que `__syncthreads()` consiste em dois caracteres de sublinhado. Quando uma função *kernel* chama `__syncthreads()`, todas as *threads* em um bloco são retidas no local da chamada até que cada *thread* no bloco atinja esse ponto [^81]. Isso garante que todas as *threads* em um bloco tenham completado uma fase de sua execução do *kernel* antes que qualquer uma delas possa avançar para a próxima fase [^81].

A sincronização de barreira é um método simples e popular de coordenação de atividades paralelas. Um exemplo prático é o de amigos indo a um shopping em um carro. Todos podem ir a lojas diferentes para fazer compras. Essa é uma atividade paralela e muito mais eficiente do que se todos permanecessem como um grupo e visitassem sequencialmente todas as lojas de interesse. No entanto, a sincronização de barreira é necessária antes de saírem do shopping [^81]. Eles precisam esperar até que todos os quatro amigos retornem ao carro antes que possam partir – aqueles que terminam mais cedo precisam esperar por aqueles que terminam mais tarde. Sem a sincronização de barreira, uma ou mais pessoas podem ser deixadas no shopping quando o carro partir, o que pode prejudicar seriamente a amizade deles!

**Restrições e Flexibilidade:**

1.  **Escopo da Sincronização:** A sincronização via `__syncthreads()` é restrita às *threads* dentro do mesmo bloco [^81]. *Threads* em blocos diferentes não podem realizar sincronização de barreira entre si [^83].
2.  **Ordem de Execução dos Blocos:** Ao não permitir que *threads* em diferentes blocos realizem sincronização de barreira entre si, o sistema de tempo de execução CUDA pode executar blocos em qualquer ordem relativa uns aos outros, já que nenhum deles precisa esperar pelo outro [^83].
3.  **Execução Condicional:** Em CUDA, uma declaração `__syncthreads()` deve ser executada por todas as *threads* em um bloco ou por nenhuma delas [^82]. Se uma instrução `__syncthreads()` for colocada dentro de uma instrução `if`, todas as *threads* em um bloco devem executar o caminho que inclui a `__syncthreads()` ou nenhuma delas [^82]. Para uma instrução `if-then-else`, se cada caminho tiver uma instrução `__syncthreads()`, todas as *threads* em um bloco devem executar a `__syncthreads()` no caminho `then` ou todas devem executar a no caminho `else` [^82]. Os dois `__syncthreads()` são diferentes pontos de sincronização de barreira. Se uma *thread* em um bloco executar o caminho `then` e outra executar o caminho `else`, elas ficariam esperando em diferentes pontos de sincronização de barreira. Elas acabariam esperando uma pela outra para sempre. É responsabilidade dos programadores escrever seu código para que esses requisitos sejam satisfeitos.
4. **Proximidade Temporal:** A capacidade de sincronizar também impõe restrições de execução nas *threads* dentro de um bloco. Essas *threads* devem ser executadas em tempo próximo umas das outras para evitar tempos de espera excessivamente longos [^83]. De fato, é preciso garantir que todas as *threads* envolvidas na sincronização de barreira tenham acesso aos recursos necessários para eventualmente chegar à barreira. Caso contrário, uma *thread* que nunca chegasse ao ponto de sincronização de barreira poderia fazer com que todos os outros esperassem para sempre [^83].

**Escalabilidade Transparente:**

Essa flexibilidade permite implementações escaláveis, onde a mesma aplicação pode ser executada em uma ampla gama de velocidades, dependendo do número de recursos de execução disponíveis [^83]. A escalabilidade transparente em CUDA é alcançada permitindo que os blocos sejam executados em qualquer ordem relativa, uma vez que as *threads* em diferentes blocos não podem sincronizar diretamente, facilitando a execução em sistemas com diferentes quantidades de recursos [^83]. A flexibilidade na execução do bloco permite que o mesmo código de aplicação seja executado em uma variedade de implementações de hardware, desde processadores móveis de baixa potência até *desktops* de alto desempenho, sem modificações no código [^83].

### Conclusão
A sincronização de *threads* é uma ferramenta poderosa para coordenar atividades dentro de um bloco CUDA. A restrição de sincronização entre blocos leva a uma escalabilidade transparente, permitindo que o código CUDA seja executado em uma variedade de plataformas de hardware sem modificações. Essa flexibilidade é fundamental para o modelo de programação CUDA, permitindo que os desenvolvedores criem aplicações que podem se adaptar a diferentes quantidades de recursos de processamento.

### Referências
[^81]: Section 4.4 Synchronization and Transparent Scalability, page 81.
[^82]: Section 4.4 Synchronization and Transparent Scalability, page 82.
[^83]: Section 4.4 Synchronization and Transparent Scalability, page 83.
<!-- END -->