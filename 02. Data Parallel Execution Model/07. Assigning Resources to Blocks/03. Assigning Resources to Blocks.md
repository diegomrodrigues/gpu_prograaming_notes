## 4.5 Alocando Recursos para Blocos

### Introdução
Este capítulo explora a alocação de recursos para blocos em CUDA, um aspecto crucial para entender o desempenho e a escalabilidade das aplicações. Discutimos como o sistema de *runtime* CUDA gerencia os recursos disponíveis e como as limitações de *hardware* influenciam a execução dos blocos. É importante notar que, ao contrário dos *threads* dentro de um mesmo bloco, que podem ser sincronizados, os blocos não possuem um mecanismo direto de sincronização entre si, o que impacta a forma como o sistema CUDA aloca recursos e executa os blocos [^21].

### Conceitos Fundamentais

Após o lançamento de um *kernel*, o sistema de *runtime* CUDA gera a *grid* correspondente de *threads* [^21]. Esses *threads* são alocados a recursos de execução em uma base bloco a bloco. Na geração atual de *hardware*, os recursos de execução são organizados em *streaming multiprocessors* (SMs) [^22]. Vários blocos de *threads* podem ser atribuídos a cada SM. Cada dispositivo tem um limite no número de blocos que podem ser atribuídos a cada SM [^22]. Por exemplo, um dispositivo CUDA pode permitir que até oito blocos sejam atribuídos a cada SM [^22].

Em situações onde há uma quantidade insuficiente de um ou mais tipos de recursos necessários para a execução simultânea de oito blocos, o *runtime* CUDA reduz automaticamente o número de blocos atribuídos a cada SM até que o uso combinado de recursos esteja abaixo do limite [^22]. Com um número limitado de SMs e um número limitado de blocos que podem ser atribuídos a cada SM, há um limite no número de blocos que podem estar ativamente em execução em um dispositivo CUDA [^22]. A maioria das *grids* contém muito mais blocos do que este número [^22]. O sistema de *runtime* mantém uma lista de blocos que precisam ser executados e atribui novos blocos a SMs à medida que eles completam a execução dos blocos atribuídos anteriormente [^22].

Uma das limitações de recursos do SM é o número de *threads* que podem ser rastreados e agendados simultaneamente [^22]. A limitação da sincronização a *threads* dentro do mesmo bloco permite que o sistema de *runtime* CUDA execute blocos em qualquer ordem relativa, o que permite implementações escaláveis [^21]. A arquitetura de *streaming multiprocessor* (SM) permite que vários blocos de *threads* sejam atribuídos a cada SM, com um limite no número de blocos que podem ser atribuídos a cada SM, dependendo dos recursos disponíveis [^21]. O sistema de *runtime* CUDA ajusta esse número automaticamente com base na disponibilidade de recursos [^21].

> A transparência da escalabilidade permite que o mesmo código de aplicação seja executado em *hardware* com diferentes números de recursos de execução, adaptando-se aos requisitos de custo, energia e desempenho [^21].

Essa flexibilidade permite a produção de uma ampla gama de implementações de acordo com os requisitos de custo, energia e desempenho de segmentos de mercado específicos [^21]. Por exemplo, um processador móvel pode executar um aplicativo lentamente, mas com um consumo de energia extremamente baixo, e um processador de *desktop* pode executar o mesmo aplicativo em uma velocidade maior, consumindo mais energia [^21]. Ambos executam exatamente o mesmo programa de aplicativo, sem nenhuma alteração no código [^21]. A capacidade de executar o mesmo código de aplicativo em *hardware* com um número diferente de recursos de execução é chamada de *transparent scalability*, o que reduz o fardo sobre os desenvolvedores de aplicativos e melhora a usabilidade dos aplicativos [^21].

### Conclusão

A alocação de recursos para blocos é um processo dinâmico gerenciado pelo sistema de *runtime* CUDA, que visa otimizar o uso dos recursos disponíveis e garantir a escalabilidade das aplicações [^21]. A limitação da sincronização entre blocos permite que o sistema execute blocos em qualquer ordem, facilitando a escalabilidade e a adaptação a diferentes configurações de *hardware* [^21]. Entender essas limitações e o funcionamento do sistema de alocação de recursos é fundamental para desenvolver aplicações CUDA eficientes e escaláveis [^21].

### Referências
[^21]: Capítulo 4, página 83
[^22]: Capítulo 4, página 84
<!-- END -->