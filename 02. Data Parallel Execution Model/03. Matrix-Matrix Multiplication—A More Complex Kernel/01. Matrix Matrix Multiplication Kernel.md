## Matrix-Matrix Multiplication: Thread Mapping and Kernel Implementation

### Introdução
Este capítulo aprofunda o conceito de **multiplicação de matrizes**, explorando um kernel CUDA mais complexo do que os apresentados anteriormente [^12]. Expandindo os conceitos de mapeamento de threads para dados, este capítulo se concentra em como cada thread pode realizar múltiplas operações aritméticas para calcular um único elemento da matriz resultante. Serão abordados os mecanismos de mapeamento de threads para elementos de dados, garantindo que cada elemento seja coberto por um thread único, utilizando `threadIdx`, `blockIdx`, `blockDim` e `gridDim` [^1].

### Conceitos Fundamentais

#### Multiplicação de Matrizes
A multiplicação de uma matriz **I×J** ($d_M$) por uma matriz **J×K** ($d_N$) resulta em uma matriz **I×K** ($d_P$) [^13]. Cada elemento da matriz resultante $d_P$ é o produto interno de uma linha de $d_M$ e uma coluna de $d_N$. Matematicamente, o elemento na linha *Row* e coluna *Col* de $d_P$, denotado como $d_{P_{Row, Col}}$, é calculado como:

$$ d_{P_{Row, Col}} = \sum_{k=0}^{Width-1} d_{M_{Row, k}} \cdot d_{N_{k, Col}} $$

onde *Width* representa a dimensão comum *J* das matrizes $d_M$ e $d_N$ (assumindo matrizes quadradas para simplificação, onde I = J = K = Width) [^13].

#### Mapeamento de Threads para Elementos da Matriz
No kernel `matrixMulKernel()`, cada thread é responsável por calcular um elemento específico da matriz $d_P$ [^1]. A posição deste elemento é determinada pelo índice da thread dentro do bloco e o índice do bloco dentro da grid [^1]:

*   **Linha (Row):** `blockIdx.y * blockDim.y + threadIdx.y`
*   **Coluna (Col):** `blockIdx.x * blockDim.x + threadIdx.x`

Este mapeamento garante que cada thread seja responsável por calcular o elemento $d_P$ na linha `blockIdx.y * blockDim.y + threadIdx.y` e na coluna `blockIdx.x * blockDim.x + threadIdx.x` [^1]. Este método efetivamente divide a matriz $d_P$ em *tiles* quadrados, onde cada bloco calcula um *tile* [^14].

#### Implementação do Kernel
O código do kernel `matrixMulKernel()` (Figura 4.7 [^15]) demonstra como cada thread itera através das linhas de $d_M$ e colunas de $d_N$ para calcular o produto interno. O kernel utiliza um loop `for` para acumular os produtos dos elementos correspondentes em uma variável local `Pvalue` [^15]:

```c++
__global__ void MatrixMulKernel(float* d_M, float* d_N, float* d_P, int Width) {
    int Row = blockIdx.y * blockDim.y + threadIdx.y;
    int Col = blockIdx.x * blockDim.x + threadIdx.x;

    if ((Row < Width) && (Col < Width)) {
        float Pvalue = 0;
        for (int k = 0; k < Width; ++k) {
            Pvalue += d_M[Row * Width + k] * d_N[k * Width + Col];
        }
        d_P[Row * Width + Col] = Pvalue;
    }
}
```

A expressão `Col = blockIdx.x*blockDim.x + threadIdx.x` gera cada valor inteiro de 0 a `blockDim.x*gridDim.x-1` [^12].

#### Otimização e Autotuning
Para otimizar o desempenho, o tamanho do bloco (*BLOCK_WIDTH*) é definido como uma constante de tempo de compilação usando `#define BLOCK_WIDTH 16` [^15]. Isso permite que o programador ajuste facilmente o tamanho do bloco para diferentes hardwares [^15]. Um sistema de *autotuning* pode iterar sobre diferentes valores para *BLOCK_WIDTH*, compilando e executando o código para encontrar o melhor valor para um hardware específico [^15].

#### Considerações sobre o Tamanho da Matriz
O kernel `matrixMulKernel()` pode lidar com matrizes de até 16 × 65.535 elementos em cada dimensão [^19]. Para matrizes maiores, é possível dividir a matriz $P$ em submatrizes e lançar kernels iterativamente [^19].

#### Acesso à Memória Linearizada
Como arrays multidimensionais são linearizados em memória (row-major layout [^8]), o acesso aos elementos da matriz requer o cálculo de índices lineares [^8]. Por exemplo, o elemento na linha *Row* e coluna *k* da matriz $d_M$ é acessado como `d_M[Row * Width + k]` [^16, 17].

### Conclusão
Este capítulo detalhou a implementação de um kernel CUDA para multiplicação de matrizes, demonstrando como mapear threads para elementos de dados e realizar cálculos complexos dentro de cada thread [^1]. A técnica de *tiling*, onde a matriz de saída é dividida em blocos que são computados independentemente por blocos de threads, é fundamental para otimizar o uso da memória e o paralelismo [^14]. O uso de constantes de tempo de compilação para o tamanho do bloco permite a fácil adaptação do código para diferentes arquiteturas de hardware [^15]. As técnicas apresentadas neste capítulo são essenciais para o desenvolvimento de aplicações CUDA de alto desempenho que envolvem operações matriciais [^12].

### Referências
[^1]: Data-Parallel Execution Model - Matrix-Matrix Multiplication—A More Complex Kernel
[^8]: Data-Parallel Execution Model - Memory Space
[^12]: Data-Parallel Execution Model - Matrix-Matrix Multiplication—A More Complex Kernel
[^13]: Data-Parallel Execution Model - Matrix-Matrix Multiplication—A More Complex Kernel
[^14]: Data-Parallel Execution Model - Matrix-Matrix Multiplication—A More Complex Kernel
[^15]: Data-Parallel Execution Model - Matrix-Matrix Multiplication—A More Complex Kernel
[^16]: Data-Parallel Execution Model - Matrix-Matrix Multiplication—A More Complex Kernel
[^17]: Data-Parallel Execution Model - Matrix-Matrix Multiplication—A More Complex Kernel
[^19]: Data-Parallel Execution Model - Matrix-Matrix Multiplication—A More Complex Kernel
<!-- END -->