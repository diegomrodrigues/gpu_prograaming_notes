## Escolha da Organização de Threads em CUDA

<imagem: Mapa mental mostrando os fatores que influenciam a escolha da organização de threads em CUDA, incluindo a natureza dos dados, o padrão de acesso à memória, o tipo de paralelismo, a arquitetura da GPU e a necessidade de sincronização. Inclua exemplos para cada caso.>

### Introdução

Em CUDA, a **escolha da organização dos threads** é uma decisão de projeto fundamental que afeta diretamente a eficiência e o desempenho da aplicação. A forma como os threads são dispostos dentro de *grids* e *blocos*, e como essa organização se alinha à estrutura dos dados, influencia o paralelismo, a localidade de acesso à memória e a complexidade do código [^6]. Este capítulo explorará os principais fatores que influenciam essa escolha, os diferentes modelos de organização (1D, 2D e 3D) e como selecionar a melhor organização para cada tipo de aplicação.

### Conceitos Fundamentais

Para tomar decisões informadas sobre a organização de threads em CUDA, é crucial compreender os seguintes conceitos:

**Conceito 1: Natureza dos Dados**

A **natureza dos dados** a serem processados é um dos principais fatores que influenciam a escolha da organização dos threads. Dados com estrutura linear (como vetores) se adaptam bem a organizações 1D, enquanto dados com estrutura bidimensional (como imagens) se beneficiam de organizações 2D. Dados volumétricos exigem uma organização 3D para um mapeamento mais natural [^6].

**Lemma 1:** *A organização de threads deve refletir a estrutura intrínseca dos dados a serem processados para facilitar o mapeamento e otimizar a localidade de acesso à memória.*

**Prova:**
Ao utilizar uma organização de threads que corresponde à estrutura dos dados, o cálculo dos índices para cada thread se torna mais simples e eficiente. Por exemplo, ao processar um array 2D, uma organização de threads bidimensional permite que cada thread calcule os índices de linha e coluna de forma mais direta, o que simplifica o código e torna o acesso aos dados mais eficiente. $\blacksquare$

**Conceito 2: Paralelismo de Dados**

O **paralelismo de dados** é um modelo de computação paralela em que a mesma operação é realizada em diferentes partes dos dados simultaneamente. Em CUDA, o tamanho do *grid* e do *bloco* e a forma como os *threads* são organizados influenciam diretamente como o paralelismo de dados é explorado [^1].

**Corolário 1:** *Uma organização de threads adequada garante que todas as partes dos dados possam ser processadas em paralelo, maximizando a utilização dos recursos da GPU.*

**Conceito 3: Localidade de Memória**

A **localidade de memória** se refere à tendência de um programa de acessar dados próximos uns dos outros na memória. Uma organização de threads que explora a localidade de memória pode reduzir o número de acessos à memória global e aumentar o desempenho. A organização de threads e sua forma de acesso aos dados definem se a localidade é maximizada ou não [^4].

> ⚠️ **Nota Importante**: A escolha da organização dos threads deve ser guiada pela natureza dos dados, pelo paralelismo desejado e pela necessidade de maximizar a localidade de acesso à memória para otimizar o desempenho [^4].

### Fatores que Influenciam a Escolha da Organização

A escolha da organização dos threads em CUDA é uma decisão multifacetada, influenciada pelos seguintes fatores:

1.  **Natureza dos Dados:** Como mencionado anteriormente, a estrutura dos dados a serem processados é um fator determinante. Dados lineares, como vetores, são mais adequados para *grids* e *blocos* 1D, enquanto dados bidimensionais ou tridimensionais requerem abordagens correspondentes.

2.  **Padrão de Acesso à Memória:** O padrão de acesso à memória dos threads também é importante. Se os threads acessarem dados sequencialmente na memória, uma organização que promova acessos coalescidos (acessos feitos em sequência por threads adjacentes) pode aumentar o desempenho. A organização dos blocos e threads deve ser feita com o objetivo de garantir o acesso coalescido à memória global.

3.  **Tipo de Paralelismo:** O tipo de paralelismo a ser explorado influencia a escolha da organização dos threads. Se o objetivo é processar cada elemento de um vetor de forma independente, uma organização 1D pode ser suficiente. Se o objetivo é realizar operações em matrizes, uma organização 2D pode ser mais adequada.

4.  **Arquitetura da GPU:** O número de *multiprocessadores de streaming (SMs)*, a quantidade de memória compartilhada disponível por *SM* e outras características da arquitetura da GPU também influenciam na escolha da organização dos *threads*. Um tamanho de bloco que maximiza a ocupação dos *SMs* pode ser crucial para o desempenho.

5.  **Necessidade de Sincronização:** A necessidade de sincronização entre threads também é um fator importante. A sincronização é mais eficiente dentro de um mesmo bloco. Uma organização que permita que os *threads* que precisam se sincronizar se encontrem dentro do mesmo bloco pode simplificar o código e melhorar o desempenho.

### Modelos de Organização de Threads

Com base nos fatores listados, é possível escolher entre os seguintes modelos de organização de threads:

1.  **Organização 1D:**

    -   **Características:** Os *threads* são organizados em uma única linha ou coluna. Apenas as dimensões x são usadas. É simples e eficiente para dados lineares.
    -   **Casos de Uso:** Adição de vetores, multiplicação de vetor por escalar, processamento de sinais, operações em listas.
    -   **Vantagens:** Simplicidade na configuração e no cálculo dos índices, boa utilização para problemas lineares.
    -   **Desvantagens:** Pode ser ineficiente para dados não lineares, a localidade da memória pode ser um problema.

2.  **Organização 2D:**

    -   **Características:** Os *threads* são organizados em uma matriz bidimensional. As dimensões x e y são usadas. É adequada para dados que podem ser dispostos em matrizes, imagens, tabelas, etc.
    -   **Casos de Uso:** Multiplicação de matrizes, processamento de imagens, simulações 2D.
    -   **Vantagens:** Mapeamento mais natural para dados bidimensionais, melhor localidade de acesso à memória em certos casos, facilidade para trabalhar com imagens.
    -   **Desvantagens:** Cálculo de índices pode ser mais complexo do que em 1D.

3.  **Organização 3D:**

    -   **Características:** Os *threads* são organizados em um volume tridimensional. As dimensões x, y e z são usadas. É adequada para o processamento de dados volumétricos.
    -   **Casos de Uso:** Simulações 3D, processamento de dados de tomografia, imagens médicas, visualização de dados 3D.
    -   **Vantagens:** Mapeamento mais natural para dados volumétricos.
    -   **Desvantagens:** Maior complexidade na configuração do *kernel* e no cálculo dos índices.

**Lemma 2:** *A escolha do modelo de organização de threads deve ser feita considerando as características dos dados, do problema a ser resolvido e do hardware da GPU. Não há uma solução universal, e a experimentação é essencial.*

**Prova:**
A natureza do problema dita o modelo de organização de threads ideal. Em um problema linear (vetor) o mapeamento 1D é mais adequado. Em um problema bidimensional (imagem) o mapeamento 2D é mais apropriado, e assim por diante. A experimentação e a análise do desempenho são cruciais para uma escolha adequada, pois a escolha incorreta levará a um uso ineficiente da GPU. $\blacksquare$

**Corolário 2:** *A variedade de modelos de organização de threads em CUDA permite aos programadores escolher a melhor forma de explorar o paralelismo para cada tipo de aplicação, balanceando a complexidade com o desempenho.*

### Exemplos de Escolha da Organização

Para ilustrar a escolha da organização dos threads, considere os seguintes exemplos:

1.  **Adição de Vetores:**

    -   **Natureza dos Dados:** Dados lineares (vetores).
    -   **Padrão de Acesso:** Acessos sequenciais à memória.
    -   **Melhor Organização:** 1D.
    -   **Justificativa:** A organização 1D é simples, e cada thread processa um elemento diferente do vetor.

2.  **Multiplicação de Matrizes:**

    -   **Natureza dos Dados:** Dados bidimensionais (matrizes).
    -   **Padrão de Acesso:** Acessos não sequenciais e com saltos para a multiplicação.
    -   **Melhor Organização:** 2D.
    -   **Justificativa:** A organização 2D permite que cada thread calcule o resultado de um elemento da matriz de saída, e o mapeamento entre as threads e os dados é mais natural.

3.  **Simulação de Fluidos 3D:**

    -   **Natureza dos Dados:** Dados tridimensionais (volume).
    -   **Padrão de Acesso:** Acessos complexos dependendo da simulação.
    -   **Melhor Organização:** 3D.
    -   **Justificativa:** A organização 3D permite que cada thread processe uma porção volumétrica dos dados de forma mais direta e intuitiva, e auxilia na aplicação de cálculos físicos 3D.

### Importância da Experimentação

É crucial realizar experimentos e testes com diferentes organizações de threads para determinar a melhor escolha para um problema específico. Não existe uma solução única e a experimentação é fundamental para encontrar a configuração que proporciona o melhor desempenho. É recomendado utilizar *profiling tools* para analisar o uso dos recursos da GPU e identificar gargalos que possam ser mitigados com a organização correta dos *threads*.

> ❗ **Ponto de Atenção**: A escolha da organização de threads é um processo de otimização iterativa. É necessário testar diferentes configurações e analisar o desempenho para garantir que a escolha seja adequada para cada tipo de aplicação e arquitetura de GPU.

### Conclusão

A escolha da organização de *threads* em CUDA é uma decisão crítica que afeta diretamente o desempenho e a eficiência das aplicações paralelas. Ao considerar a natureza dos dados, o padrão de acesso à memória, o tipo de paralelismo a ser explorado, a arquitetura da GPU e a necessidade de sincronização, os programadores podem escolher a organização mais apropriada para cada problema. A experimentação é fundamental para validar e otimizar a escolha, garantindo que a aplicação CUDA aproveite ao máximo o poder da GPU.

### Referências

[^1]: "Fine-grained, data-parallel threads are the fundamental means of parallel execution in CUDA. Each thread uses a unique coordinate, or thread index, to identify the portion of the data structure to process." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*

[^2]: "Recall from Chapter 3 that all CUDA threads in a grid execute the same kernel function and they rely on coordinates to distinguish themselves from each other and to identify the appropriate portion of the data to pro- cess." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*

[^3]: "In general, a grid is a 3D array of blocks¹ and each block is a 3D array of threads. The programmer can choose to use fewer dimensions by setting the unused dimensions to 1." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*

[^4]: "For example, in a CUDA kernel function, gridDim, blockDim, blockIdx, and threadIdx are all built-in variables." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*

[^5]: "Each threadIdx also consists of three fields: the x coordinate threadId.x, the y coordinate threadIdx.y, and the z coordinate threadIdx.z." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*

[^6]: "The choice of 1D, 2D, or 3D thread organizations is usually based on the nature of the data." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*

## Tratamento de Threads Extras em CUDA

<imagem: Diagrama mostrando o tratamento de threads extras em CUDA, com um exemplo visual de um grid de threads processando um array, indicando as threads que estão dentro e fora do limite do array, e como o uso de condicionais garante a execução correta. Inclua código C++ exemplificando o uso de condicionais.>

### Introdução

Em CUDA, a organização dos *threads* em *grids* e *blocos* frequentemente resulta em um número de *threads* que é maior do que o número de elementos de dados a serem processados [^6]. Isso ocorre porque o número de *threads* é geralmente um múltiplo do tamanho do *bloco*, e o tamanho do *bloco* é escolhido com base na arquitetura da GPU e no desempenho desejado, não necessariamente no tamanho exato dos dados [^13]. Este capítulo explora as técnicas para lidar com essas *threads extras* em CUDA, incluindo o uso de condicionais e outras práticas recomendadas para garantir a correta execução do *kernel* e evitar erros de acesso à memória.

### Conceitos Fundamentais

Para entender como lidar com *threads extras* em CUDA, é crucial compreender os seguintes conceitos:

**Conceito 1: Sobredimensionamento de Threads**

O **sobredimensionamento de threads** é uma prática comum em CUDA, onde o número de *threads* criadas para processar um conjunto de dados excede o número de elementos de dados. Essa prática é intencional e utilizada para simplificar o lançamento do *kernel* e para maximizar a utilização dos recursos da GPU. No entanto, é importante que o código do *kernel* consiga lidar com as *threads extras* corretamente [^13].

**Lemma 1:** *O sobredimensionamento de threads simplifica o lançamento de kernels e promove a utilização eficiente dos recursos da GPU, embora seja necessário um tratamento adequado das threads extras para evitar erros.*

**Prova:**
A escolha do número de *threads* geralmente é guiada pelo tamanho dos *blocos* e pela arquitetura da GPU, e nem sempre corresponde ao número exato de elementos de dados. Lançar um número de *threads* um pouco maior, garante que toda a GPU esteja sendo utilizada e evita a necessidade de cálculos adicionais no *host* para adequar o número de *threads* ao tamanho exato dos dados. $\blacksquare$

**Conceito 2: Índices Globais de Threads**

Cada thread em um *grid* possui um **índice global único**, que é usado para identificar a porção dos dados que ela deve processar. Os índices globais são calculados com base nas variáveis *built-in* `blockIdx` e `threadIdx` e nas dimensões dos *blocos* e dos *grids* [^4]. É esse índice que define qual thread trabalha em qual parte do dataset.

**Corolário 1:** *Os índices globais permitem que cada thread identifique sua porção de dados, mesmo quando o número de threads excede o número de elementos de dados.*

**Conceito 3: Condicionais em Kernels**

As **condicionais** (por exemplo, `if` statements) são usadas dentro do *kernel* para verificar se o índice global da *thread* está dentro do intervalo válido dos dados. Essas condicionais garantem que apenas as *threads* que correspondem a elementos de dados válidos realizem as operações de processamento [^7].

> ⚠️ **Nota Importante**: O tratamento de *threads extras* em CUDA é realizado através da utilização de condicionais dentro do *kernel*, que verificam se o índice global da thread está dentro do intervalo válido dos dados, evitando acessos inválidos [^7].

### Técnicas para Lidar com Threads Extras

Existem diversas técnicas para lidar com *threads extras* em CUDA, mas a mais comum e recomendada é o uso de instruções condicionais, que permitem que cada *thread* determine se deve ou não realizar uma operação.

1. **Condicionais Simples**:
   A forma mais direta de lidar com *threads extras* é usar uma condicional simples que verifique se o índice global da *thread* está dentro do intervalo válido de elementos de dados:

   ```c++
   __global__ void kernel(float* data, int size) {
       int i = blockIdx.x * blockDim.x + threadIdx.x;
       if (i < size) {
           // Operações de processamento para a thread i
           data[i] = data[i] * 2.0f; // Exemplo
       }
   }
   ```

   Neste exemplo, a variável `i` representa o índice global da *thread*. Se `i` for menor que o tamanho dos dados, a *thread* realiza a operação de processamento. Caso contrário, ela não realiza nenhuma operação, evitando um acesso inválido à memória.

2. **Múltiplas Condicionais:**

   Em casos mais complexos, pode ser necessário verificar várias condições para lidar com *threads extras* em diferentes dimensões. Por exemplo, ao processar imagens (estruturas 2D), pode-se usar:

   ```c++
   __global__ void kernel(float* data, int width, int height) {
       int row = blockIdx.y * blockDim.y + threadIdx.y;
       int col = blockIdx.x * blockDim.x + threadIdx.x;
   
       if (row < height && col < width) {
         // Operações de processamento para a thread (row, col)
         int index = row * width + col;
          data[index] = data[index] * 2.0f; // Exemplo
       }
   }
   ```

   Neste caso, tanto a linha quanto a coluna são verificadas para garantir que a *thread* processe apenas os *pixels* dentro da imagem. A condição composta com o operador lógico `&&` garante que as operações são realizadas apenas quando ambos os índices estão dentro dos intervalos permitidos.

3. **Cálculos de Índices Seguros:**
   A condição pode também estar no cálculo do índice, em vez de em uma operação, conforme o seguinte exemplo:

   ```c++
     __global__ void kernel(float* data, int width, int height) {
         int row = blockIdx.y * blockDim.y + threadIdx.y;
         int col = blockIdx.x * blockDim.x + threadIdx.x;
         int index = -1;
   
         if (row < height && col < width) {
             index = row * width + col;
             data[index] = data[index] * 2.0f; // Exemplo
         }
     }
   ```

   Nesse exemplo, se o `row` ou `col` forem inválidos, a variável `index` não é inicializada, o que garante que o código não irá fazer acessos incorretos.

4. **Acessos Condicionais à Memória:**
   O acesso à memória também pode ser condicional, para garantir que apenas threads válidas acessem as porções de memória corretas.

   ```c++
   __global__ void kernel(float* data, int size) {
       int i = blockIdx.x * blockDim.x + threadIdx.x;
       float value;
       if (i < size) {
           value = data[i]; // leitura da memória apenas se i < size
           value = value * 2.0f; // Exemplo
            data[i] = value; // escrita da memória apenas se i < size
       }
   }
   ```

   Nesse caso, as leituras e escritas na memória global são condicionadas à validade do índice `i`, evitando acessos inválidos.

5. **Operações Condicionais:**
   Outra forma de lidar com *threads extras* é condicionar a operação em si, sem condicionar o acesso à memória:

   ```c++
      __global__ void kernel(float* data, int size) {
        int i = blockIdx.x * blockDim.x + threadIdx.x;
           if(i<size){
             float value = data[i];
           if (i % 2 == 0) {
                 data[i] = value * 2.0f;
               }
          }
       }
   ```

     Nesse caso, todas as threads lêem a memória. Mas, apenas as que possuem o `i` par é que modificam o valor.

### Práticas Recomendadas

Além do uso de condicionais, algumas práticas recomendadas ao lidar com *threads extras* incluem:

1.  **Escolha Adequada do Tamanho do Bloco:** Escolher o tamanho do bloco adequado, que seja um múltiplo do tamanho do *warp*, para maximizar a utilização da GPU. Evitar o uso de tamanhos de bloco muito pequenos que gerem muitos *threads extras*.

2.  **Cálculo de Índices Eficiente:** Calcular os índices globais de maneira eficiente e evitar operações complexas dentro do *kernel*. O cálculo de índices deve ser otimizado para reduzir a sobrecarga e o tempo de execução.

3.  **Evitar Ramificações:** Minimizar o uso de condicionais dentro do *kernel*, pois podem gerar ramificações e reduzir o desempenho. No entanto, é importante usar as condicionais para tratar *threads extras*. Em alguns casos, a otimização é minimizar as operações dentro da condicional.

4.  **Testar em Diferentes GPUs:** Testar a aplicação em diferentes GPUs para garantir que ela se comporte corretamente com diferentes números de *threads*, tamanhos de bloco e capacidades computacionais.

**Lemma 3:** *A utilização de condicionais de forma eficiente e a escolha de um tamanho de bloco adequado permitem o tratamento correto das threads extras em CUDA, garantindo que todos os elementos de dados sejam processados sem erros e que o paralelismo seja explorado de forma adequada.*

**Prova:**
As condicionais garantem que apenas as threads que correspondem a dados válidos executem as operações de processamento, evitando acessos inválidos à memória. A escolha de um tamanho de bloco adequado evita a criação de muitos threads extras e permite um melhor aproveitamento da arquitetura da GPU. $\blacksquare$

**Corolário 2:** *O tratamento adequado das threads extras, combinado com boas práticas de programação, resulta em aplicações CUDA robustas, eficientes e com maior desempenho.*

### Impacto no Desempenho

Embora as condicionais sejam essenciais para lidar com *threads extras*, elas podem ter um impacto no desempenho, pois podem gerar ramificações no fluxo de execução dos *threads*. Contudo, esse impacto é geralmente mínimo se as condicionais forem usadas de forma eficiente e se a maioria das *threads* estiver dentro do intervalo válido de dados. Em muitos casos, o ganho em termos de flexibilidade e correção do código justifica a pequena perda de desempenho.

### Conclusão

Lidar com *threads extras* é uma tarefa comum na programação CUDA e é essencial para garantir que todos os elementos de dados sejam processados corretamente e que a aplicação aproveite ao máximo o paralelismo oferecido pela GPU. A utilização de condicionais dentro do *kernel*, combinada com outras práticas recomendadas, permite que os programadores criem aplicações CUDA robustas, eficientes e escaláveis. A escolha adequada do tamanho do *bloco* e a otimização do código do *kernel* são fundamentais para minimizar o impacto do tratamento das *threads extras* no desempenho final da aplicação.

### Referências

[^1]: "Fine-grained, data-parallel threads are the fundamental means of parallel execution in CUDA." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*

[^2]: "Recall from Chapter 3 that all CUDA threads in a grid execute the same kernel function and they rely on coordinates to distinguish themselves from each other." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*

[^3]: "In general, a grid is a 3D array of blocks¹ and each block is a 3D array of threads. The programmer can choose to use fewer dimensions by setting the unused dimensions to 1." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*

[^4]: "For example, in a CUDA kernel function, gridDim, blockDim, blockIdx, and threadIdx are all built-in variables." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*

[^5]: "Each threadIdx also consists of three fields: the x coordinate threadId.x, the y coordinate threadIdx.y, and the z coordinate threadIdx.z." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*

[^6]: "That is, we will generate 80 × 64 threads to process 76 × 62 pixels. This is similar to the situation where a 1,000-element vector is processed by the 1D vecAddKernel in Figure 3.10 using four 256-thread blocks." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*
[^7]: "Analogously, we should expect that the picture processing kernel function will have if statements to test whether the thread indices threadIdx.x and threadIdx.y fall within the valid range of pixels." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*
[^13]: "We also used vecAddKernel() and pictureKernel() to introduce the phenomenon that the number of threads that we create is a multiple of the block dimension. As a result, we will likely end up with more threads than data elements. Not all threads will process elements of an array. We use an if statement to test if the global index values of a thread are within the valid range." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*

## Linearização de Arrays Dinâmicos em CUDA

<imagem: Diagrama ilustrando a linearização de um array 2D em um array 1D usando o esquema row-major. Inclua a representação gráfica do array 2D original e do array 1D linearizado, com índices e anotações mostrando a fórmula de cálculo do índice.>

### Introdução

Em CUDA, o tratamento de *arrays* multidimensionais alocados dinamicamente apresenta desafios específicos devido à necessidade de que o compilador conheça o tamanho das dimensões no tempo de compilação. Como esse tamanho pode variar em tempo de execução, os programadores precisam linearizar ou "achatar" *arrays* multidimensionais em *arrays* unidimensionais equivalentes [^7]. Este capítulo explora as técnicas de linearização de *arrays* dinâmicos em CUDA, detalhando o esquema *row-major*, o *column-major*, o processo de cálculo de índices e o uso de *pointers* para acesso à memória.

### Conceitos Fundamentais

Para entender a linearização de *arrays* dinâmicos em CUDA, é crucial compreender os seguintes conceitos:

**Conceito 1: Arrays Multidimensionais**

*Arrays* **multidimensionais** são estruturas de dados que organizam elementos em mais de uma dimensão, como matrizes (2D) e cubos (3D). Em C/C++, *arrays* multidimensionais são representados na memória de forma linearizada, onde os elementos são armazenados em sequência contínua [^8].

**Lemma 1:** *A representação de arrays multidimensionais na memória é sempre linearizada, e o programador precisa fazer o mapeamento entre índices lógicos (multidimensionais) e índices físicos (lineares).*

**Prova:**
A memória de um computador é linear, composta por posições de memória sequenciais. Portanto, um array multidimensional, como uma matriz, deve ser armazenado na memória linear de forma sequencial. O programador precisa mapear os índices (row, col) de uma matriz para um índice linear na memória. $\blacksquare$

**Conceito 2: Alocação Dinâmica de Memória**

A **alocação dinâmica de memória** é o processo pelo qual a memória é alocada durante a execução de um programa. Essa técnica permite criar *arrays* cujos tamanhos são conhecidos apenas em tempo de execução. Em CUDA, *arrays* alocados dinamicamente no *host* precisam ser copiados para o *device*, e a manipulação desses *arrays* no *device* requer atenção especial [^7].

**Corolário 1:** *A alocação dinâmica permite que o tamanho dos arrays seja determinado em tempo de execução, mas requer que o programador gerencie a linearização e o acesso à memória de forma cuidadosa.*

**Conceito 3: Compilação vs Runtime**

Em C/C++ e CUDA, algumas informações são conhecidas em **tempo de compilação**, enquanto outras são determinadas em **tempo de execução**. O compilador CUDA precisa de informações sobre a estrutura dos dados no tempo de compilação para gerar código eficiente, mas o tamanho dos *arrays* dinâmicos é conhecido apenas em tempo de execução. A linearização resolve esse problema [^7].

> ⚠️ **Nota Importante**: A linearização de arrays dinâmicos é essencial para que o código CUDA consiga acessar os elementos da forma esperada, pois o compilador não tem informação sobre as dimensões no momento da compilação [^7].

### Linearização de Arrays

A **linearização** é o processo de transformar um *array* multidimensional em um *array* unidimensional equivalente. Isso é feito mapeando cada elemento do *array* multidimensional para uma posição específica no *array* unidimensional. Em CUDA, dois esquemas de linearização são comuns:

1. **Row-Major Layout**

   No **row-major layout**, os elementos de um *array* 2D são armazenados na memória linha por linha. Isso significa que todos os elementos da primeira linha são armazenados em sequência, seguidos pelos elementos da segunda linha, e assim por diante. Em C/C++, os *arrays* 2D são armazenados utilizando este esquema [^8].
    Para um array M com `rows` linhas e `cols` colunas, o elemento M[j][i] é mapeado para o índice linear:

   $$
   \text{linear\_index} = j \times \text{cols} + i
   $$

    Onde j é o índice da linha e i é o índice da coluna.

2. **Column-Major Layout**
    No **column-major layout**, os elementos de um *array* 2D são armazenados na memória coluna por coluna. Todos os elementos da primeira coluna são armazenados em sequência, seguidos pelos elementos da segunda coluna, e assim por diante. FORTRAN usa esse tipo de esquema.
    Para um array M com `rows` linhas e `cols` colunas, o elemento M[j][i] é mapeado para o índice linear:
   $$
    \text{linear\_index} = i \times \text{rows} + j
   $$

    Onde j é o índice da linha e i é o índice da coluna.

### Linearização de Arrays Dinâmicos em CUDA

Em CUDA, os *arrays* alocados dinamicamente no *host* devem ser copiados para o *device*. A linearização é feita no *host* durante a cópia ou no *device* quando o *kernel* acessa a memória.

**Linearização no Host:**

Durante a cópia de dados do *host* para o *device*, pode-se usar um *array* 1D equivalente para representar os dados que estão alocados dinamicamente em um *array* multidimensional. O mapeamento do índice do *array* multidimensional para o *array* unidimensional precisa ser calculado pelo programador usando a fórmula apropriada:

```c++
// Exemplo de cópia de um array 2D para um array 1D (row-major)
float* host_array_2d; // Array 2D alocado dinamicamente no host
float* host_array_1d; // Array 1D para cópia
int rows, cols; // dimensões do array 2D

// Alocar host_array_1d com tamanho rows * cols
host_array_1d = (float*) malloc(rows * cols * sizeof(float));

for (int j = 0; j < rows; j++) {
    for (int i = 0; i < cols; i++) {
       int linear_index = j * cols + i;
       host_array_1d[linear_index] = host_array_2d[j * cols + i];
    }
}

// Copiar host_array_1d para device_array_1d
cudaMemcpy(device_array_1d, host_array_1d, rows * cols * sizeof(float), cudaMemcpyHostToDevice);
```

**Linearização no Device (Kernel):**

Dentro do *kernel*, o programador deve calcular o índice linear correspondente aos índices de *thread* para acessar a porção correta da memória do *device*. Por exemplo, ao utilizar row-major layout:

```c++
__global__ void kernel(float* data, int rows, int cols) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < rows && col < cols) {
       int linear_index = row * cols + col;
       data[linear_index] = data[linear_index] * 2.0f;
    }
}
```

Aqui, o índice linear é calculado dentro do *kernel* para garantir que cada thread acesse o elemento correto da matriz bidimensional linearizada. As dimensões `rows` e `cols` são passadas para o *kernel* como argumentos, pois não são conhecidas em tempo de compilação.

### O Uso de Pointers para Acessar a Memória

Em CUDA, *pointers* são usados para acessar a memória do *device*. A linearização transforma um acesso multidimensional em um acesso através de um índice linear. É possível utilizar *pointers* para acessar a memória de diversas formas. Uma delas é utilizando um *pointer* que aponta para o início do array 1D, e acessando-o através do índice linear. O outro método é utilizar o acesso normal, `data[linear_index]`, que também utiliza um *pointer* internamente, através do índice linear.

### Comparação com Arrays Estáticos

Para *arrays* alocados estaticamente, o compilador C/C++ é capaz de calcular automaticamente os índices lineares. No entanto, essa funcionalidade não está disponível para *arrays* alocados dinamicamente, pois as dimensões não são conhecidas no tempo de compilação. Isso faz com que a linearização seja um passo necessário na manipulação de *arrays* dinâmicos em CUDA.

### Uso de `cudaMemcpy2D`

CUDA também oferece a função `cudaMemcpy2D` para cópia de *arrays* 2D, que simplifica a cópia de memória. Contudo, `cudaMemcpy2D` ainda trabalha com *arrays* linearizados na memória.

```c++
cudaMemcpy2D(device_array, pitch, host_array, host_pitch, width, height, cudaMemcpyHostToDevice);
```

O parâmetro `pitch` indica o número de bytes entre linhas consecutivas.

**Lemma 2:** *A linearização de arrays dinâmicos é necessária em CUDA para compatibilizar as estruturas multidimensionais lógicas com o espaço de endereçamento linear da memória física, garantindo que todos os elementos sejam acessados corretamente.*

**Prova:**
O compilador CUDA precisa conhecer o tamanho das dimensões do array em tempo de compilação para gerar o código de acesso à memória. Como o tamanho de arrays alocados dinamicamente só é conhecido em tempo de execução, o programador precisa fazer a linearização para o *device*. $\blacksquare$

**Corolário 2:** *A escolha do esquema de linearização (row-major ou column-major) deve ser consistente entre o código do host e o código do device para garantir o correto acesso à memória. O uso de row-major em CUDA simplifica o acesso e processamento dos dados.*

### Conclusão

A linearização de *arrays* dinâmicos é um processo fundamental na programação CUDA. Embora o compilador C/C++ lide com a linearização de *arrays* estáticos, o programador deve realizar a linearização de *arrays* dinâmicos explicitamente para garantir o acesso correto aos dados na memória do *device*. A utilização do esquema *row-major*, o cálculo adequado de índices lineares e o uso de *pointers* são essenciais para criar aplicações CUDA eficientes e sem erros, especialmente quando se trata de dados alocados dinamicamente.

### Referências

[^1]: "Fine-grained, data-parallel threads are the fundamental means of parallel execution in CUDA. As we explained in Chapter 3, launching a CUDA kernel creates a grid of threads that all execute the kernel function. That is, the kernel function specifies the C statements that are executed by each individual thread at runtime." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*

[^2]: "Recall from Chapter 3 that all CUDA threads in a grid execute the same kernel function and they rely on coordinates to distinguish themselves from each other and to identify the appropriate portion of the data to pro- cess." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*

[^3]: "In general, a grid is a 3D array of blocks¹ and each block is a 3D array of threads." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*

[^4]: "For example, in a CUDA kernel function, gridDim, blockDim, blockIdx, and threadIdx are all built-in variables." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*

[^5]: "Each threadIdx also consists of three fields: the x coordinate threadId.x, the y coordinate threadIdx.y, and the z coordinate threadIdx.z." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*

[^6]: "That is, we will generate 80 × 64 threads to process 76 × 62 pixels." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*
[^7]: "Unfortunately, this information is not known at compiler time for dynami- cally allocated arrays. In fact, part of the reason why one uses dynamically allocated arrays is to allow the sizes and dimensions of these arrays to vary according to data size at runtime. Thus, the information on the num- ber of columns in a dynamically allocated 2D array is not known at com- pile time by design. As a result, programmers need to explicitly linearize, or "flatten," a dynamically allocated 2D array into an equivalent 1D array in the current CUDA C." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*
[^8]: "In reality, all multidimensional arrays in C are linearized. This is due to the use of a “flat” memory space in modern computers (see “Memory Space” sidebar)." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*

## Row-Major Layout em CUDA

<imagem: Diagrama ilustrando o row-major layout com um exemplo de um array 2D (matriz) sendo linearizado em um array 1D. Inclua as posições dos elementos, os índices no array 2D e os correspondentes índices no array 1D, com anotações detalhadas da fórmula de cálculo do índice.>

### Introdução

Em CUDA, o **row-major layout** é um esquema fundamental para a linearização de *arrays* multidimensionais, especialmente quando esses *arrays* são alocados dinamicamente [^8]. Esse esquema, amplamente utilizado em C/C++, define a ordem em que os elementos de um *array* 2D ou multidimensional são armazenados na memória linear, onde a linha é percorrida primeiro antes de passar para a próxima linha [^8]. Este capítulo detalha o funcionamento do *row-major layout*, sua importância em CUDA, o processo de cálculo dos índices lineares e sua utilização na manipulação de dados no *device*.

### Conceitos Fundamentais

Para entender o *row-major layout* em CUDA, é crucial compreender os seguintes conceitos:

**Conceito 1: Linearização de Arrays Multidimensionais**

A **linearização de *arrays* multidimensionais** é o processo de mapear um *array* de mais de uma dimensão para um *array* unidimensional, de forma que os elementos possam ser acessados através de um único índice. Esse processo é necessário porque a memória do computador é linear e todos os elementos precisam ser armazenados sequencialmente [^8]. O *row-major layout* é uma das maneiras mais comuns de realizar essa linearização.

**Lemma 1:** *A linearização é necessária para compatibilizar a representação lógica de arrays multidimensionais com a organização linear da memória, sendo que o row-major é um dos esquemas de linearização mais utilizados.*

**Prova:**
A memória do computador é organizada de forma linear, onde cada byte tem um endereço sequencial. Para armazenar dados em arrays multidimensionais, como matrizes, na memória linear, é necessário mapear os índices lógicos (linha e coluna) para um único índice linear. O *row-major layout* é um esquema que realiza essa tarefa percorrendo as linhas da matriz sequencialmente. $\blacksquare$

**Conceito 2: Row-Major vs. Column-Major**

Existem diferentes formas de linearizar *arrays* multidimensionais. O **row-major layout** armazena os elementos linha por linha, enquanto o **column-major layout** armazena os elementos coluna por coluna. A escolha entre esses esquemas afeta a ordem de armazenamento dos dados na memória e como os índices são calculados [^9].

**Corolário 1:** *A escolha entre row-major e column-major define a ordem de armazenamento dos dados na memória, e é fundamental garantir a consistência no uso do mesmo esquema tanto no host quanto no device.*

**Conceito 3: Acesso à Memória em CUDA**

Em CUDA, o acesso à memória é fundamental para o desempenho. A forma como os *threads* acessam a memória, seja global ou compartilhada, afeta diretamente a latência e a largura de banda. O *row-major layout* afeta a forma como os *threads* são mapeados para os dados e como o acesso à memória é realizado [^26].

> ⚠️ **Nota Importante**: O *row-major layout* é um esquema de linearização que influencia como as *threads* acessam a memória, especialmente quando se trabalha com *arrays* multidimensionais alocados dinamicamente, o que é muito comum na programação CUDA [^8].

### Funcionamento do Row-Major Layout

No **row-major layout**, os elementos de um *array* 2D (matriz), por exemplo, são armazenados na memória da seguinte forma:

1.  Os elementos da primeira linha são armazenados sequencialmente.
2.  Os elementos da segunda linha são armazenados sequencialmente após a primeira linha.
3.  Esse processo continua para todas as linhas subsequentes.

Para um *array* 2D M com `rows` linhas e `cols` colunas, o elemento `M[j][i]` (onde *j* é o índice da linha e *i* é o índice da coluna) é mapeado para o índice linear na memória usando a seguinte fórmula:

$$
\text{linear\_index} = j \times \text{cols} + i
$$

Onde:

-   `linear_index` é o índice linear do elemento no *array* 1D equivalente.
-   `j` é o índice da linha (0 até `rows` - 1).
-   `cols` é o número total de colunas no *array* 2D.
-   `i` é o índice da coluna (0 até `cols` - 1).

### Exemplo Prático

Considere um *array* 2D de inteiros com 3 linhas e 4 colunas:

```
M = {
    {1, 2, 3, 4},
    {5, 6, 7, 8},
    {9, 10, 11, 12}
};
```

No *row-major layout*, esse *array* seria armazenado na memória como um *array* 1D da seguinte forma:

```
M_linear = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}
```

O mapeamento dos elementos para o índice linear é feito da seguinte forma:

| Elemento | j    | i    | linear_index = j * cols + i |
| -------- | ---- | ---- | --------------------------- |
| M[0][0]  | 0    | 0    | 0 * 4 + 0 = 0               |
| M[0][1]  | 0    | 1    | 0 * 4 + 1 = 1               |
| M[0][2]  | 0    | 2    | 0 * 4 + 2 = 2               |
| M[0][3]  | 0    | 3    | 0 * 4 + 3 = 3               |
| M[1][0]  | 1    | 0    | 1 * 4 + 0 = 4               |
| M[1][1]  | 1    | 1    | 1 * 4 + 1 = 5               |
| M[1][2]  | 1    | 2    | 1 * 4 + 2 = 6               |
| M[1][3]  | 1    | 3    | 1 * 4 + 3 = 7               |
| M[2][0]  | 2    | 0    | 2 * 4 + 0 = 8               |
| M[2][1]  | 2    | 1    | 2 * 4 + 1 = 9               |
| M[2][2]  | 2    | 2    | 2 * 4 + 2 = 10              |
| M[2][3]  | 2    | 3    | 2 * 4 + 3 = 11              |

### Row-Major em CUDA

Em CUDA, o *row-major layout* é utilizado na linearização de *arrays* multidimensionais dinâmicos no *device*, tanto no *host* quanto no *kernel*:

1. **No Host:**
   Durante a cópia de dados do *host* para o *device*, o *array* 2D alocado dinamicamente é linearizado usando o *row-major layout* e copiado para um *array* 1D no *device*:

   ```c++
       float* host_array_2d; // Array 2D alocado dinamicamente no host
       float* host_array_1d; // Array 1D para cópia
       int rows, cols; // dimensões do array 2D
   
       // Alocar host_array_1d com tamanho rows * cols
       host_array_1d = (float*) malloc(rows * cols * sizeof(float));
   
       for (int j = 0; j < rows; j++) {
           for (int i = 0; i < cols; i++) {
              int linear_index = j * cols + i;
              host_array_1d[linear_index] = host_array_2d[j * cols + i];
           }
       }
   
       // Copiar host_array_1d para device_array_1d
       cudaMemcpy(device_array_1d, host_array_1d, rows * cols * sizeof(float), cudaMemcpyHostToDevice);
   ```

2. **No Kernel:**
   No *kernel*, os índices lineares são calculados com base nos índices globais dos *threads* e nas dimensões do *array* usando a mesma fórmula do row-major layout:

   ```c++
   __global__ void kernel(float* data, int rows, int cols) {
       int row = blockIdx.y * blockDim.y + threadIdx.y;
       int col = blockIdx.x * blockDim.x + threadIdx.x;
   
       if (row < rows && col < cols) {
          int linear_index = row * cols + col;
          data[linear_index] = data[linear_index] * 2.0f;
       }
   }
   ```

### Uso de Pointers e o Row-Major Layout

É possível trabalhar com *pointers* e o *row-major layout*. Por exemplo, dado um *pointer* `data` para a memória alocada, o acesso ao elemento `M[j][i]` usando *row-major layout* pode ser feito como:

```c++
float value = data[j * cols + i];
```

O cálculo do índice linear `j * cols + i` e a adição do offset ao endereço de memória, com o uso de um *pointer*, é o mecanismo para acessar o elemento correto na memória.

### Vantagens do Row-Major Layout

O *row-major layout* oferece algumas vantagens:

1.  **Compatibilidade com C/C++:** O *row-major layout* é o padrão para armazenar *arrays* multidimensionais em C/C++, o que facilita a integração do código CUDA com código C/C++ [^8].

2.  **Simplicidade no cálculo de índices:** A fórmula para calcular o índice linear usando o *row-major layout* é relativamente simples e fácil de entender e implementar.

3.  **Eficiência em certos casos:** Em algumas aplicações, como o processamento de imagens onde as operações são feitas linha por linha, o *row-major layout* é naturalmente mais eficiente.

### Desvantagens do Row-Major Layout

Embora o *row-major layout* seja amplamente utilizado, também apresenta algumas desvantagens:

1.  **Acessos não coalescidos em algumas operações:** Em algumas operações que envolvem o acesso a colunas de um *array* 2D, o *row-major layout* pode levar a acessos não coalescidos à memória global, o que pode reduzir o desempenho. A mesma coisa ocorre quando o processamento é feito coluna por coluna.

2.  **Complexidade em certos casos:** Em algumas aplicações com padrões de acesso mais complexos, pode ser necessário transpor o *array* ou utilizar outras formas de acesso para otimizar o desempenho.

**Lemma 2:** *O row-major layout é um esquema eficiente para linearizar arrays multidimensionais, mas pode levar a acessos não coalescidos à memória em alguns casos, especialmente ao processar colunas de um array 2D.*

**Prova:**
O *row-major layout* armazena dados sequencialmente por linhas. Quando o processamento ocorre também por linhas (o que é comum em aplicações de processamento de imagem), os acessos à memória se tornam contíguos e eficientes. Contudo, quando o processamento é por colunas, cada thread precisa acessar dados não contíguos, o que causa acessos não coalescidos e perda de desempenho. $\blacksquare$

**Corolário 2:** *A escolha entre row-major e column-major, ou até mesmo um mapeamento diferente, deve levar em conta o padrão de acesso à memória da aplicação para maximizar o desempenho.*

### Conclusão

O *row-major layout* é um esquema fundamental para a linearização de *arrays* multidimensionais em CUDA, e sua compreensão é essencial para criar aplicações eficientes que utilizam dados dinâmicos. A correta aplicação da fórmula de cálculo dos índices lineares, tanto no *host* quanto no *kernel*, é fundamental para garantir a funcionalidade do código e o acesso adequado aos dados. Embora possa apresentar desvantagens em determinados cenários, o *row-major layout* é a forma padrão de linearização utilizada em CUDA e, portanto, um conhecimento essencial.

### Referências

[^1]: "Fine-grained, data-parallel threads are the fundamental means of parallel execution in CUDA. As we explained in Chapter 3, launching a CUDA kernel creates a grid of threads that all execute the kernel function. That is, the kernel function specifies the C statements that are executed by each individual thread at runtime." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*

[^2]: "Recall from Chapter 3 that all CUDA threads in a grid execute the same kernel function and they rely on coordinates to distinguish themselves from each other and to identify the appropriate portion of the data to pro- cess." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*

[^3]: "In general, a grid is a 3D array of blocks¹ and each block is a 3D array of threads." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*

[^4]: "For example, in a CUDA kernel function, gridDim, blockDim, blockIdx, and threadIdx are all built-in variables." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*

[^5]: "Each threadIdx also consists of three fields: the x coordinate threadId.x, the y coordinate threadIdx.y, and the z coordinate threadIdx.z." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*

[^6]: "That is, we will generate 80 × 64 threads to process 76 × 62 pixels." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*
[^8]: "In reality, all multidimensional arrays in C are linearized. This is due to the use of a “flat” memory space in modern computers (see “Memory Space” sidebar)." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*
[^9]: "Another way to linearize a 2D array is to place all elements of the same column into consecutive locations. The columns are then placed one after another into the memory space. This arrangement, called column- major layout, is used by FORTRAN compilers." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*
[^26]: "We now turn our attention to the work done by each thread. Recall that d_PRow, Col is the inner product of the Row row of d_M and the Col column of d_N. In Figure 4.7, we use a for loop to perform this inner product operation. Before we enter the loop, we initialize a local variable Pvalue to 0. Each iteration of the loop accesses an element in the Row row of d_M, an element in the Col column of d_N, multiplies the two elements together, and accumulates the product into Pvalue." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*

## Column-Major Layout em CUDA

<imagem: Diagrama ilustrando o column-major layout com um exemplo de um array 2D (matriz) sendo linearizado em um array 1D. Inclua as posições dos elementos, os índices no array 2D e os correspondentes índices no array 1D, com anotações detalhadas da fórmula de cálculo do índice.>

### Introdução

Em CUDA, embora o *row-major layout* seja o esquema de linearização padrão para *arrays* multidimensionais, o **column-major layout** também é relevante, especialmente ao interagir com código legado ou bibliotecas que utilizam esse esquema [^9]. Este capítulo explora o *column-major layout*, seu funcionamento, a fórmula para calcular os índices lineares, sua comparação com o *row-major layout* e suas implicações no processamento de dados em CUDA.

### Conceitos Fundamentais

Para compreender o *column-major layout* em CUDA, é essencial relembrar e aprofundar os seguintes conceitos:

**Conceito 1: Linearização de Arrays Multidimensionais**

A **linearização de *arrays* multidimensionais** é o processo de mapear um *array* com mais de uma dimensão para um *array* unidimensional. Essa transformação é necessária para armazenar dados multidimensionais na memória linear do computador. Tanto o *row-major layout* quanto o *column-major layout* são esquemas que realizam essa transformação [^8].

**Lemma 1:** *A linearização de arrays multidimensionais é necessária devido à natureza linear da memória, e diferentes esquemas, como o row-major e o column-major, afetam a ordem em que os elementos são armazenados.*

**Prova:**
A memória do computador é linear e sequencial. Por isso, é necessária uma forma de armazenar os dados de *arrays* multidimensionais de forma linear. Tanto o *row-major layout*, quanto o *column-major layout*, garantem que os dados multidimensionais sejam armazenados em uma sequência contígua na memória, mas com ordens de leitura distintas. $\blacksquare$

**Conceito 2: Row-Major vs. Column-Major**

O **row-major layout** armazena os elementos do *array* linha por linha, enquanto o **column-major layout** armazena os elementos coluna por coluna. A escolha do esquema afeta o mapeamento entre os índices lógicos (multidimensionais) e o índice físico (linear). Em C/C++, o *row-major layout* é o padrão, enquanto que em FORTRAN, é utilizado o *column-major layout* [^9].

**Corolário 1:** *A escolha entre o row-major e o column-major layout define a ordem de armazenamento dos dados na memória, e a consistência no uso do mesmo esquema tanto no host quanto no device é fundamental.*

**Conceito 3: Interoperabilidade com Bibliotecas e Linguagens**

A interoperabilidade com **bibliotecas** e **linguagens** é um aspecto crítico na computação de alto desempenho. Ao utilizar bibliotecas que usam o *column-major layout*, o programador CUDA deve garantir que os dados sejam organizados de forma apropriada para evitar erros e perdas de desempenho [^9].

> ⚠️ **Nota Importante**: O *column-major layout* é uma forma alternativa de linearizar *arrays* multidimensionais e é relevante quando se trabalha com bibliotecas ou código legado que utilizam esse esquema de armazenamento [^9].

### Funcionamento do Column-Major Layout

No **column-major layout**, os elementos de um *array* 2D (matriz) são armazenados na memória da seguinte forma:

1.  Os elementos da primeira coluna são armazenados sequencialmente.
2.  Os elementos da segunda coluna são armazenados sequencialmente após a primeira coluna.
3.  Esse processo continua para todas as colunas subsequentes.

Para um *array* 2D `M` com `rows` linhas e `cols` colunas, o elemento `M[j][i]` (onde *j* é o índice da linha e *i* é o índice da coluna) é mapeado para o índice linear na memória usando a seguinte fórmula:

$$
\text{linear\_index} = i \times \text{rows} + j
$$

Onde:

-   `linear_index` é o índice linear do elemento no *array* 1D equivalente.
-   `i` é o índice da coluna (0 até `cols` - 1).
-   `rows` é o número total de linhas no *array* 2D.
-   `j` é o índice da linha (0 até `rows` - 1).

### Exemplo Prático

Considere o mesmo *array* 2D de inteiros com 3 linhas e 4 colunas do exemplo anterior:

```
M = {
    {1, 2, 3, 4},
    {5, 6, 7, 8},
    {9, 10, 11, 12}
};
```

No *column-major layout*, esse *array* seria armazenado na memória como um *array* 1D da seguinte forma:

```
M_linear = {1, 5, 9, 2, 6, 10, 3, 7, 11, 4, 8, 12}
```

O mapeamento dos elementos para o índice linear é feito da seguinte forma:

| Elemento | j    | i    | linear_index = i * rows + j |
| -------- | ---- | ---- | --------------------------- |
| M[0][0]  | 0    | 0    | 0 * 3 + 0 = 0               |
| M[1][0]  | 1    | 0    | 0 * 3 + 1 = 1               |
| M[2][0]  | 2    | 0    | 0 * 3 + 2 = 2               |
| M[0][1]  | 0    | 1    | 1 * 3 + 0 = 3               |
| M[1][1]  | 1    | 1    | 1 * 3 + 1 = 4               |
| M[2][1]  | 2    | 1    | 1 * 3 + 2 = 5               |
| M[0][2]  | 0    | 2    | 2 * 3 + 0 = 6               |
| M[1][2]  | 1    | 2    | 2 * 3 + 1 = 7               |
| M[2][2]  | 2    | 2    | 2 * 3 + 2 = 8               |
| M[0][3]  | 0    | 3    | 3 * 3 + 0 = 9               |
| M[1][3]  | 1    | 3    | 3 * 3 + 1 = 10              |
| M[2][3]  | 2    | 3    | 3 * 3 + 2 = 11              |

### Column-Major em CUDA

Assim como o *row-major layout*, o *column-major layout* também pode ser implementado tanto no *host* quanto no *kernel*:

1. **No Host:**
    Durante a cópia de dados do *host* para o *device*, o *array* 2D alocado dinamicamente é linearizado usando o *column-major layout* e copiado para um *array* 1D no *device*:

   ```c++
       float* host_array_2d; // Array 2D alocado dinamicamente no host
       float* host_array_1d; // Array 1D para cópia
       int rows, cols; // dimensões do array 2D
   
       // Alocar host_array_1d com tamanho rows * cols
       host_array_1d = (float*) malloc(rows * cols * sizeof(float));
   
       for (int i = 0; i < cols; i++) {
           for (int j = 0; j < rows; j++) {
              int linear_index = i * rows + j;
              host_array_1d[linear_index] = host_array_2d[j][i];
           }
       }
   
       // Copiar host_array_1d para device_array_1d
       cudaMemcpy(device_array_1d, host_array_1d, rows * cols * sizeof(float), cudaMemcpyHostToDevice);
   ```

2. **No Kernel:**

   No *kernel*, os índices lineares são calculados com base nos índices globais dos *threads* e nas dimensões do *array* usando a fórmula do *column-major layout*:

   ```c++
   __global__ void kernel(float* data, int rows, int cols) {
       int row = blockIdx.y * blockDim.y + threadIdx.y;
       int col = blockIdx.x * blockDim.x + threadIdx.x;
   
       if (row < rows && col < cols) {
          int linear_index = col * rows + row;
          data[linear_index] = data[linear_index] * 2.0f;
       }
   }
   ```

### Uso de Pointers e o Column-Major Layout

Semelhante ao *row-major layout*, o acesso a elementos utilizando *pointers* e *column-major layout* também é possível:

```c++
float value = data[i * rows + j];
```

Aqui, a fórmula de cálculo do índice linear é usada diretamente para acessar a porção correta da memória, através do *pointer* `data`.

### Comparação com o Row-Major Layout

As principais diferenças entre o *row-major layout* e o *column-major layout* são:

1.  **Ordem de armazenamento:** O *row-major* armazena elementos linha por linha, enquanto o *column-major* armazena coluna por coluna.

2.  **Cálculo dos índices:** As fórmulas para calcular os índices lineares são diferentes, e a escolha da fórmula errada levará a erros no acesso aos dados.

3.  **Compatibilidade com linguagens e bibliotecas:** C/C++ usam o *row-major*, enquanto FORTRAN utiliza o *column-major*. Ao lidar com código legado ou bibliotecas, é importante garantir que o esquema de linearização seja o correto.

4.  **Desempenho:** Em algumas operações de acesso à memória, um esquema pode ser mais eficiente do que o outro, dependendo do padrão de acesso.

**Lemma 2:** *A escolha entre row-major e column-major layout deve levar em consideração o padrão de acesso à memória da aplicação, pois o acesso contíguo à memória é geralmente mais eficiente do que acessos dispersos.*

**Prova:**
O acesso à memória é mais eficiente quando os dados são acessados de forma sequencial na memória. Ao usar um esquema de linearização que esteja alinhado com o padrão de acesso da aplicação, é possível minimizar a latência e maximizar a largura de banda. $\blacksquare$

**Corolário 2:** *A consistência na utilização do mesmo esquema de linearização (row-major ou column-major) entre o host e o device é fundamental para garantir a corretude e a eficiência de aplicações CUDA que manipulam arrays multidimensionais.*

### Casos de Uso do Column-Major

Embora o *row-major layout* seja mais comum, o *column-major layout* é relevante nos seguintes casos:

1.  **Interoperabilidade com FORTRAN:** Ao trabalhar com bibliotecas escritas em FORTRAN, onde o *column-major layout* é o padrão, pode ser necessário usar esse mesmo esquema em CUDA para evitar a transposição dos dados.
2.  **Interoperabilidade com Bibliotecas BLAS:** Ao usar certas bibliotecas de álgebra linear, como a BLAS, o *column-major layout* pode ser utilizado, pois ele é padrão em algumas implementações dessas bibliotecas.
3.  **Acesso Otimizado a Colunas:** Em algumas aplicações onde o acesso aos dados é feito coluna a coluna, o *column-major layout* pode ser mais eficiente, pois garante acessos contínuos na memória.

### Conclusão

O *column-major layout* é um esquema de linearização alternativo ao *row-major layout*, importante para garantir a interoperabilidade com bibliotecas e código legado. Compreender a fórmula para calcular índices lineares no *column-major layout* e seu funcionamento é essencial para escrever código CUDA correto, eficiente e que possa interagir com diferentes sistemas e bibliotecas. A escolha entre *row-major* e *column-major* deve ser baseada nas necessidades da aplicação e no padrão de acesso à memória.

### Referências

[^1]: "Fine-grained, data-parallel threads are the fundamental means of parallel execution in CUDA. As we explained in Chapter 3, launching a CUDA kernel creates a grid of threads that all execute the kernel function. That is, the kernel function specifies the C statements that are executed by each individual thread at runtime." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*

[^2]: "Recall from Chapter 3 that all CUDA threads in a grid execute the same kernel function and they rely on coordinates to distinguish themselves from each other and to identify the appropriate portion of the data to pro- cess." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*

[^3]: "In general, a grid is a 3D array of blocks¹ and each block is a 3D array of threads." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*

[^4]: "For example, in a CUDA kernel function, gridDim, blockDim, blockIdx, and threadIdx are all built-in variables." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*

[^5]: "Each threadIdx also consists of three fields: the x coordinate threadId.x, the y coordinate threadIdx.y, and the z coordinate threadIdx.z." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*

[^6]: "That is, we will generate 80 × 64 threads to process 76 × 62 pixels." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*

[^7]: "Unfortunately, this information is not known at compiler time for dynami- cally allocated arrays. In fact, part of the reason why one uses dynamically allocated arrays is to allow the sizes and dimensions of these arrays to vary according to data size at runtime. Thus, the information on the num- ber of columns in a dynamically allocated 2D array is not known at com- pile time by design." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*
[^8]: "In reality, all multidimensional arrays in C are linearized. This is due to the use of a “flat” memory space in modern computers (see “Memory Space” sidebar)." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*
[^9]: "Another way to linearize a 2D array is to place all elements of the same column into consecutive locations. The columns are then placed one after another into the memory space. This arrangement, called column- major layout, is used by FORTRAN compilers." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*

## Mapeamento de Threads para Dados no `pictureKernel()`

<imagem: Diagrama mostrando o mapeamento de threads para dados no `pictureKernel()`, com um grid 2D de blocos e cada bloco contendo um grid 2D de threads, e como esses threads são mapeados para pixels de uma imagem. Inclua a representação gráfica da imagem, os blocos, os threads e as condicionais utilizadas para tratar threads extras, com exemplos de código e anotações detalhadas.>

### Introdução

O `pictureKernel()` é um exemplo de *kernel* CUDA que demonstra como *threads* são mapeadas para elementos de dados em um *array* 2D, representando uma imagem. Este capítulo aprofunda o mapeamento de *threads* para dados no `pictureKernel()`, detalhando o uso de *grids* e *blocos* 2D, o cálculo de índices globais, o uso de condicionais para lidar com *threads extras* e como essa organização permite o processamento eficiente de imagens na GPU.

### Conceitos Fundamentais

Para entender o mapeamento de *threads* para dados no `pictureKernel()`, é essencial relembrar os seguintes conceitos:

**Conceito 1: Grids e Blocos 2D**

Em CUDA, a organização dos *threads* em *grids* e *blocos* é fundamental para o processamento paralelo. Um *grid* 2D é uma estrutura que organiza os *blocos* em duas dimensões, e cada *bloco* contém *threads* organizados também em duas dimensões. Essa estrutura hierárquica permite explorar o paralelismo disponível em GPUs, mapeando cada *thread* a um elemento dos dados [^6].

**Lemma 1:** *A organização de threads em grids e blocos 2D é adequada para processar arrays bidimensionais, como imagens, permitindo que cada thread seja associada a uma região específica dos dados.*

**Prova:**
A estrutura de um *array* bidimensional (como uma imagem) pode ser naturalmente mapeada em um grid 2D de blocos, e cada bloco em um array 2D de threads. Isso permite que cada thread calcule o seu índice de linha e coluna nos dados e processe a porção correspondente do *array*, de forma paralela, em um *kernel* CUDA. $\blacksquare$

**Conceito 2: Índices Globais e Locais de Threads**

Cada *thread* possui índices **locais** (dentro do *bloco*), representados por `threadIdx.x` e `threadIdx.y`, e índices **globais** (dentro do *grid*), que são calculados com base em `blockIdx`, `threadIdx` e as dimensões do *grid* e do *bloco*. Os índices globais são utilizados para mapear as *threads* para os dados, enquanto os índices locais são utilizados no compartilhamento de dados dentro do bloco [^7].

**Corolário 1:** *A combinação dos índices globais e locais permite que cada thread identifique sua porção específica de dados e também acesse os dados que compartilham com outros threads dentro do mesmo bloco.*

**Conceito 3: Tratamento de Threads Extras**

Ao utilizar *grids* e *blocos* com dimensões que não correspondem exatamente aos tamanhos dos dados, é comum que algumas *threads* se tornem extras e não correspondam a elementos válidos dos dados. Nesses casos, é necessário usar instruções **condicionais** para evitar que essas *threads* executem operações de processamento em posições de memória inválidas [^7].

> ⚠️ **Nota Importante**: O `pictureKernel()` utiliza *grids* e *blocos* 2D para o processamento de imagens, combinando índices globais, locais e condicionais para garantir que cada thread processe sua porção de dados e evite acessar posições inválidas [^7].

### Mapeamento de Threads no `pictureKernel()`

O `pictureKernel()` exemplifica como as *threads* são mapeadas para *pixels* de uma imagem usando um *grid* e *blocos* 2D. O *kernel* recebe como entrada um *array* 1D que representa uma imagem (linearizada usando o esquema row-major), suas dimensões (largura `n` e altura `m`), e um *array* 1D de saída, onde cada thread irá escrever seu resultado.

1. **Configuração do Grid e Blocos:**
   O código *host* configura as dimensões dos blocos `dimBlock` como fixas em 16x16 e define as dimensões do *grid* `dimGrid`, que depende do tamanho da imagem:

   ```c++
   dim3 dimBlock(16, 16, 1);
   dim3 dimGrid(ceil(n / 16.0), ceil(m / 16.0), 1);
   pictureKernel<<<dimGrid, dimBlock>>>(d_Pin, d_Pout, n, m);
   ```

   Aqui, a função `ceil` garante que o número de blocos seja suficiente para cobrir todos os *pixels* da imagem.

2. **Cálculo dos Índices de Linha e Coluna:**
   Dentro do *kernel*, os índices de linha (`Row`) e coluna (`Col`) são calculados com base nas variáveis *built-in* `blockIdx`, `threadIdx` e `blockDim`:

   ```c++
   __global__ void pictureKernel(float* d_Pin, float* d_Pout, int n, int m) {
     int Row = blockIdx.y * blockDim.y + threadIdx.y;
     int Col = blockIdx.x * blockDim.x + threadIdx.x;
   
     // ...
   }
   ```

   A variável `Row` representa a linha global que cada thread irá processar e `Col` a coluna.

3. **Uso de Condicionais para Tratamento de Threads Extras:**
   O *kernel* utiliza uma condicional para verificar se os índices de linha e coluna estão dentro dos limites da imagem, evitando que *threads extras* acessassem posições inválidas da memória:

   ```c++
       if ((Row < m) && (Col < n)) {
          // Processar o pixel
          int index = Row * n + Col;
           d_Pout[index] = 2 * d_Pin[index];
       }
   ```

   Essa verificação garante que apenas as *threads* que correspondem a *pixels* válidos na imagem realizem a operação de processamento. A expressão `(Row < m) && (Col < n)` garante que as linhas e colunas estão dentro dos limites da imagem (m é o número de linhas e n é o número de colunas).

4. **Acesso aos Dados:**
   O acesso aos *pixels* da imagem é realizado através do índice linearizado calculado a partir dos índices globais da *thread*:

   ```c++
   int index = Row * n + Col;
   d_Pout[index] = 2*d_Pin[index];
   ```

   Essa linha mostra que a *thread* processa um *pixel* cuja posição linear é dada por `index`. O valor desse *pixel* é lido do *array* `d_Pin`, multiplicado por 2 e armazenado em `d_Pout`.

### Análise do Mapeamento

O mapeamento dos *threads* para os *pixels* no `pictureKernel()` é feito da seguinte forma:

-   Cada *thread* é responsável por processar um único *pixel* da imagem.
-   As coordenadas do *pixel* são calculadas com base nos índices `blockIdx`, `threadIdx` e nas dimensões dos *blocos*.
-   O *array* de imagem é tratado como um *array* 1D linearizado, utilizando o *row-major layout*.
-   A condicional `if` garante que as *threads extras* que não correspondem a nenhum *pixel* não realizem nenhuma operação.

### Tratamento de Threads Extras em Diferentes Áreas da Imagem

Ao executar o `pictureKernel()`, dependendo das dimensões da imagem, os *threads* se comportam de formas diferentes, que podem ser categorizadas em quatro áreas:

1.  **Área Principal (Cobertura Total):**
     Essa área corresponde aos *blocos* que cobrem completamente a imagem, onde cada *thread* dentro desses *blocos* processa um *pixel* válido. As condicionais `if` retornam verdadeiro e todas as *threads* nesse *bloco* processam um *pixel* da imagem.

2.  **Área Superior-Direita (Sobras na Largura):**
    Essa área corresponde aos *blocos* que cobrem a borda superior direita da imagem. As *threads* nessa área têm o valor de `Col` que pode exceder o tamanho da largura `n` da imagem, e portanto o `if` retorna falso e elas não fazem nenhum processamento. As *threads* que têm o valor de `Col` dentro da largura da imagem processam o *pixel* correspondente.

3.  **Área Inferior-Esquerda (Sobras na Altura):**
    Essa área corresponde aos *blocos* que cobrem a borda inferior esquerda da imagem. Similar à área anterior, as *threads* podem ter o valor de `Row` que excede a altura `m` da imagem, e o `if` faz com que essas *threads* não processem nada. As *threads* que têm o valor de `Row` dentro da altura da imagem processam o *pixel* correspondente.

4.  **Área Inferior-Direita (Sobras na Largura e Altura):**
      Essa área corresponde aos *blocos* que cobrem a borda inferior direita da imagem. Os valores de `Row` e `Col` podem exceder, e assim, os *threads* nessa área não processam nada. As *threads* que se localizam dentro da área válida da imagem, processam o *pixel* correspondente.

Essa análise mostra que a escolha do tamanho do bloco e as condicionais dentro do *kernel* são cruciais para um processamento correto da imagem e para evitar acessos inválidos à memória.

**Lemma 2:** *A utilização de condicionais no `pictureKernel()` garante que somente as threads que correspondem a pixels válidos realizem o processamento, evitando acessos incorretos à memória e garantindo a integridade da imagem resultante.*

**Prova:**
Ao adicionar a condicional `if ((Row < m) && (Col < n))`, o código verifica a validade das coordenadas, garantindo que apenas threads correspondentes a pixels válidos acessem a região da memória e processem o pixel. $\blacksquare$

**Corolário 2:** *A combinação de grids e blocos 2D com cálculos de índices globais e condicionais permite o mapeamento preciso de threads para dados em imagens, com um tratamento eficiente de threads extras que não correspondem a nenhum pixel.*

### Considerações sobre Desempenho

Embora as condicionais sejam necessárias para lidar com *threads extras*, elas podem apresentar um pequeno custo de desempenho devido ao desvio de fluxo e ramificações nas *threads*. No entanto, para a maioria das aplicações práticas, a perda de desempenho é mínima e é compensada pela correção da execução e pela flexibilidade oferecida pelo mapeamento *thread-dados*. Para maximizar o desempenho é recomendado otimizar a quantidade de operações dentro do *if*.

### Conclusão

O `pictureKernel()` ilustra como usar *grids* e *blocos* 2D em CUDA para processar dados bidimensionais, utilizando condicionais para garantir a correta execução de todas as *threads*, especialmente aquelas que não têm um *pixel* correspondente na imagem. A combinação do mapeamento de *threads*, o cálculo de índices globais e as verificações condicionais mostram o funcionamento eficiente da computação paralela em CUDA, em especial para processamento de imagens. Ao compreender a organização dos *threads* e os mecanismos de controle, os programadores podem otimizar as aplicações e explorar o paralelismo máximo nas GPUs.

### Referências

[^1]: "Fine-grained, data-parallel threads are the fundamental means of parallel execution in CUDA. As we explained in Chapter 3, launching a CUDA kernel creates a grid of threads that all execute the kernel function. That is, the kernel function specifies the C statements that are executed by each individual thread at runtime. Each thread uses a unique coordinate, or thread index, to identify the portion of the data structure to process." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*

[^2]: "Recall from Chapter 3 that all CUDA threads in a grid execute the same kernel function and they rely on coordinates to distinguish themselves from each other and to identify the appropriate portion of the data to pro- cess." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*

[^3]: "In general, a grid is a 3D array of blocks¹ and each block is a 3D array of threads." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*

[^4]: "For example, in a CUDA kernel function, gridDim, blockDim, blockIdx, and threadIdx are all built-in variables." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*

[^5]: "Each threadIdx also consists of three fields: the x coordinate threadId.x, the y coordinate threadIdx.y, and the z coordinate threadIdx.z." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*

[^6]: "The choice of 1D, 2D, or 3D thread organizations is usually based on the nature of the data. For example, pictures are a 2D array of pixels. It is often convenient to use a 2D grid that consists of 2D blocks to process the pixels in a picture." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*

[^7]: "Analogously, we should expect that the picture processing kernel function will have if statements to test whether the thread indices threadIdx.x and threadIdx.y fall within the valid range of pixels." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*
[^13]: "We also used vecAddKernel() and pictureKernel() to introduce the phenomenon that the number of threads that we create is a multiple of the block dimension. As a result, we will likely end up with more threads than data elements. Not all threads will process elements of an array. We use an if statement to test if the global index values of a thread are within the valid range." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*

## Condição de Contorno no `pictureKernel()`

<imagem: Diagrama detalhado do comportamento das threads na borda de uma imagem ao usar o `pictureKernel()`. Inclua a representação da imagem, as regiões de blocos com threads que processam a imagem e as regiões com threads que não processam dados, demonstrando como a condição de contorno atua em diferentes áreas.>

### Introdução

No `pictureKernel()`, a **condição de contorno** é um mecanismo essencial para lidar com *threads* que são criadas além do tamanho real dos dados da imagem (as *threads extras*). Essa condição garante que apenas os *threads* que correspondem a *pixels* válidos da imagem realizem o processamento, evitando acessos a posições inválidas de memória e garantindo a correta execução do *kernel*. Este capítulo explora detalhadamente como essa condição de contorno funciona, os diferentes casos que ela abrange e como isso afeta o desempenho do *kernel*.

### Conceitos Fundamentais

Para entender a importância da condição de contorno no `pictureKernel()`, é crucial relembrar os seguintes conceitos:

**Conceito 1: Sobredimensionamento de Threads e Grids**

Em CUDA, é comum utilizar um número de *threads* que é um múltiplo do tamanho do *bloco* e, por isso, frequentemente o número total de *threads* excede o número de elementos de dados. Isso é feito para maximizar o uso dos recursos da GPU e simplificar a lógica do *kernel*. O **sobredimensionamento de *threads*** e *grids* faz com que algumas *threads* fiquem "extras", não processando nenhum dado útil [^6].

**Lemma 1:** *O sobredimensionamento de threads e grids, embora conveniente, exige mecanismos de controle para garantir que apenas threads válidas processem os dados e que acessos inválidos à memória sejam evitados.*

**Prova:**
O uso de um número de *threads* que é um múltiplo do tamanho do *bloco* é uma forma eficiente de garantir que todos os *multiprocessadores de streaming* (SMs) da GPU sejam utilizados, maximizando o paralelismo. No entanto, isso faz com que um número maior de *threads* do que o número de elementos de dados sejam criadas, e assim se torna necessário o uso de condicionais para evitar que essas *threads* extras causem problemas no acesso à memória. $\blacksquare$

**Conceito 2: Índices Globais de Threads em 2D**

Em um *grid* 2D, cada *thread* é identificada por seus índices globais (`Row` e `Col`), que são calculados com base nos índices de *bloco* (`blockIdx`) e de *thread* dentro do *bloco* (`threadIdx`) nas dimensões x e y. Esses índices são utilizados para mapear os *threads* para as coordenadas de um *pixel* na imagem [^7].

**Corolário 1:** *Os índices globais de threads em 2D permitem que cada thread seja associada a uma posição específica em um array bidimensional, mas requerem mecanismos de controle para lidar com threads extras.*

**Conceito 3: Condições de Contorno**

As **condições de contorno** são instruções condicionais que verificam se os índices globais dos *threads* estão dentro dos limites válidos dos dados. No contexto do `pictureKernel()`, a condição de contorno verifica se os índices de linha e coluna de uma *thread* correspondem a um *pixel* válido dentro da imagem [^7].

> ⚠️ **Nota Importante**: As condições de contorno no `pictureKernel()` são fundamentais para garantir que apenas os threads que correspondem a posições de *pixels* válidos dentro da imagem realizem o processamento, prevenindo erros de acesso à memória e garantindo a integridade dos resultados [^7].

### Funcionamento da Condição de Contorno no `pictureKernel()`

No `pictureKernel()`, a condição de contorno é implementada através de uma instrução condicional `if` que verifica se os índices `Row` e `Col` estão dentro das dimensões da imagem (altura `m` e largura `n`):

```c++
__global__ void pictureKernel(float* d_Pin, float* d_Pout, int n, int m) {
  int Row = blockIdx.y * blockDim.y + threadIdx.y;
  int Col = blockIdx.x * blockDim.x + threadIdx.x;

    if ((Row < m) && (Col < n)) {
        // Processar o pixel
        d_Pout[Row * n + Col] = 2 * d_Pin[Row * n + Col];
    }
}
```

Essa condição é crucial para o funcionamento correto do *kernel* e, portanto, seu comportamento é fundamental para entender a estrutura do código. Ela pode ser analisada nas diferentes regiões da imagem:

1.  **Região Interna:** Na região onde as *threads* mapeiam para *pixels* dentro da imagem, tanto a linha `Row` quanto a coluna `Col` são menores que a altura `m` e a largura `n`, respectivamente. Nesses casos, a condição `(Row < m) && (Col < n)` é verdadeira e a *thread* processa o *pixel* correspondente.

2.  **Região da Borda Inferior:** Nas linhas inferiores da imagem, onde as *threads* mapeiam para uma região onde a linha `Row` é maior ou igual à altura `m` e a coluna `Col` é menor que a largura `n`, a condição `Row < m` se torna falsa. Consequentemente, a condição geral `(Row < m) && (Col < n)` também se torna falsa, fazendo com que a *thread* não processe nada.

3.  **Região da Borda Direita:** Nas colunas da borda direita da imagem, onde os *threads* mapeiam para regiões onde a coluna `Col` é maior ou igual à largura `n` e a linha `Row` é menor que a altura `m`, a condição `Col < n` se torna falsa. Similar à região anterior, a condição geral é falsa, e a *thread* não processa nada.

4.  **Região da Borda Inferior Direita:** Nos cantos inferiores direitos da imagem, onde os *threads* mapeiam para regiões onde tanto a linha `Row` é maior ou igual a altura `m` e a coluna `Col` é maior ou igual à largura `n`, ambas as condições, `Row < m` e `Col < n` são falsas, fazendo com que a *thread* não processe nada.

### Análise Detalhada da Condição

A condição `(Row < m) && (Col < n)` é uma condição composta que utiliza o operador lógico `&&` (E lógico). A tabela-verdade do operador `&&` é:

| Condição 1 (Row < m) | Condição 2 (Col < n) | (Row < m) && (Col < n) |
| -------------------- | -------------------- | ---------------------- |
| Verdadeira           | Verdadeira           | Verdadeira             |
| Verdadeira           | Falsa                | Falsa                  |
| Falsa                | Verdadeira           | Falsa                  |
| Falsa                | Falsa                | Falsa                  |

O processamento só ocorre se ambas as condições, `Row < m` e `Col < n`, forem verdadeiras. Isso garante que apenas os *threads* que estão dentro dos limites da imagem processem os *pixels* correspondentes.

### Impacto no Desempenho

A condição de contorno, embora essencial para a correção do código, pode ter um pequeno impacto no desempenho devido ao desvio de fluxo e ramificação. As *threads* que não correspondem a *pixels* válidos ainda são executadas, mas não acessam a memória nem realizam operações de processamento. Para minimizar o impacto no desempenho:

1.  **Escolha Adequada do Tamanho do Bloco:** Escolha tamanhos de *bloco* que reduzam a quantidade de *threads extras*. Por exemplo, para uma imagem 76x62, utilizar um tamanho de *bloco* 16x16 gera um menor número de *threads extras* comparado com usar blocos 32x32.

2.  **Otimização de Condicionais:** Tente manter a condicional o mais simples possível para reduzir o tempo de execução do *kernel*.

3.  **Mínima Computação Dentro da Condicional:** Realize apenas as operações necessárias dentro da condicional, para minimizar o custo das *threads extras*.

Embora a condição de contorno seja fundamental para garantir que o *kernel* processe os dados corretamente, o programador deve buscar maneiras de otimizar seu uso para minimizar o impacto no desempenho.

### Comparação com Abordagens Alternativas

Outras abordagens para lidar com *threads extras* incluem:

1.  **Geração de *Threads* Exatas:** Ajustar o tamanho do *grid* para que o número de *threads* corresponda exatamente ao número de *pixels*, usando funções como `ceil`, `floor` ou cálculos mais complexos para obter o número exato de blocos necessários. Contudo, essa estratégia geralmente gera um código mais complexo e difícil de manter.

2.  **Padding de Imagem:** Preencher a imagem com *pixels* extras para que o tamanho da imagem se torne um múltiplo do tamanho do *bloco*. Essa estratégia também pode adicionar sobrecarga para a cópia e manipulação dos dados, bem como a necessidade de um novo cálculo dos índices dentro do *kernel*.

Comparativamente, a condição de contorno através da condicional `if` é a mais simples, flexível e geralmente a mais eficiente em termos de custo/benefício para lidar com *threads extras* em CUDA.

**Lemma 3:** *A utilização de uma condicional para verificar o índice da thread dentro do kernel é a forma mais simples e flexível de lidar com threads extras, e garante a execução correta do kernel em diferentes situações.*

**Prova:**
A condicional `if ((Row < m) && (Col < n))` permite que o kernel verifique, para cada thread, se a sua região de processamento é válida e, com isso, evitar acessos inválidos à memória. A escolha adequada dos parâmetros de lançamento também garante a eficiência do processo. $\blacksquare$

**Corolário 2:** *A combinação de um lançamento com sobreposição de threads, com uma condição de contorno, oferece uma forma robusta e eficiente de garantir a correção da execução do kernel em CUDA, ao mesmo tempo em que mantém um código simples e legível.*

### Conclusão

A condição de contorno no `pictureKernel()` é fundamental para o seu funcionamento correto, pois ela permite que o *kernel* processe dados corretamente e evite erros de acesso à memória, ao mesmo tempo em que explora o paralelismo massivo oferecido pela GPU. Compreender seu funcionamento, os diferentes casos que ela abrange e seu impacto no desempenho é essencial para o desenvolvimento de aplicações CUDA eficientes, escaláveis e robustas. A escolha de tamanho dos *blocos*, juntamente com o uso adequado de condicionais, garante o funcionamento eficiente da aplicação.

### Referências

[^1]: "Fine-grained, data-parallel threads are the fundamental means of parallel execution in CUDA. As we explained in Chapter 3, launching a CUDA kernel creates a grid of threads that all execute the kernel function. That is, the kernel function specifies the C statements that are executed by each individual thread at runtime. Each thread uses a unique coordinate, or thread index, to identify the portion of the data structure to process." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*

[^2]: "Recall from Chapter 3 that all CUDA threads in a grid execute the same kernel function and they rely on coordinates to distinguish themselves from each other and to identify the appropriate portion of the data to pro- cess." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*

[^3]: "In general, a grid is a 3D array of blocks¹ and each block is a 3D array of threads." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*

[^4]: "For example, in a CUDA kernel function, gridDim, blockDim, blockIdx, and threadIdx are all built-in variables." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*

[^5]: "Each threadIdx also consists of three fields: the x coordinate threadId.x, the y coordinate threadIdx.y, and the z coordinate threadIdx.z." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*

[^6]: "That is, we will generate 80 × 64 threads to process 76 × 62 pixels." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*
[^7]: "Analogously, we should expect that the picture processing kernel function will have if statements to test whether the thread indices threadIdx.x and threadIdx.y fall within the valid range of pixels." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*
[^26]: "We now turn our attention to the work done by each thread. Recall that d_PRow, Col is the inner product of the Row row of d_M and the Col column of d_N. In Figure 4.7, we use a for loop to perform this inner product operation. Before we enter the loop, we initialize a local variable Pvalue to 0. Each iteration of the loop accesses an element in the Row row of d_M, an element in the Col column of d_N, multiplies the two elements together, and accumulates the product into Pvalue." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*

## Mapeamento de Threads para Dados em Arrays 3D em CUDA

<imagem: Diagrama ilustrando o mapeamento de threads para dados em arrays 3D, mostrando a estrutura hierárquica de grids, blocos e threads, e como esses threads são mapeados para elementos de um volume tridimensional. Inclua as fórmulas para calcular os índices lineares e anotações detalhadas.>

### Introdução

Em CUDA, o processamento de *arrays* tridimensionais (3D) é fundamental em diversas aplicações, como simulações científicas, processamento de imagens médicas e visualização de dados volumétricos. O mapeamento eficiente de *threads* para os elementos de *arrays* 3D requer uma compreensão clara de como as estruturas hierárquicas do CUDA (grids, blocos e threads) interagem e como os índices globais são calculados. Este capítulo explora em detalhes o mapeamento de *threads* para dados em *arrays* 3D, com foco na linearização desses *arrays*, no cálculo de índices globais e no uso de condicionais para lidar com *threads extras*.

### Conceitos Fundamentais

Para entender o mapeamento de *threads* para dados em *arrays* 3D em CUDA, é essencial compreender os seguintes conceitos:

**Conceito 1: Arrays Tridimensionais (3D)**

Um **array tridimensional (3D)** é uma estrutura de dados que organiza elementos em três dimensões, como largura, altura e profundidade. Em CUDA, o processamento de *arrays* 3D é frequentemente encontrado em simulações científicas e no processamento de dados volumétricos [^12].

**Lemma 1:** *Arrays tridimensionais são necessários para representar e processar dados com estrutura volumétrica, como modelos 3D ou simulações físicas.*

**Prova:**
Arrays tridimensionais permitem que os programadores representem dados que possuem três dimensões espaciais. Sem essa estrutura, o mapeamento de dados tridimensionais em estruturas 1D ou 2D seria complexo e ineficiente. $\blacksquare$

**Conceito 2: Linearização de Arrays 3D**

Assim como *arrays* 2D, *arrays* 3D também precisam ser **linearizados** para serem armazenados na memória linear. A linearização mapeia os elementos do *array* 3D para um *array* 1D equivalente. O processo de linearização é necessário para acesso correto à memória.

**Corolário 1:** *A linearização permite que arrays 3D sejam armazenados na memória linear, e o programador precisa escolher um esquema de linearização consistente para acessar os dados corretamente.*

**Conceito 3: Grids e Blocos 3D**

Em CUDA, *grids* e *blocos* podem ser organizados em três dimensões, o que é adequado para problemas que envolvem *arrays* 3D. Um *grid* 3D é composto por *blocos* 3D, e cada *bloco* contém *threads* organizadas em três dimensões. O uso de *grids* e *blocos* 3D reflete melhor a natureza dos dados a serem processados [^4].

> ⚠️ **Nota Importante**: O processamento de dados em *arrays* 3D em CUDA utiliza *grids* e *blocos* 3D para mapear *threads* para os elementos do *array* de forma eficiente, juntamente com a linearização dos dados para o acesso à memória [^4].

### Linearização de Arrays 3D

A linearização de um *array* 3D em um *array* 1D envolve um mapeamento dos índices tridimensionais para um índice linear único. A fórmula para calcular o índice linear no *row-major layout* é:

$$
\text{linear\_index} = \text{plane} \times \text{rows} \times \text{cols} + \text{row} \times \text{cols} + \text{col}
$$

Onde:

-   `linear_index` é o índice linear no *array* 1D equivalente.
-   `plane` é o índice do plano (a profundidade, ou a terceira dimensão).
-   `rows` é o número de linhas no *array* 3D.
-   `cols` é o número de colunas no *array* 3D.
-   `row` é o índice da linha (0 até `rows` - 1).
-   `col` é o índice da coluna (0 até `cols` - 1).

O termo `plane * rows * cols` pula todos os elementos dos planos anteriores. O termo `row * cols` pula todos os elementos das linhas anteriores. O termo `col` seleciona a coluna correta.

### Mapeamento de Threads para Dados em Arrays 3D

Em CUDA, o mapeamento de *threads* para *arrays* 3D envolve os seguintes passos:

1. **Configuração do Grid e Blocos 3D:**
   No *host*, as dimensões do *grid* e do *bloco* são especificadas utilizando o tipo `dim3`. Por exemplo:

   ```c++
   dim3 dimBlock(8, 8, 8);
   dim3 dimGrid(ceil(width/8.0), ceil(height/8.0), ceil(depth/8.0));
   ```

   onde `width`, `height` e `depth` são as dimensões do *array* 3D.

2. **Cálculo dos Índices Globais:**
    Dentro do *kernel*, os índices globais de *plane*, `Row` e `Col` são calculados utilizando os índices de *bloco* (`blockIdx`), *threads* (`threadIdx`) e as dimensões dos *blocos* (`blockDim`):

   ```c++
   __global__ void kernel(float* data, int width, int height, int depth) {
       int plane = blockIdx.z * blockDim.z + threadIdx.z;
       int row = blockIdx.y * blockDim.y + threadIdx.y;
       int col = blockIdx.x * blockDim.x + threadIdx.x;
   
      // ...
   }
   ```

3. **Condição de Contorno:**
   Para lidar com *threads extras* que não mapeiam para um elemento válido do *array* 3D, utiliza-se uma condicional para verificar se os índices globais estão dentro dos limites do *array*:

   ```c++
      if (plane < depth && row < height && col < width) {
       // Processar o elemento
       int linear_index = plane * height * width + row * width + col;
       data[linear_index] = data[linear_index] * 2.0f; // exemplo
      }
   ```

   O termo `plane < depth` verifica se o plano é válido, o termo `row < height` verifica se a linha é válida, e o termo `col < width` verifica se a coluna é válida.

4. **Acesso aos Dados:**
   O acesso aos elementos do *array* 3D linearizado é feito usando o índice linear calculado:

   ```c++
      int linear_index = plane * height * width + row * width + col;
      data[linear_index] = data[linear_index] * 2.0f; // Exemplo
   ```

### Exemplo Prático

Considere o seguinte código para simular um processamento em um *array* 3D:

```c++
__global__ void process3DArray(float* data, int width, int height, int depth) {
   int plane = blockIdx.z * blockDim.z + threadIdx.z;
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (plane < depth && row < height && col < width) {
      int linear_index = plane * height * width + row * width + col;
        data[linear_index] = data[linear_index] * 2.0f;
    }
}

int main() {
    int width = 64;
    int height = 64;
    int depth = 64;
    int size = width * height * depth;

    float* hostData = (float*)malloc(size * sizeof(float));
    for (int i=0; i<size; i++) {
      hostData[i] = i*1.0f;
    }
    float* deviceData;
    cudaMalloc(&deviceData, size*sizeof(float));
    cudaMemcpy(deviceData, hostData, size * sizeof(float), cudaMemcpyHostToDevice);

    dim3 dimBlock(8, 8, 8);
    dim3 dimGrid(ceil(width / 8.0), ceil(height / 8.0), ceil(depth / 8.0));

    process3DArray<<<dimGrid, dimBlock>>>(deviceData, width, height, depth);

     cudaMemcpy(hostData, deviceData, size * sizeof(float), cudaMemcpyDeviceToHost);
      for (int i=0; i<size; i++) {
        std::cout << hostData[i] << std::endl;
      }

     cudaFree(deviceData);
     free(hostData);

     return 0;
}
```

Nesse exemplo:

-   O *array* 3D é alocado dinamicamente no *host* e copiado para o *device*.
-   O *kernel* `process3DArray` processa cada elemento do *array* 3D.
-   Os índices `plane`, `Row` e `Col` são calculados para cada *thread*.
-   A condição de contorno garante que apenas os *threads* dentro do limite do *array* 3D realizem o processamento.
-   A fórmula `linear_index` é utilizada para acessar os elementos do *array* linearizado.

### Vantagens do Mapeamento 3D

O mapeamento 3D oferece diversas vantagens:

1.  **Representação Natural de Dados 3D:** Permite que dados tridimensionais sejam representados e processados de forma mais intuitiva, simplificando o desenvolvimento e tornando o código mais fácil de entender.

2.  **Localidade de Memória:** Em problemas com padrões de acesso que exploram a localidade dos dados, um mapeamento 3D pode apresentar uma melhor utilização da memória e do cache, maximizando a velocidade de execução do *kernel*.

3.  **Redução de Cálculos de Índices:** Permite a utilização das variáveis *built-in* para o cálculo dos índices nas 3 dimensões, sem a necessidade de uma lógica complexa de mapeamento de índices.

### Desafios do Mapeamento 3D

Apesar de suas vantagens, o mapeamento 3D também apresenta alguns desafios:

1.  **Complexidade no cálculo de índices:** O cálculo do índice linear em 3D é mais complexo do que em 2D, o que pode dificultar a otimização do *kernel*.

2.  **Requisitos de hardware:** O uso de *grids* e *blocos* 3D pode ser mais exigente em termos de uso de registradores e *shared memory* nos *SMs*.

3.  **Necessidade de Condicionais:** Para garantir o correto funcionamento, a condição de contorno se torna ainda mais importante para evitar erros no acesso à memória, ao manipular *arrays* 3D.

**Lemma 3:** *A linearização de arrays 3D, juntamente com o mapeamento de threads, permite o processamento eficiente de dados volumétricos em CUDA, desde que as limitações de hardware e os padrões de acesso à memória sejam considerados.*

**Prova:**
O mapeamento 3D permite a utilização eficiente de grids e blocos 3D para explorar o paralelismo dos dados. Contudo, este mapeamento só funciona se o programador garantir o acesso à memória através de uma linearização que mapeie os dados em uma sequência de posições de memória contíguas. $\blacksquare$

**Corolário 3:** *A combinação de grids e blocos 3D, com cálculos de índices apropriados e tratamento adequado das threads extras, resulta em aplicações CUDA robustas e eficientes para o processamento de arrays tridimensionais.*

### Conclusão

O mapeamento de *threads* para dados em *arrays* 3D é um aspecto fundamental da programação CUDA para simulações, visualizações e processamento de dados volumétricos. Ao compreender a estrutura do *grid* e dos *blocos* em 3D, o processo de linearização de *arrays* 3D, o cálculo de índices globais e o uso de condicionais para lidar com *threads extras*, os programadores podem desenvolver aplicações que explorem o poder do processamento paralelo das GPUs para processamento de dados tridimensionais complexos.

### Referências

[^1]: "Fine-grained, data-parallel threads are the fundamental means of parallel execution in CUDA. As we explained in Chapter 3, launching a CUDA kernel creates a grid of threads that all execute the kernel function. That is, the kernel function specifies the C statements that are executed by each individual thread at runtime. Each thread uses a unique coordinate, or thread index, to identify the portion of the data structure to process." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*

[^2]: "Recall from Chapter 3 that all CUDA threads in a grid execute the same kernel function and they rely on coordinates to distinguish themselves from each other and to identify the appropriate portion of the data to pro- cess." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*

[^3]: "In general, a grid is a 3D array of blocks¹ and each block is a 3D array of threads." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*

[^4]: "For example, in a CUDA kernel function, gridDim, blockDim, blockIdx, and threadIdx are all built-in variables." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*

[^5]: "Each threadIdx also consists of three fields: the x coordinate threadId.x, the y coordinate threadIdx.y, and the z coordinate threadIdx.z." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*

[^6]: "The choice of 1D, 2D, or 3D thread organizations is usually based on the nature of the data." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*
[^12]: "We can easily extend our discussion of 2D arrays to 3D arrays by including another dimension when we linearize arrays. This is done by placing each “plane” of the array one after another." *(Trecho do Capítulo 4 - Data-Parallel Execution Model)*