## Warp Scheduling e Tolerância à Latência

### Introdução
Este capítulo explora o conceito de **warp scheduling** como uma técnica fundamental para a tolerância à latência em GPUs. Em continuidade ao capítulo anterior, onde discutimos a organização de threads e a execução de kernels CUDA, agora focaremos em como o hardware da GPU gerencia a execução de threads para maximizar a utilização dos recursos, mesmo na presença de operações de longa latência [^1].

### Conceitos Fundamentais

**Warp Scheduling** é uma técnica utilizada para tolerar latências de operações como aritmética de ponto flutuante pipelinada e instruções de branching [^1]. A ideia central é que, com um número suficiente de *warps* disponíveis, o hardware da GPU provavelmente encontrará um *warp* pronto para executar a qualquer momento, utilizando plenamente os recursos de execução, apesar dessas operações de longa latência [^1].

Um **warp** é um grupo de threads (tipicamente 32) que executam a mesma instrução simultaneamente, seguindo o modelo SIMD (Single Instruction, Multiple Data) [^1]. Quando uma instrução executada pelos threads em um *warp* precisa esperar pelo resultado de uma operação de longa latência iniciada anteriormente, o *warp* não é selecionado para execução [^1]. Em vez disso, outro *warp* residente que não está mais esperando por resultados é selecionado [^1].

Essa capacidade de tolerar longas latências de operação é a principal razão pela qual as GPUs não dedicam tanta área do chip para memórias cache e mecanismos de predição de branch quanto as CPUs [^1]. Como resultado, as GPUs podem dedicar mais área do chip para recursos de execução de ponto flutuante [^1]. O preenchimento do tempo de latência das operações com o trabalho de outros threads é chamado de **tolerância à latência** ou **latency hiding** [^1].

[^88] afirma que uma vez que um bloco é atribuído a um SM, ele é dividido em unidades de 32 threads chamadas warps. O tamanho dos warps é específico da implementação. De fato, os warps não fazem parte da especificação CUDA. No entanto, o conhecimento dos warps pode ser útil para entender e otimizar o desempenho de aplicações CUDA em gerações particulares de dispositivos CUDA.

Para exemplificar, considere um cenário onde um SM tem três blocos atribuídos (Bloco 1, Bloco 2 e Bloco 3), e cada bloco é dividido em *warps* para fins de *scheduling* [^88].

A GPU é projetada para executar todos os threads em um *warp* seguindo o modelo SIMD. Isso significa que, a qualquer momento, uma instrução é buscada e executada para todos os threads no *warp* [^88]. É importante notar que esses threads aplicarão a mesma instrução a diferentes porções dos dados [^88]. Como resultado, todos os threads em um *warp* sempre terão o mesmo tempo de execução [^88].

[^88] também indica um número de processadores de streaming de hardware (SPs) que realmente executam instruções. Em geral, existem menos SPs do que o número de threads atribuídos a cada SM. Isso significa que cada SM tem hardware suficiente apenas para executar instruções de um pequeno subconjunto de todos os threads atribuídos ao SM em qualquer ponto no tempo. Em projetos de GPU anteriores, cada SM podia executar apenas uma instrução para um único warp em qualquer instante. Em projetos mais recentes, cada SM pode executar instruções para um pequeno número de warps em qualquer ponto no tempo.

Se uma instrução executada pelos threads em um *warp* precisa esperar pelo resultado de uma operação de longa latência iniciada anteriormente, o *warp* não é selecionado para execução [^89]. Outro *warp* residente que não está mais esperando por resultados será selecionado para execução. Se mais de um *warp* estiver pronto para execução, um mecanismo de prioridade será usado para selecionar um para execução [^89]. Esse mecanismo de preenchimento do tempo de latência das operações com o trabalho de outros threads é frequentemente chamado de **tolerância à latência** ou **latency hiding** [^89].

Este conceito é crucial para o desempenho em GPUs, pois permite que o hardware oculte as latências inerentes a certas operações, mantendo a alta utilização dos recursos de computação.

### Conclusão

O *warp scheduling* é uma técnica essencial para tolerar a latência em GPUs, permitindo que o hardware oculte as latências de operações de longa duração, como aritmética de ponto flutuante e acessos à memória global [^1]. Ao alternar entre diferentes *warps*, o hardware pode manter a alta utilização dos recursos de computação, mesmo na presença de operações de longa latência [^1]. Essa capacidade é fundamental para o desempenho de aplicações CUDA em GPUs [^1].

### Referências
[^1]: Capítulo 4, Seção 4.7
[^88]: Capítulo 4, página 88
[^89]: Capítulo 4, página 89
<!-- END -->