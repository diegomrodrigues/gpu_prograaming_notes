## Thread Scheduling and Warp Execution in CUDA

### Introdução
Este capítulo explora o conceito de **thread scheduling** no contexto da arquitetura CUDA, com foco em como os *streaming multiprocessors (SMs)* gerenciam a execução de threads para tolerar a latência de operações de memória [^87]. O entendimento do modelo de *warp execution* é crucial para otimizar o desempenho de aplicações CUDA.

### Conceitos Fundamentais
Em CUDA, o *thread scheduling* é um conceito de implementação específico do hardware e, portanto, deve ser discutido no contexto de implementações de hardware específicas [^87]. Uma vez que um bloco é atribuído a um SM, ele é dividido em unidades de 32 threads chamados **warps** [^88]. O *warp* é a unidade de *thread scheduling* nos SMs [^88]. Cada *warp* consiste em 32 threads de valores `threadIdx` consecutivos: threads 0-31 formam o primeiro warp, 32-63 o segundo warp, e assim por diante [^88].

**Organização dos Warps**
Após a atribuição de um bloco a um SM, ele é dividido em unidades de 32 threads chamados warps, que são a unidade de thread scheduling nos SMs [^88].
$$\
\text{Número de warps por bloco} = \frac{\text{Número de threads por bloco}}{32}\
$$
Por exemplo, se um bloco tem 256 threads, ele será dividido em 8 warps [^88]:
$$\
\frac{256 \text{ threads}}{32 \text{ threads/warp}} = 8 \text{ warps}\
$$
Um SM é projetado para executar todos os threads em um warp seguindo o modelo *single instruction, multiple data (SIMD)* [^88]. Isso significa que, a qualquer instante, uma instrução é buscada e executada para todos os threads no warp [^88]. Esses threads aplicarão a mesma instrução a diferentes porções dos dados [^88]. Como resultado, todos os threads em um warp sempre terão o mesmo tempo de execução [^88].

**Hardware Streaming Processors (SPs)**
A Figura 4.14 [^88] também mostra vários *hardware streaming processors (SPs)* que realmente executam instruções. Em geral, existem menos SPs do que o número de threads atribuídos a cada SM [^88]. Isso significa que cada SM tem hardware suficiente para executar instruções de um pequeno subconjunto de todos os threads atribuídos ao SM em qualquer ponto no tempo [^88]. Em projetos de GPU anteriores, cada SM podia executar apenas uma instrução para um único warp em um determinado instante [^88]. Em projetos mais recentes, cada SM pode executar instruções para um pequeno número de warps em um determinado ponto no tempo [^88]. Em ambos os casos, o hardware pode executar instruções para um pequeno subconjunto de todos os warps no SM [^88].

**Tolerância à Latência**
Uma pergunta legítima é: por que precisamos ter tantos warps em um SM se ele pode executar apenas um pequeno subconjunto deles em qualquer instante? [^88] A resposta é que é assim que os processadores CUDA executam eficientemente operações de longa latência, como acessos à memória global [^88]. Quando uma instrução executada pelos threads em um warp precisa esperar pelo resultado de uma operação de longa latência iniciada anteriormente, o warp não é selecionado para execução [^89]. Outro warp residente que não está mais esperando por resultados será selecionado para execução [^89]. Se mais de um warp estiver pronto para execução, um mecanismo de prioridade é usado para selecionar um para execução [^89]. Este mecanismo de preencher o tempo de latência de operações com trabalho de outros threads é frequentemente chamado de **tolerância à latência** ou *latency hiding* [^89].

**Zero-Overhead Thread Scheduling**
O *warp scheduling* também é usado para tolerar outros tipos de latências de operação, como aritmética de ponto flutuante *pipelined* e instruções de branch [^90]. Com warps suficientes por perto, o hardware provavelmente encontrará um warp para executar a qualquer ponto no tempo, utilizando assim totalmente o hardware de execução, apesar dessas operações de longa latência [^90]. A seleção de warps prontos para execução não introduz nenhum tempo ocioso na linha do tempo de execução, o que é referido como **zero-overhead thread scheduling** [^90]. Com o *warp scheduling*, o longo tempo de espera das instruções do warp é "escondido" pela execução de instruções de outros warps [^90].

### Conclusão
O thread scheduling em CUDA, baseado no conceito de warps, é fundamental para maximizar a utilização dos recursos de hardware e tolerar a latência de operações de memória [^88]. Ao dividir os blocos em warps e alternar a execução entre eles, os SMs conseguem manter um alto nível de *throughput*, mesmo quando os threads individuais estão aguardando a conclusão de operações de longa latência [^90].

### Referências
[^87]: Seção 4.7: Thread Scheduling and Latency Tolerance.
[^88]: Seção 4.7: Thread Scheduling and Latency Tolerance.
[^89]: Seção 4.7: Thread Scheduling and Latency Tolerance.
[^90]: Seção 4.7: Thread Scheduling and Latency Tolerance.
<!-- END -->