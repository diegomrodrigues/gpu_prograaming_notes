## Otimização do Acesso à Memória Global em CUDA

### Introdução

A eficiência da execução de kernels CUDA depende fortemente do acesso otimizado à **memória global**, o espaço de memória principal para dados que precisam ser compartilhados entre a CPU (host) e a GPU (device). A memória global, diferentemente da memória compartilhada ou dos registradores, possui latência de acesso significativamente maior, tornando a otimização do seu uso crucial para obter alto desempenho. Este capítulo detalha a arquitetura e o funcionamento da memória global, concentrando-se em como as características das DRAMs subjacentes influenciam a estratégia de acesso. Entender essas nuances permite aos desenvolvedores CUDA escrever código que maximize a utilização da largura de banda da memória global, minimizando gargalos de desempenho.

### Conceitos Fundamentais

A memória global em GPUs CUDA é implementada utilizando **DRAMs** (Dynamic Random Access Memories) [^1]. DRAMs armazenam dados como cargas elétricas em pequenos capacitores. O processo de leitura desses dados envolve a transferência dessas cargas, um processo relativamente lento [^1]. Essa lentidão intrínseca é um fator limitante no desempenho global da aplicação.

![DRAM cell architecture highlighting data storage and signal transmission.](./../images/image6.jpg)

Para mitigar a lentidão da leitura em DRAMs, as arquiteturas modernas empregam **paralelismo** [^1]. Ao acessar uma determinada localização de memória, o sistema também acessa localizações consecutivas [^1]. Isso é feito através de múltiplos sensores que operam concorrentemente, permitindo a transferência de dados em alta velocidade [^1]. O grau de paralelismo e o layout físico da memória afetam diretamente a largura de banda efetiva que pode ser alcançada.

**Largura de Banda da Memória Global:** A largura de banda da memória global é uma métrica crucial, definida como a taxa máxima na qual os dados podem ser transferidos entre a memória global e os núcleos de processamento da GPU. Maximizar a utilização dessa largura de banda é essencial para otimizar o desempenho.

**Acessos Coalescidos:** Aproveitar o paralelismo das DRAMs é alcançado através de acessos *coalescidos*. Acessos coalescidos ocorrem quando threads em um warp acessam localizações de memória contíguas. Nesse cenário, a GPU pode agrupar múltiplas solicitações de memória em uma única transação, utilizando a largura de banda total disponível.

![Coalesced memory access pattern in CUDA showcasing consecutive threads accessing consecutive memory locations for efficient data transfer.](./../images/image7.jpg)

**Acessos Não Coalescidos:** Quando threads em um warp acessam localizações de memória não contíguas ou desalinhadas, os acessos são considerados *não coalescidos*. Esses acessos resultam em múltiplas transações menores, reduzindo drasticamente a utilização da largura de banda da memória global e introduzindo latência adicional.

![Padrões de acesso à memória coalescidos (b) vs. não coalescidos (a) em C 2D arrays para CUDA.](./../images/image2.jpg)

**Implicações da Arquitetura DRAM:** A arquitetura DRAM impõe algumas restrições no acesso à memória global. A organização física da memória, incluindo bancos de memória e linhas de cache, influencia o desempenho do acesso. Acessar dados dentro da mesma linha de cache é significativamente mais rápido do que acessar dados em linhas de cache diferentes ou em bancos de memória diferentes.

### Conclusão

A compreensão do funcionamento interno da memória global, particularmente a arquitetura DRAM subjacente e o conceito de acessos coalescidos, é fundamental para otimizar o desempenho de aplicações CUDA. Ao projetar kernels CUDA, os desenvolvedores devem se esforçar para organizar os dados e os padrões de acesso de forma a promover acessos coalescidos, minimizando o número de transações de memória necessárias. Além disso, o conhecimento da organização física da memória pode ser usado para evitar conflitos de banco e maximizar a utilização da largura de banda disponível. Estratégias avançadas, como o uso de memória compartilhada como cache gerenciado pelo usuário, podem ser empregadas para reduzir ainda mais a dependência da memória global e melhorar o desempenho.

### Referências
[^1]: Global memory is implemented with DRAMs, where data bits are stored as charges in small capacitors. Reading data involves a relatively slow charge transfer process. Modern DRAMs employ parallelism to increase data access rates; accessing one location also accesses consecutive locations, with multiple sensors operating concurrently for high-speed data transfer.
<!-- END -->