## Otimização da Razão Aritmética-Memória em Kernels CUDA

### Introdução
Em kernels CUDA, a performance é frequentemente limitada pela razão entre cálculos aritméticos de ponto flutuante e acessos à memória global [^6]. Quando essa razão é baixa, o kernel gasta relativamente mais tempo buscando dados da memória global do que realizando operações computacionais. Isso resulta em um gargalo de memória, impedindo que o potencial máximo da GPU seja alcançado. Este capítulo aborda técnicas para reduzir o número de acessos à memória global, melhorando, assim, a performance dos kernels CUDA.

### Conceitos Fundamentais

A memória global, sendo o espaço de memória principal na GPU, possui alta latência de acesso. Minimizar os acessos a essa memória é crucial para otimizar o desempenho. Diversas técnicas podem ser empregadas para atingir esse objetivo.

1.  **Utilização de Memória Compartilhada (Shared Memory):** A memória compartilhada é uma memória on-chip de baixa latência, acessível a todos os threads dentro de um bloco. Ao carregar dados da memória global para a memória compartilhada, os threads dentro do bloco podem acessar esses dados repetidamente a um custo muito menor.

    *Exemplo:* Considere um kernel que calcula a média de um array. Em vez de cada thread acessar a memória global para ler os elementos do array repetidamente, os elementos podem ser carregados uma vez na memória compartilhada e, em seguida, cada thread pode acessar a memória compartilhada para realizar seus cálculos.

    *Implementação:*
    ```c++
    __global__ void kernel(float* global_data, float* output, int size) {
        __shared__ float shared_data[BLOCK_SIZE];
        int idx = blockIdx.x * blockDim.x + threadIdx.x;

        // Carrega dados da memória global para a memória compartilhada
        if (idx < size) {
            shared_data[threadIdx.x] = global_data[idx];
        }
        __syncthreads(); // Garante que todos os threads tenham carregado os dados

        // Realiza cálculos usando dados da memória compartilhada
        float sum = 0.0f;
        for (int i = 0; i < BLOCK_SIZE; ++i) {
            sum += shared_data[i];
        }

        // Escreve o resultado na memória global
        if (threadIdx.x == 0) {
            output[blockIdx.x] = sum / BLOCK_SIZE;
        }
    }
    ```
    Neste exemplo, `shared_data` é alocado na memória compartilhada. A diretiva `__syncthreads()` garante que todos os threads tenham completado a leitura dos dados na memória compartilhada antes de qualquer thread começar a calcular a soma.



![CUDA memory model: Grid of blocks with shared memory, registers, and threads interacting with global and constant memory.](./../images/image10.jpg)

2.  **Coalesced Memory Access:** GPUs são projetadas para acessar a memória global de forma eficiente quando os threads dentro de um warp acessam localidades de memória contíguas. Isso é conhecido como *coalesced memory access*. Ao organizar os dados na memória de forma que threads em um warp acessem dados contíguos, podemos maximizar a largura de banda da memória.

    *Exemplo:* Em vez de cada thread acessar elementos dispersos na memória, organizar os dados de forma que threads adjacentes acessem elementos adjacentes.

    *Implementação:* Se uma matriz é armazenada em *row-major order*, threads devem acessar os dados por linhas para obter coalesced access.

    Considere o seguinte código não otimizado:
    ```c++
    __global__ void non_coalesced_kernel(float* matrix, float* output, int width, int height) {
        int row = blockIdx.x * blockDim.x + threadIdx.x;
        int col = blockIdx.y * blockDim.y + threadIdx.y;

        if (row < height && col < width) {
            output[row * width + col] = matrix[col * height + row]; // Non-coalesced access
        }
    }
    ```
    Aqui, os threads acessam a matriz de forma não coalescida. Para otimizar, podemos usar:
    ```c++
    __global__ void coalesced_kernel(float* matrix, float* output, int width, int height) {
        int row = blockIdx.x * blockDim.x + threadIdx.x;
        int col = blockIdx.y * blockDim.y + threadIdx.y;

        if (row < height && col < width) {
            output[row * width + col] = matrix[row * width + col]; // Coalesced access
        }
    }
    ```
    Nesta versão otimizada, os threads acessam a matriz de forma coalescida.

3.  **Utilização de Texture Cache:** O cache de textura é otimizado para operações de leitura e oferece melhor desempenho para dados que exibem localidade espacial. Dados que são acessados repetidamente e que estão localizados próximos um do outro podem se beneficiar significativamente do uso do cache de textura.

    *Exemplo:* Em processamento de imagens, os pixels adjacentes são frequentemente acessados repetidamente. Utilizar texture cache pode melhorar a performance significativamente.

    *Implementação:*
    ```c++
    texture<float, cudaTextureType2D, cudaReadModeElementType> tex;

    __global__ void texture_kernel(float* output, int width, int height) {
        int x = blockIdx.x * blockDim.x + threadIdx.x;
        int y = blockIdx.y * blockDim.y + threadIdx.y;

        if (x < width && y < height) {
            float value = tex2D(tex, x, y);
            output[y * width + x] = value;
        }
    }

    // Configuração da textura
    cudaChannelFormatDesc channelDesc = cudaCreateChannelDesc<float>();
    cudaMallocArray(&cuArray, &channelDesc, width, height);
    cudaMemcpyToArray(cuArray, 0, 0, input_data, width * height * sizeof(float), cudaMemcpyHostToDevice);

    tex.addressMode[0] = cudaAddressModeClamp;
    tex.addressMode[1] = cudaAddressModeClamp;
    tex.filterMode = cudaFilterModePoint;
    tex.channelDesc = channelDesc;

    cudaBindTextureToArray(tex, cuArray, channelDesc);
    ```
    Este código demonstra como usar o cache de textura para acessar dados. Primeiro, a textura é configurada com `cudaChannelFormatDesc`, `cudaMallocArray` e `cudaMemcpyToArray`. Em seguida, a textura é acessada no kernel usando `tex2D`.



![Simplified diagram of a modern processor's cache hierarchy, showing the levels of cache memory.](./../images/image5.jpg)

4.  **Utilização de Constant Memory:** A memória constante é um tipo de memória na GPU que é armazenada no chip e é cacheada. É otimizada para dados que são lidos com frequência por todos os threads. Se os dados são acessados repetidamente e não são alterados durante a execução do kernel, usar a memória constante pode melhorar a performance.

    *Exemplo:* Parâmetros de configuração que são usados por todos os threads.

    *Implementação:*
    ```c++
    __constant__ float constant_data[SIZE];

    __global__ void constant_memory_kernel(float* output, int size) {
        int idx = blockIdx.x * blockDim.x + threadIdx.x;

        if (idx < size) {
            output[idx] = constant_data[idx];
        }
    }
    ```
    Neste exemplo, `constant_data` é alocado na memória constante e é acessado por todos os threads.

5.  **Loop Unrolling e Instruction-Level Parallelism:** Desenrolar loops pode reduzir a sobrecarga de loop e expor mais paralelismo em nível de instrução (ILP). Isso permite que o compilador otimize o código para melhor desempenho.

    *Exemplo:* Substituir um loop simples por várias instruções equivalentes.

    *Implementação:*
    ```c++
    // Código original com loop
    float sum = 0.0f;
    for (int i = 0; i < 4; ++i) {
        sum += data[i];
    }

    // Código com loop unrolling
    float sum = data[0] + data[1] + data[2] + data[3];
    ```

### Conclusão

A otimização da razão aritmética-memória é fundamental para alcançar o máximo desempenho em kernels CUDA. Técnicas como a utilização de memória compartilhada, coalesced memory access, cache de textura, memória constante e loop unrolling podem reduzir significativamente o número de acessos à memória global, resultando em kernels mais eficientes. A escolha da técnica mais adequada depende das características específicas do problema e do padrão de acesso aos dados. É importante analisar o perfil do kernel para identificar os gargalos de memória e aplicar as otimizações apropriadas.

### Referências
[^6]: Information provided in the problem description about the low ratio between floating-point arithmetic calculation and global memory accesses limiting performance.
<!-- END -->