Okay, I will add Mermaid diagrams to enhance the text, focusing on architecture and system relationships, using flowcharts and sequence diagrams where appropriate.

```mermaid
  flowchart LR
    A["CPU/GPU Core"] -->|Data Request| B("L1 Cache")
    B -->|Cache Hit| A
    B -->|Cache Miss| C("L2 Cache")
    C -->|Cache Hit| B
    C -->|Cache Miss| D("Main Memory")
    D --> C
    style A fill:#f9f,stroke:#333,stroke-width:2px
```

### Introdu√ß√£o

A **coer√™ncia de cache** √© um problema que surge em sistemas multiprocessadores, como as GPUs, quando m√∫ltiplos n√∫cleos ou processadores compartilham o mesmo espa√ßo de mem√≥ria e possuem seus pr√≥prios *caches*. Em um sistema com caches, a mesma regi√£o de mem√≥ria pode estar armazenada em diferentes caches, e se um n√∫cleo modifica um valor que est√° armazenado no cache, os outros n√∫cleos podem n√£o ter acesso ao valor atualizado, e isso √© chamado de problema de coer√™ncia do cache. Em um kernel CUDA para convolu√ß√£o, threads de diferentes blocos podem tentar acessar e modificar a mem√≥ria, e o problema de coer√™ncia deve ser tratado para evitar resultados incorretos. Neste cap√≠tulo, exploraremos o problema de coer√™ncia de cache, como ele afeta kernels CUDA para convolu√ß√£o, e como garantir a coer√™ncia de cache em implementa√ß√µes paralelas.

### Conceitos Fundamentais da Coer√™ncia de Cache

A **coer√™ncia de cache** √© um mecanismo que garante que todos os processadores em um sistema multiprocessador tenham acesso √† vers√£o mais recente dos dados compartilhados. Quando m√∫ltiplos processadores acessam e modificam dados em cache, √© poss√≠vel que diferentes vers√µes dos mesmos dados sejam armazenadas em *caches* diferentes. O mecanismo de coer√™ncia de cache tem como objetivo garantir que essas vers√µes sejam consistentes, de forma que todos os processadores leiam os mesmos valores, e, quando eles s√£o modificados, o valor mais recente seja sempre acessado.

**Conceito 1: O Problema da Coer√™ncia de Cache**

O problema da coer√™ncia de cache surge quando m√∫ltiplos processadores acessam e modificam dados na mem√≥ria principal que tamb√©m est√£o armazenados nos seus caches locais. Se um processador modificar um dado no seu cache local, e os outros processadores tamb√©m acessarem esse dado do seus caches, pode haver uma inconsist√™ncia nos valores, que pode levar a resultados incorretos.

**Lemma 1:** *A coer√™ncia de cache √© um problema que surge em sistemas multiprocessadores, quando m√∫ltiplos caches armazenam c√≥pias de dados modificados, e o problema da coer√™ncia garante que todos os processadores acessem a vers√£o mais recente desses dados.*

**Prova:** A replica√ß√£o de dados em caches diferentes, em arquiteturas multiprocessadores, faz com que diferentes c√≥pias dos dados sejam armazenadas em diferentes n√≠veis de mem√≥ria, e a necessidade de coer√™ncia surge quando essas c√≥pias s√£o modificadas, para que todos os processadores tenham acesso ao dado correto e mais recente. $\blacksquare$

**Conceito 2: Protocolos de Coer√™ncia de Cache**

Existem diversos protocolos para garantir a coer√™ncia de cache, como o protocolo **MESI** (Modified, Exclusive, Shared, Invalid), que √© utilizado em muitas arquiteturas de processadores. Esses protocolos definem os estados poss√≠veis para um bloco de dados em *cache*, e como esses estados devem ser gerenciados durante as opera√ß√µes de leitura e escrita. O protocolo MESI, por exemplo, define que um dado pode ser *Modified* (modificado e ainda n√£o escrito na mem√≥ria principal), *Exclusive* (√∫nica c√≥pia do dado no cache), *Shared* (c√≥pia compartilhada em m√∫ltiplos caches), ou *Invalid* (a c√≥pia do dado no cache n√£o √© v√°lida).

> ‚ùó **Ponto de Aten√ß√£o:** A implementa√ß√£o de mecanismos de coer√™ncia de cache √© complexa e exige um overhead no hardware, e a efici√™ncia desse mecanismo tem um impacto direto no desempenho do sistema multiprocessador.

**Corol√°rio 1:** *Os protocolos de coer√™ncia de cache s√£o usados para manter a consist√™ncia dos dados em um sistema multiprocessador, garantindo que todos os processadores acessem a vers√£o mais recente dos dados, o que √© essencial para a opera√ß√£o correta de qualquer aplica√ß√£o.*

**Conceito 3: Coer√™ncia de Cache em GPUs**

Em GPUs, a coer√™ncia de cache √© mais complexa do que em CPUs, devido ao n√∫mero muito maior de n√∫cleos e √† arquitetura de mem√≥ria diferente. GPUs geralmente sacrificam a coer√™ncia de *cache* entre os SMs (Streaming Multiprocessors) para otimizar a largura de banda e a velocidade de execu√ß√£o. As GPUs geralmente possuem *caches* L1 dedicados a cada SM, e um *cache* L2 compartilhado entre v√°rios SMs. A mem√≥ria constante tamb√©m utiliza um *cache* dedicado. Em geral, n√£o existe uma coer√™ncia entre os caches L1 de SMs diferentes, e √© o programador que precisa garantir que os dados sejam acessados corretamente atrav√©s do uso da mem√≥ria compartilhada e de outras abordagens.

```mermaid
  flowchart LR
    subgraph GPU
      SM1["SM 1"] --> L1_1("L1 Cache 1")
      SM2["SM 2"] --> L1_2("L1 Cache 2")
      SM3["SM 3"] --> L1_3("L1 Cache 3")
      L1_1 & L1_2 & L1_3 --> L2("L2 Cache")
      L2 --> GlobalMem("Global Memory")
    end
    style L1_1 fill:#ccf,stroke:#333,stroke-width:1px
    style L1_2 fill:#ccf,stroke:#333,stroke-width:1px
    style L1_3 fill:#ccf,stroke:#333,stroke-width:1px

```

### Coer√™ncia de Cache em Kernels CUDA para Convolu√ß√£o

Em kernels CUDA para convolu√ß√£o, a coer√™ncia de *cache* pode surgir quando os threads em diferentes blocos acessam e modificam dados na mem√≥ria global, como no caso de algoritmos que n√£o utilizam mem√≥ria compartilhada, ou quando o tratamento das *boundary conditions* exige que a mesma posi√ß√£o da entrada seja utilizada por dois ou mais blocos. Se diferentes blocos de threads acessam os mesmos dados na mem√≥ria global, e os dados s√£o modificados por algum desses blocos, os outros blocos podem estar usando dados desatualizados dos seus caches. No entanto, a falta de coer√™ncia √© um problema maior no caso das mem√≥rias privadas de cada SM, como a L1, que n√£o compartilham a atualiza√ß√£o de dados modificados entre as diferentes regi√µes.

1.  **Mem√≥ria Global:** Em kernels de convolu√ß√£o, a mem√≥ria global √© utilizada para armazenar os *arrays* de entrada (N) e o *array* de sa√≠da (P). Se diferentes blocos modificam o array de sa√≠da, pode ocorrer a falta de coer√™ncia, j√° que a escrita em um dos *caches* n√£o propaga a mudan√ßa para os outros.
2.  **Mem√≥ria Compartilhada:** A mem√≥ria compartilhada √© utilizada para armazenar dados reutilizados por threads do mesmo bloco, e nesse caso, o problema de coer√™ncia de *cache* n√£o ocorre, j√° que todos os threads que compartilham a mem√≥ria, fazem parte do mesmo bloco.

> üí° **Dica:** O uso da mem√≥ria compartilhada √© uma forma de mitigar o problema da falta de coer√™ncia de cache, j√° que todos os dados que precisam ser acessados por um bloco, s√£o armazenados na mem√≥ria compartilhada, que tem coer√™ncia garantida.

**Lemma 2:** *A falta de coer√™ncia de cache em GPUs pode levar a resultados incorretos em kernels CUDA para convolu√ß√£o, se dados que s√£o compartilhados entre diferentes SMs s√£o modificados, e isso deve ser levado em conta no projeto do kernel, e a utiliza√ß√£o da mem√≥ria compartilhada √© uma forma de minimizar esse problema.*

**Prova:**  A falta de coer√™ncia faz com que as c√≥pias dos dados na hierarquia de mem√≥ria n√£o sejam consistentes, e, com isso, os threads da GPU podem acessar valores desatualizados da mem√≥ria, o que leva a c√°lculos e resultados incorretos, especialmente em implementa√ß√µes paralelas. $\blacksquare$

**Corol√°rio 2:** *A coer√™ncia de cache √© um problema complexo em GPUs, e os programadores devem estar conscientes desse problema e utilizar estrat√©gias que minimizem a sua influ√™ncia, atrav√©s do uso da mem√≥ria compartilhada ou atrav√©s de outras t√©cnicas de otimiza√ß√£o.*

### Estrat√©gias para Lidar com a Coer√™ncia de Cache

```mermaid
  flowchart LR
    A["Global Memory"] --> B("L2 Cache")
    B --> C("L1 Cache (SM)")
    C --> D("Shared Memory")
    D --> E["Threads (within Block)"]
    style D fill:#aaf,stroke:#333,stroke-width:2px
```

Em kernels CUDA para convolu√ß√£o, existem diversas estrat√©gias para lidar com o problema da coer√™ncia de cache:

1.  **Utiliza√ß√£o da Mem√≥ria Compartilhada:** Como mencionado, a mem√≥ria compartilhada garante a coer√™ncia de dados dentro de um mesmo bloco de threads. Ao carregar os dados para a mem√≥ria compartilhada, a necessidade de acessar a mem√≥ria global √© reduzida. O uso da mem√≥ria compartilhada garante que os dados sejam acessados na sua √∫ltima vers√£o, j√° que ela √© compartilhada apenas entre os threads do mesmo bloco, o que evita os problemas de coer√™ncia entre blocos diferentes.

2.  **Acesso Coalescente √† Mem√≥ria Global:** Organizar o acesso √† mem√≥ria global de forma que os acessos sejam coalescentes, e o acesso seja feito de maneira sequencial, dentro de um mesmo warp, minimiza a necessidade de utilizar os caches L1, e aumenta a efici√™ncia do acesso √† mem√≥ria. A organiza√ß√£o dos dados na mem√≥ria deve ser tal que o acesso coalescente seja garantido, e os dados sejam acessados de forma sequencial pelos threads do mesmo *warp*.
3. **Utiliza√ß√£o de Mem√≥ria Constante:** A utiliza√ß√£o da mem√≥ria constante para a *convolution mask* elimina a necessidade de tratar a coer√™ncia dos dados da m√°scara, j√° que a mem√≥ria constante √© somente leitura durante a execu√ß√£o do kernel.
4. **Organiza√ß√£o dos Acessos:** Quando o uso da mem√≥ria compartilhada n√£o for poss√≠vel, o c√≥digo deve garantir que, se a mesma posi√ß√£o da mem√≥ria global for acessada por m√∫ltiplos blocos, o acesso seja feito de forma correta, com o uso de sincroniza√ß√£o e t√©cnicas que garantam a coer√™ncia dos dados, e os acessos n√£o devem ser feitos de forma aleat√≥ria, e sim, utilizando um padr√£o que minimize a lat√™ncia e maximize a largura de banda do acesso.

**Lemma 3:** *O uso da mem√≥ria compartilhada, o acesso coalescente, a utiliza√ß√£o da mem√≥ria constante, e a organiza√ß√£o dos acessos minimizam os problemas de coer√™ncia de cache em kernels CUDA para convolu√ß√£o, e todas essas abordagens garantem o acesso consistente e eficiente aos dados.*

**Prova:** O uso da mem√≥ria compartilhada garante que os threads de um bloco acessem dados consistentes, o acesso coalescente garante o melhor uso da largura de banda da mem√≥ria global, e a mem√≥ria constante garante o acesso eficiente a um dado que n√£o precisa ser modificado durante a execu√ß√£o do kernel. $\blacksquare$

**Corol√°rio 3:** *A utiliza√ß√£o combinada de abordagens como mem√≥ria compartilhada, acesso coalescente e mem√≥ria constante permitem minimizar o problema da coer√™ncia de cache em kernels CUDA para convolu√ß√£o e otimizar o acesso √† mem√≥ria.*

### An√°lise Te√≥rica Avan√ßada da Coer√™ncia de Cache

**Pergunta Te√≥rica Avan√ßada 1:** *Como a escolha do tamanho dos tiles e blocos de threads interagem com o mecanismo de coer√™ncia de cache em GPUs, e como otimizar o tamanho dos tiles para maximizar o desempenho e a utiliza√ß√£o dos caches?*

**Resposta:**

A escolha do **tamanho dos *tiles*** e dos **blocos de threads** interage de forma complexa com o mecanismo de **coer√™ncia de cache** em GPUs. A escolha do tamanho do tile, do tamanho do bloco e da organiza√ß√£o dos threads tem um impacto direto na quantidade de dados que ser√£o armazenados em *cache* e em mem√≥ria compartilhada.

**Lemma 4:** *A escolha do tamanho dos tiles e blocos afeta a forma como os dados s√£o utilizados, e tamb√©m a necessidade de que dados da mem√≥ria global sejam utilizados para compor o resultado final, e tamb√©m influencia a forma como os caches s√£o utilizados, e tamb√©m o problema da falta de coer√™ncia de cache.*

**Prova:** O tamanho do tile determina a quantidade de dados que s√£o carregados para a mem√≥ria compartilhada, e tamb√©m afeta a reutiliza√ß√£o desses dados. O tamanho do bloco define a quantidade de threads que executam ao mesmo tempo, e, consequentemente, a necessidade de acesso √† mem√≥ria global, e tamb√©m √† mem√≥ria compartilhada. $\blacksquare$

O **tamanho dos *tiles*** e o tamanho dos **blocos de threads** interagem da seguinte forma com a coer√™ncia de *cache*:

1.  **Tamanho dos Tiles e Caches L1/L2:** Um *tile* menor pode caber totalmente no *cache* L1, o que reduz o tr√°fego da mem√≥ria global e aumenta o desempenho. Um *tile* maior pode n√£o caber no *cache* L1, e os dados precisam ser carregados no *cache* L2 ou mesmo na mem√≥ria global, o que aumenta a lat√™ncia do acesso.
2.  **Tamanho dos Blocos e Mem√≥ria Compartilhada:** O tamanho do bloco influencia o uso da mem√≥ria compartilhada, que deve ser utilizada para armazenar todos os dados que s√£o necess√°rios para o c√°lculo de uma regi√£o. O n√∫mero de threads por bloco influencia a utiliza√ß√£o da mem√≥ria compartilhada, e do acesso coalescente.
3.  **Tiles e Coer√™ncia de Cache:** O tamanho do *tile* influencia a necessidade de que os dados sejam acessados em diferentes blocos, e, com isso, um *tile* menor permite que menos blocos disputem a mesma regi√£o da mem√≥ria. Blocos muito grandes que necessitam de dados de outros blocos levam a conflitos e problemas de coer√™ncia de *cache*.
4.  **Sobreposi√ß√£o de Tiles:** A sobreposi√ß√£o entre os *tiles*, definida pelo tamanho da *convolution mask*, pode causar a duplica√ß√£o de dados em diferentes blocos, o que tamb√©m pode afetar a coer√™ncia de *cache*.

A otimiza√ß√£o do tamanho do *tile* e do bloco deve considerar todos esses fatores, para maximizar a reutiliza√ß√£o dos dados no *cache* e na mem√≥ria compartilhada, minimizar o tr√°fego da mem√≥ria global e o problema da coer√™ncia.

**Corol√°rio 4:** *A escolha adequada do tamanho dos tiles e dos blocos de threads, que deve considerar o tamanho dos caches L1 e L2, o tipo de convolu√ß√£o, e os requisitos de mem√≥ria do kernel, √© fundamental para que a coer√™ncia do cache seja gerenciada de forma eficiente.*

**Pergunta Te√≥rica Avan√ßada 2:** *Como a hierarquia de caches em GPUs interage com a utiliza√ß√£o da mem√≥ria constante e como o programador pode maximizar o uso desses caches para reduzir o acesso √† mem√≥ria global e minimizar o problema da coer√™ncia?*

**Resposta:**

A **hierarquia de caches** em GPUs interage de forma complexa com a utiliza√ß√£o da **mem√≥ria constante**, e o entendimento dessa intera√ß√£o √© crucial para otimizar o desempenho de kernels CUDA para convolu√ß√£o. A mem√≥ria constante √© armazenada em um *cache* otimizado para *broadcast*, e a forma como a mem√≥ria constante √© utilizada impacta a forma como a hierarquia de *cache* √© utilizada em conjunto com os dados de entrada.

**Lemma 7:** *A hierarquia de caches na GPU (L1, L2 e caches da mem√≥ria constante) √© fundamental para reduzir a lat√™ncia do acesso √† mem√≥ria, e o uso estrat√©gico desses caches, em conjunto com a mem√≥ria constante, permite reduzir o acesso √† mem√≥ria global e minimizar o problema da coer√™ncia.*

**Prova:** Os caches de n√≠veis L1 e L2 ajudam a reduzir a lat√™ncia de acesso a dados que est√£o armazenados na mem√≥ria global, e, quando usados em conjunto com a mem√≥ria constante, eles podem melhorar o desempenho. A mem√≥ria constante, por sua vez, utiliza um cache pr√≥prio, otimizado para o acesso a dados do tipo somente leitura, como a *convolution mask*, e isso reduz a lat√™ncia do acesso e o tr√°fego na mem√≥ria global. $\blacksquare$

```mermaid
  flowchart LR
    GlobalMem["Global Memory"] --> L2("L2 Cache")
    L2 --> L1("L1 Cache (SM)")
    L2 --> ConstCache["Constant Memory Cache"]
    style ConstCache fill:#afa,stroke:#333,stroke-width:2px

```

A **intera√ß√£o** entre a hierarquia de *caches* e a mem√≥ria constante ocorre da seguinte forma:

1.  **Caches da Mem√≥ria Constante:** A *convolution mask* √© carregada no *cache* da mem√≥ria constante, e esse cache tem um acesso de baixa lat√™ncia e alta largura de banda, que √© compartilhado entre todos os threads, e isso reduz a necessidade do acesso √† mem√≥ria global.

2.  **Cache L1:** O cache L1 armazena dados que s√£o utilizados frequentemente pelos threads de um SM. A forma com que os dados de entrada s√£o acessados, e a decis√£o de utiliz√°-los na mem√≥ria compartilhada, influenciam diretamente o uso do cache L1.
3.  **Cache L2:** O cache L2 √© compartilhado entre diversos SMs, e √© utilizado quando um dado n√£o est√° presente no cache L1. Se a mem√≥ria compartilhada est√° sendo utilizada, e os dados de entrada s√£o armazenados na mem√≥ria global, o *cache* L2 √© um ponto intermedi√°rio para o acesso a essa mem√≥ria. A escolha de um tile, e da forma como os dados s√£o acessados na mem√≥ria global, influencia diretamente o uso do *cache* L2.

Para otimizar o uso dos *caches*, as seguintes estrat√©gias podem ser utilizadas:

1.  **Utilizar Mem√≥ria Constante:** Armazenar a *convolution mask* na mem√≥ria constante, para que os dados sejam acessados atrav√©s do cache de baixa lat√™ncia.
2.  **Tiling:** Utilizar o *tiling* e o acesso coalescente √† mem√≥ria global, para garantir que os dados sejam carregados para a mem√≥ria compartilhada de forma eficiente.
3. **Pre-fetching:** Utilizar *pre-fetching*, sempre que poss√≠vel, para carregar os dados nos caches antes que eles sejam necess√°rios, de forma a garantir que a lat√™ncia do acesso seja minimizada, e que os dados estejam dispon√≠veis quando os threads precisarem deles.

**Corol√°rio 7:** *O uso adequado da hierarquia de caches e da mem√≥ria constante permite reduzir o acesso √† mem√≥ria global, utilizar a largura de banda de forma mais eficiente, reduzir a lat√™ncia de acesso √† mem√≥ria, e minimizar o problema da falta de coer√™ncia de cache em kernels CUDA para convolu√ß√£o.*

### Dedu√ß√£o Te√≥rica Complexa: Modelagem do Tempo de Execu√ß√£o da Convolu√ß√£o com Coer√™ncia de Cache

O **tempo de execu√ß√£o** de uma convolu√ß√£o em CUDA, incluindo o problema da **coer√™ncia de cache**, pode ser modelado levando em considera√ß√£o o tempo gasto nas opera√ß√µes de computa√ß√£o, o tempo de acesso √† mem√≥ria, e o *overhead* da coer√™ncia.

O modelo do tempo de execu√ß√£o pode ser definido como:
$$
T_{kernel} = T_{memoria} + T_{computacao} + T_{coerencia}
$$
Onde $T_{memoria}$ representa o tempo de acesso √† mem√≥ria, $T_{computacao}$ o tempo de computa√ß√£o, e $T_{coerencia}$ o tempo gasto com as opera√ß√µes de coer√™ncia de cache.

**Lemma 8:** *O tempo de execu√ß√£o de um kernel de convolu√ß√£o √© modelado pela soma dos tempos de acesso √† mem√≥ria, de computa√ß√£o e do overhead gerado pela necessidade de coer√™ncia dos caches. A redu√ß√£o desses fatores leva a um melhor desempenho do kernel.*

**Prova:** O tempo total de execu√ß√£o corresponde ao tempo gasto na execu√ß√£o de cada parte do kernel, e o overhead da coer√™ncia do cache tamb√©m impacta o tempo total de execu√ß√£o. $\blacksquare$

O tempo de acesso √† mem√≥ria, $T_{memoria}$, pode ser modelado como:
$$
T_{memoria} =  N_{acessos} * T_{latencia} + \frac{Data_{acessada}}{BW}
$$

Onde $N_{acessos}$ √© o n√∫mero de acessos √† mem√≥ria, $T_{latencia}$ a lat√™ncia do acesso, $Data_{acessada}$ a quantidade de dados acessados e $BW$ a largura de banda. O tempo de computa√ß√£o √© dado por:

$$
T_{computacao} = \frac{N_{op}}{P} * T_{op}
$$

Onde $N_{op}$ √© o n√∫mero de opera√ß√µes de multiplica√ß√£o e soma, P o n√∫mero de threads e $T_{op}$ o tempo para realizar uma opera√ß√£o. O tempo gasto com a coer√™ncia dos caches, $T_{coerencia}$, √© um fator que considera o tempo que o kernel gasta para lidar com as inconsist√™ncias dos caches:
$$
T_{coerencia} = C_{coerencia} *  N_{threads}
$$
Onde  $C_{coerencia}$ representa o overhead da coer√™ncia do cache, e $N_{threads}$ o n√∫mero de threads envolvidos na computa√ß√£o. A utiliza√ß√£o da mem√≥ria compartilhada e da mem√≥ria constante, o acesso coalescente e o *tiling* podem reduzir a necessidade de opera√ß√µes de coer√™ncia.

**Corol√°rio 8:** *O modelo do tempo de execu√ß√£o com coer√™ncia de cache mostra como o tempo de acesso √† mem√≥ria, o tempo de computa√ß√£o e o overhead da coer√™ncia influenciam o desempenho do kernel. O uso da mem√≥ria compartilhada, o acesso coalescente e o uso da mem√≥ria constante reduzem o tempo de acesso √† mem√≥ria, e o overhead da coer√™ncia, o que resulta em um maior desempenho do kernel.*

### Conclus√£o

(Nota: N√£o conclua o cap√≠tulo at√© que o usu√°rio solicite.)

### Refer√™ncias

[^1]: "In the next several chapters, we will discuss a set of important parallel computation patterns. These patterns are the basis of many parallel algorithms that appear in applications." *(Trecho de <Parallel Patterns: Convolution>)*

[^2]: "Mathematically, convolution is an array operation where each output data element is a weighted sum of a collection of neighboring input elements. The weights used in the weighted sum calculation are defined by an input mask array, commonly referred to as the convolution kernel." *(Trecho de <Parallel Patterns: Convolution>)*
[^3]: "Because convolution is defined in terms of neighboring elements, boundary conditions naturally exist for output elements that are close to the ends of an array." *(Trecho de <Parallel Patterns: Convolution>)*

[^4]: "Kernel functions access constant memory variables as global variables. Thus, their pointers do not need to be passed to the kernel as parameters." *(Trecho de <Parallel Patterns: Convolution>)*

[^5]: "For image processing and computer vision, input data is usually in 2D form, with pixels in an x-y space. Image convolutions are also two dimensional." *(Trecho de <Parallel Patterns: Convolution>)*

[^6]: "A more serious problem is memory bandwidth. The ratio of floating-point arithmetic calculation to global memory accesses is only about 1.0 in the kernel." *(Trecho de <Parallel Patterns: Convolution>)*
[^7]: "The CUDA programming model allows programmers to declare a variable in the constant memory. Like global memory variables, constant memory variables are also visible to all thread blocks. The main difference is that a constant memory variable cannot be changed by threads during kernel execution. Furthermore, the size of the constant memory can vary from device to device." *(Trecho de <Parallel Patterns: Convolution>)*
[^8]:  "We will discuss two input data tiling strategies for reducing the total number of global memory accesses." *(Trecho de <Parallel Patterns: Convolution>)*
[^9]:  "Constant memory variables play an interesting role in using caches in massively parallel processors. Since they are not changed during kernel execution, there is no cache coherence issue during the execution of a kernel." *(Trecho de <Parallel Patterns: Convolution>)*
[^10]:  "Furthermore, the design of caches in these processors is typically optimized to broadcast a value to a large number of threads." *(Trecho de <Parallel Patterns: Convolution>)*
[^11]:  "As a result, modern processors often employ multiple levels of caches." *(Trecho de <Parallel Patterns: Convolution>)*
[^12]: "Unlike CUDA shared memory, or scratchpad memories in general, caches are 'transparent‚Äô to programs." *(Trecho de <Parallel Patterns: Convolution>)*
[^13]:  "There is a trade-off between the size of a memory and the speed of a memory." *(Trecho de <Parallel Patterns: Convolution>)*
[^14]: "A major design issue with using caches in a massively parallel processor is cache coherence, which arises when one or more processor cores modify cached data." *(Trecho de <Parallel Patterns: Convolution>)*
[^15]: "Since L1 caches are typically directly attached to only one of the processor cores, changes in its contents are not easily observed by other processor cores." *(Trecho de <Parallel Patterns: Convolution>)*

Deseja que eu continue com as pr√≥ximas se√ß√µes?
