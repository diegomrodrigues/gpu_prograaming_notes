## Memória Constante em CUDA: Visibilidade Global e Imutabilidade

### Introdução
No contexto da programação CUDA, a **memória constante** desempenha um papel crucial na otimização de kernels que requerem acesso frequente a dados imutáveis. Este capítulo explora em profundidade a declaração, alocação e uso de variáveis na memória constante, destacando suas características e benefícios em comparação com outros tipos de memória, como a memória global.

### Conceitos Fundamentais
A memória constante em CUDA é um espaço de memória localizado no dispositivo (GPU) que oferece algumas vantagens distintas em relação à memória global [^7]. A principal característica da memória constante é que as variáveis declaradas nesse espaço são **visíveis para todos os blocos de threads** e **não podem ser alteradas durante a execução do kernel** [^7]. Essa imutabilidade permite que a GPU otimize o acesso a esses dados, geralmente através de um cache dedicado.

![CUDA memory model: Grid of blocks with shared memory, registers, and threads interacting with global and constant memory.](./../images/image10.jpg)

**Declaração de Variáveis na Memória Constante:**
Para declarar uma variável na memória constante, utiliza-se o especificador `__constant__` [^7]. A sintaxe geral é a seguinte:

```c++
__constant__ float constante_float;
__constant__ int constante_int;
```

Essas declarações devem ser feitas no escopo global do código CUDA.

**Alocação e Cópia de Dados na Memória Constante:**
Diferentemente da memória global, a alocação e a cópia de dados para a memória constante exigem funções específicas do CUDA [^7]. A função principal para essa finalidade é `cudaMemcpyToSymbol()`. Esta função permite copiar dados da memória do host (CPU) para uma variável declarada na memória constante do dispositivo (GPU).

A sintaxe geral de `cudaMemcpyToSymbol()` é:

```c++
cudaError_t cudaMemcpyToSymbol(
    const void* symbol,
    const void* src,
    size_t count,
    size_t offset = 0,
    cudaMemcpyKind kind = cudaMemcpyHostToDevice
);
```

Onde:
*   `symbol`: É o endereço do símbolo (variável) na memória constante do dispositivo.
*   `src`: É o ponteiro para os dados na memória do host a serem copiados.
*   `count`: É o número de bytes a serem copiados.
*   `offset`: É um deslocamento opcional em bytes a partir do início do símbolo onde a cópia deve começar.
*   `kind`: Especifica a direção da cópia, que geralmente é `cudaMemcpyHostToDevice` para copiar do host para o dispositivo.

**Exemplo de Alocação e Cópia:**
Considere o seguinte exemplo de código CUDA que declara, aloca e copia dados para uma variável na memória constante:

```c++
__constant__ float filtro[5];

__global__ void kernel_convolucao(float *in, float *out, int width) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= width) return;

    float soma = 0.0f;
    for (int i = -2; i <= 2; ++i) {
        int k = idx + i;
        if (k >= 0 && k < width) {
            soma += in[k] * filtro[i + 2];
        }
    }
    out[idx] = soma;
}

int main() {
    float filtro_host[5] = {0.0625, 0.25, 0.375, 0.25, 0.0625};
    float *in_device, *out_device;
    int width = 1024;
    size_t size = width * sizeof(float);

    cudaMalloc(&in_device, size);
    cudaMalloc(&out_device, size);

    cudaMemcpyToSymbol(filtro, filtro_host, 5 * sizeof(float)); // Copia filtro_host para filtro na memória constante

    // Chamada do kernel
    dim3 dimBlock(256);
    dim3 dimGrid((width + dimBlock.x - 1) / dimBlock.x);
    kernel_convolucao<<<dimGrid, dimBlock>>>(in_device, out_device, width);

    cudaFree(in_device);
    cudaFree(out_device);

    return 0;
}
```

Neste exemplo, `filtro` é declarado na memória constante. Dentro da função `main`, `cudaMemcpyToSymbol()` é utilizada para copiar os valores do array `filtro_host` (na memória do host) para a variável `filtro` (na memória constante do dispositivo).

**Vantagens e Desvantagens:**

*   **Vantagens:**
    *   **Visibilidade Global:** Facilita o acesso aos mesmos dados por todos os threads, eliminando a necessidade de passar dados como argumentos para cada kernel.
    *   **Otimização de Acesso:** A GPU pode otimizar o acesso à memória constante através de um cache dedicado, resultando em um acesso mais rápido em comparação com a memória global, especialmente se os dados forem acessados repetidamente por diferentes threads.

    ![Simplified diagram of a modern processor's cache hierarchy, showing the levels of cache memory.](./../images/image5.jpg)

*   **Desvantagens:**
    *   **Imutabilidade:** Uma vez que os dados são copiados para a memória constante, eles não podem ser alterados durante a execução do kernel. Isso limita seu uso a dados que permanecem constantes durante toda a computação.
    *   **Tamanho Limitado:** O tamanho da memória constante é limitado em comparação com a memória global. É importante verificar as especificações da GPU para determinar o tamanho máximo disponível.

### Conclusão
A memória constante em CUDA oferece uma maneira eficiente de compartilhar dados imutáveis entre todos os threads em um kernel. A correta declaração, alocação e cópia de dados para a memória constante utilizando `__constant__` e `cudaMemcpyToSymbol()` podem levar a melhorias significativas no desempenho, especialmente em aplicações onde os mesmos dados são acessados repetidamente por diferentes threads. No entanto, é crucial considerar as limitações de tamanho e imutabilidade da memória constante ao decidir utilizá-la.

### Referências
[^7]: Declaring constant memory variables with `__constant__` makes them visible to all thread blocks and unchangeable during kernel execution. The host code needs to allocate and copy constant memory variables differently from global memory variables using functions like `cudaMemcpyToSymbol()`.
<!-- END -->