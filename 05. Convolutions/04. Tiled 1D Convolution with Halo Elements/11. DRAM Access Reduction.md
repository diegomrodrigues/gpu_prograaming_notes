Okay, I've added Mermaid diagrams to enhance the text, focusing on visualizing the memory hierarchy and data flow in CUDA convolution kernels. Here's the enhanced text with the diagrams:

## DRAM Access Reduction in CUDA Convolution Kernels

```mermaid
graph LR
    A["Input Data (Global Memory)"] --> B{"Load to Shared Memory (Tiling)"};
    B --> C{"Compute (Threads)"};
    C --> D["Output Data (Global Memory)"];
    subgraph "Memory Hierarchy"
      direction TB
      E["Registers"] --> F["Shared Memory"]
      F --> G["L1 Cache"]
      G --> H["L2 Cache"]
      H --> I["Global Memory (DRAM)"]
    end
    style I fill:#f9f,stroke:#333,stroke-width:2px
    style A fill:#ccf,stroke:#333,stroke-width:1px
    style D fill:#ccf,stroke:#333,stroke-width:1px
    style B fill:#fff,stroke:#333,stroke-width:1px
    style C fill:#fff,stroke:#333,stroke-width:1px
    linkStyle 0,3 stroke:#ccf,stroke-width:2px
    linkStyle 1,2 stroke:#afa,stroke-width:2px
```

### Introdu√ß√£o

A **redu√ß√£o do acesso √† DRAM**, tamb√©m conhecida como mem√≥ria global, √© um objetivo fundamental no desenvolvimento de kernels CUDA para convolu√ß√£o, e que tamb√©m se aplica em qualquer outro problema. A mem√≥ria global, como visto em cap√≠tulos anteriores, √© a mem√≥ria principal da GPU, que possui alta lat√™ncia e largura de banda limitada. A minimiza√ß√£o do tr√°fego nessa mem√≥ria √© crucial para que o desempenho do kernel seja o mais alto poss√≠vel, e tamb√©m para que o potencial da GPU seja utilizado ao m√°ximo. Neste cap√≠tulo, exploraremos como o acesso √† DRAM afeta o desempenho de kernels CUDA para convolu√ß√£o, quais t√©cnicas podem ser utilizadas para reduzir o tr√°fego na DRAM, como o *tiling*, o *caching* e a utiliza√ß√£o da mem√≥ria compartilhada, e como essas estrat√©gias podem levar a um melhor desempenho.

### Conceitos Fundamentais da Redu√ß√£o do Acesso √† DRAM

A redu√ß√£o do acesso √† DRAM (mem√≥ria global) em kernels CUDA √© um objetivo central de diversas t√©cnicas de otimiza√ß√£o, e ela se baseia no princ√≠pio de que a mem√≥ria global possui uma lat√™ncia mais alta e uma largura de banda menor, em rela√ß√£o a outros tipos de mem√≥ria na GPU, como a mem√≥ria compartilhada, os registradores e os *caches*.

**Conceito 1: A Mem√≥ria Global como um Gargalo**

A mem√≥ria global (DRAM) √© um gargalo no desempenho de aplica√ß√µes CUDA, devido √† sua alta lat√™ncia e baixa largura de banda, em compara√ß√£o com as mem√≥rias *on-chip*. Cada acesso √† mem√≥ria global pode custar centenas de ciclos de *clock*, e esse tempo de espera pode causar um grande impacto no desempenho, especialmente em aplica√ß√µes que fazem uso intensivo da mem√≥ria.

**Lemma 1:** *A mem√≥ria global √© um gargalo no desempenho de kernels CUDA, devido √† sua alta lat√™ncia e baixa largura de banda, e a minimiza√ß√£o do acesso a essa mem√≥ria √© fundamental para otimizar o desempenho.*

**Prova:** O acesso √† mem√≥ria global √© mais lento do que o acesso aos registradores, ao *cache* e √† mem√≥ria compartilhada. Um n√∫mero excessivo de acessos √† mem√≥ria global diminui a efici√™ncia do processamento paralelo na GPU e aumenta o tempo total de execu√ß√£o do kernel. $\blacksquare$

**Conceito 2: Abordagens para Reduzir o Acesso √† DRAM**

Existem diversas abordagens para reduzir o acesso √† DRAM, que podem ser combinadas para obter o melhor resultado:

1.  **Reutiliza√ß√£o da Mem√≥ria Compartilhada:** Utilizar a mem√≥ria compartilhada para armazenar dados que ser√£o utilizados repetidamente pelos threads de um bloco, evitando acessos repetidos √† mem√≥ria global, e tamb√©m reduzindo a lat√™ncia dos acessos, j√° que a mem√≥ria compartilhada tem lat√™ncia menor.

2.  **Utiliza√ß√£o da Mem√≥ria Constante:** Utilizar a mem√≥ria constante para armazenar dados que n√£o s√£o modificados pelo kernel, como a *convolution mask*. Os dados na mem√≥ria constante s√£o acessados atrav√©s de um *cache*, que tem lat√™ncia menor que a mem√≥ria global.
3.  **Tiling:** Utilizar *tiling* para carregar blocos menores de dados na mem√≥ria compartilhada, o que reduz o n√∫mero de acessos √† mem√≥ria global. Ao dividir a entrada em *tiles*, e carregar cada *tile* na mem√≥ria compartilhada, o acesso √† mem√≥ria global se torna menos frequente e mais eficiente.
4.  **Acesso Coalescente:** Utilizar o acesso coalescente √† mem√≥ria global, para que os threads acessem dados que estejam cont√≠guos na mem√≥ria. A escolha da forma de acesso √† mem√≥ria global √© fundamental para otimizar o desempenho do acesso aos dados.
5.  **Pre-fetching:** Utilizar *pre-fetching* para trazer os dados do pr√≥ximo *tile* para a mem√≥ria compartilhada ou *cache*, enquanto o *tile* atual est√° sendo processado. Isso permite que a lat√™ncia do acesso √† mem√≥ria global seja escondida por outras tarefas que est√£o sendo executadas em paralelo.

> üí° **Dica:** A escolha de qual estrat√©gia utilizar depende do problema e da sua caracter√≠stica, e a combina√ß√£o dessas abordagens √© geralmente a melhor forma de garantir um alto desempenho.

**Corol√°rio 1:** *A redu√ß√£o do acesso √† DRAM, atrav√©s do uso da mem√≥ria compartilhada, da mem√≥ria constante, do tiling, do acesso coalescente e do pre-fetching, permite aumentar a efici√™ncia do acesso √† mem√≥ria e reduzir a lat√™ncia, o que tem um impacto direto no desempenho dos kernels CUDA para convolu√ß√£o.*

**Conceito 3: A Hierarquia de Mem√≥ria e o Acesso √† DRAM**

A escolha dos n√≠veis da hierarquia de mem√≥ria que s√£o utilizados √© fundamental para reduzir a necessidade de acessos √† DRAM. Registradores, *caches* L1 e L2, mem√≥ria compartilhada e mem√≥ria constante devem ser utilizados sempre que poss√≠vel, j√° que o acesso √† DRAM √© o mais lento. Uma arquitetura bem planejada para o acesso √† mem√≥ria √© fundamental para garantir o alto desempenho de kernels CUDA para convolu√ß√£o.

### T√©cnicas para Reduzir o Acesso √† DRAM em Kernels CUDA

```mermaid
sequenceDiagram
    participant Global Memory (DRAM)
    participant Shared Memory
    participant Constant Memory
    participant Threads
    
    Global Memory (DRAM) ->> Shared Memory: Load input tile
    Global Memory (DRAM) ->> Constant Memory: Load convolution mask
    Shared Memory -->> Threads: Access tile data
    Constant Memory -->> Threads: Access mask data
    Threads ->> Threads: Compute convolution
    Threads -->> Global Memory (DRAM): Write output tile
```

Existem v√°rias estrat√©gias para reduzir o acesso √† mem√≥ria global (DRAM) em kernels CUDA para convolu√ß√£o:

1.  **Mem√≥ria Compartilhada:** O uso da mem√≥ria compartilhada para carregar os *input tiles* (incluindo os *halo elements*) √© fundamental para reduzir o n√∫mero de acessos √† mem√≥ria global. Ao carregar os dados na mem√≥ria compartilhada, os threads podem reutilizar esses dados sem a necessidade de acessos adicionais √† mem√≥ria global, o que diminui a lat√™ncia e aumenta a largura de banda.
    ```cpp
     __shared__ float N_ds [TILE_SIZE + MAX_MASK_WIDTH - 1];
    ```

2. **Mem√≥ria Constante:** A utiliza√ß√£o da mem√≥ria constante para armazenar a *convolution mask* (M) elimina a necessidade de que todos os threads acessem essa m√°scara na mem√≥ria global. Ao utilizar a mem√≥ria constante, o acesso √© feito atrav√©s do cache, que tem baixa lat√™ncia e uma alta largura de banda.
     ```cpp
    __constant__ float M[MAX_MASK_WIDTH];
     ```

3. **Tiling:** O uso de *tiling* divide o problema de convolu√ß√£o em problemas menores, e o carregamento de um *tile* na mem√≥ria compartilhada permite que a quantidade de dados que precisam ser acessados na mem√≥ria global seja reduzida.
4.  **Acesso Coalescente:** O acesso √† mem√≥ria global deve ser feito atrav√©s de acessos coalescentes, onde threads de um mesmo warp acessam posi√ß√µes cont√≠nuas da mem√≥ria global. Para que isso ocorra, a aloca√ß√£o dos dados na mem√≥ria global, e o c√°lculo dos √≠ndices para os acessos devem ser planejados de forma a respeitar o padr√£o de acesso coalescente.
    ```cpp
      N_ds[threadIdx.x] = N[blockIdx.x*blockDim.x + threadIdx.x];
    ```
5. **Pre-Fetching:** O *pre-fetching* carrega os dados de entrada para o *cache* antes que eles sejam necess√°rios, o que minimiza a lat√™ncia e tamb√©m reduz o tr√°fego na mem√≥ria global. O *prefetching* √© realizado atrav√©s da antecipa√ß√£o da leitura dos dados da entrada, antes que eles sejam realmente necess√°rios para o c√°lculo da convolu√ß√£o.
6. **Reutiliza√ß√£o:** A reutiliza√ß√£o dos dados que j√° foram carregados na mem√≥ria compartilhada √© uma estrat√©gia importante para reduzir o n√∫mero de acessos √† mem√≥ria global. O planejamento dos algoritmos deve levar em considera√ß√£o a reutiliza√ß√£o dos dados, de forma que o acesso √† mem√≥ria global seja feito apenas quando necess√°rio.

**Lemma 5:** *A redu√ß√£o do acesso √† DRAM em kernels CUDA para convolu√ß√£o √© realizada atrav√©s da combina√ß√£o de diferentes t√©cnicas, como a utiliza√ß√£o da mem√≥ria compartilhada, da mem√≥ria constante, do tiling, do acesso coalescente, do pre-fetching e da reutiliza√ß√£o dos dados na mem√≥ria compartilhada.*

**Prova:** A aplica√ß√£o combinada dessas t√©cnicas permite a otimiza√ß√£o do uso da hierarquia de mem√≥ria, o que reduz o tr√°fego da mem√≥ria global, e o tempo total de acesso aos dados para o c√°lculo da convolu√ß√£o. $\blacksquare$

**Corol√°rio 5:** *O uso estrat√©gico dos diferentes n√≠veis de mem√≥ria, a organiza√ß√£o adequada dos acessos e a utiliza√ß√£o de t√©cnicas como o tiling s√£o fundamentais para reduzir o acesso √† DRAM e aumentar o desempenho de kernels CUDA para convolu√ß√£o.*

### An√°lise Te√≥rica Avan√ßada da Redu√ß√£o do Acesso √† DRAM

**Pergunta Te√≥rica Avan√ßada 1:** *Como a escolha do tamanho do tile e do tamanho da convolution mask influencia o n√∫mero de acessos √† DRAM em kernels CUDA para convolu√ß√£o, e quais trade-offs s√£o necess√°rios para minimizar o tr√°fego na mem√≥ria global?*

**Resposta:**

A escolha do **tamanho do *tile*** e do **tamanho da *convolution mask*** tem um impacto direto no **n√∫mero de acessos √† DRAM** (mem√≥ria global) em kernels CUDA para convolu√ß√£o. A escolha do tamanho adequado desses dois par√¢metros, em conjunto com as caracter√≠sticas da arquitetura da GPU, permite minimizar a necessidade de acesso √† mem√≥ria global, e o planejamento deve considerar os *trade-offs* envolvidos.

**Lemma 6:** *A escolha do tamanho do tile e da convolution mask influencia o n√∫mero de acessos √† DRAM, e o tamanho ideal deve buscar um balan√ßo entre a necessidade de reutiliza√ß√£o da mem√≥ria compartilhada, a quantidade de halo elements e o tr√°fego na mem√≥ria global.*

**Prova:** O uso de tiles muito pequenos faz com que mais dados tenham que ser lidos, para tratar de *boundary conditions*, e de *halo elements*, enquanto um *tile* muito grande pode n√£o caber na mem√≥ria compartilhada, e causar um n√∫mero maior de acessos √† mem√≥ria global. A escolha dos tamanhos de *tile* e *mask* deve buscar um balan√ßo entre a reutiliza√ß√£o de dados na mem√≥ria compartilhada, e a minimiza√ß√£o da necessidade de acesso √† mem√≥ria global. $\blacksquare$

A intera√ß√£o entre o tamanho do *tile* e o tamanho da *convolution mask* com o acesso √† DRAM:

1.  **Tiles Pequenos:** Ao se utilizar *tiles* pequenos, o kernel acaba necessitando de mais acessos √† mem√≥ria global, j√° que uma maior quantidade de *halo elements* √© necess√°ria, e menos dados da entrada s√£o acessados com um √∫nico acesso.
2.  **Tiles Grandes:** A utiliza√ß√£o de *tiles* grandes pode reduzir o n√∫mero de acessos √† mem√≥ria global, mas pode tamb√©m reduzir a reutiliza√ß√£o da mem√≥ria compartilhada, e exigir um espa√ßo maior na mem√≥ria compartilhada, e tamb√©m pode levar a acessos n√£o coalescentes √† mem√≥ria.
3.  **M√°scaras Pequenas:** M√°scaras menores exigem menos *halo elements*, e consequentemente, um menor n√∫mero de acessos √† mem√≥ria global para o c√°lculo da convolu√ß√£o, mas, com isso, a √°rea de abrang√™ncia da opera√ß√£o de convolu√ß√£o √© reduzida.
4.  **M√°scaras Grandes:** M√°scaras grandes necessitam de mais *halo elements*, e, com isso, mais acessos √† mem√≥ria global para a realiza√ß√£o da opera√ß√£o de convolu√ß√£o, o que aumenta a lat√™ncia e diminui o desempenho do kernel.

O *trade-off* na escolha entre os tamanhos do *tile* e da m√°scara envolve o balanceamento do acesso √† mem√≥ria compartilhada, da largura de banda da mem√≥ria global e do impacto da largura da *convolution mask* nos c√°lculos, e a escolha ideal deve ser feita em fun√ß√£o da aplica√ß√£o e do tipo de problema que est√° sendo resolvido.

**Corol√°rio 6:** *A escolha do tamanho do tile e do tamanho da convolution mask influencia o n√∫mero de acessos √† DRAM e o desempenho do kernel, e o tamanho ideal deve balancear a reutiliza√ß√£o dos dados, o tr√°fego na mem√≥ria global e o tamanho da m√°scara, para maximizar o desempenho da aplica√ß√£o.*

**Pergunta Te√≥rica Avan√ßada 2:** *Como a utiliza√ß√£o de t√©cnicas de pre-fetching de dados afeta a lat√™ncia do acesso √† mem√≥ria global durante o carregamento da mem√≥ria compartilhada e qual o impacto dessa abordagem no desempenho de kernels CUDA para convolu√ß√£o com tiling?*

**Resposta:**

A utiliza√ß√£o de t√©cnicas de **pre-fetching** de dados afeta a **lat√™ncia do acesso √† mem√≥ria global** durante o carregamento da mem√≥ria compartilhada em kernels CUDA para convolu√ß√£o com *tiling*. O *pre-fetching* envolve trazer os dados da mem√≥ria global para um n√≠vel de *cache* ou para a mem√≥ria compartilhada, antes que eles sejam realmente necess√°rios, de forma que a lat√™ncia do acesso seja minimizada, j√° que o acesso a esses dados ocorre em n√≠veis de mem√≥ria com baixa lat√™ncia, e tamb√©m o tempo de espera para o acesso √† mem√≥ria pode ser parcialmente ocultado pela execu√ß√£o de outras opera√ß√µes.

**Lemma 7:** *O pre-fetching de dados reduz a lat√™ncia do acesso √† mem√≥ria global e aumenta a efici√™ncia do carregamento da mem√≥ria compartilhada em kernels CUDA para convolu√ß√£o, e o tempo total gasto para o acesso √† mem√≥ria √© menor.*

**Prova:** Ao carregar dados antecipadamente, √© poss√≠vel minimizar o tempo de espera para a realiza√ß√£o do acesso √† mem√≥ria global. Se os dados j√° estiverem em *cache* ou na mem√≥ria compartilhada quando o thread precisar deles, a lat√™ncia de acesso ser√° menor, o que resulta em um melhor desempenho do kernel. $\blacksquare$

O **pre-fetching** de dados pode ser utilizado da seguinte forma:

1.  **Overlapping:** O *pre-fetching* pode ser utilizado para carregar os dados do pr√≥ximo *tile* na mem√≥ria compartilhada, enquanto o kernel est√° realizando a computa√ß√£o do *tile* atual, ou seja, a transfer√™ncia de dados √© feita de forma paralela ao processamento dos dados que j√° est√£o dispon√≠veis, o que reduz o tempo de espera.
2.  **Mem√≥ria Compartilhada:** A mem√≥ria compartilhada pode ser utilizada para fazer o pre-fetching dos dados, de forma que as opera√ß√µes de escrita e leitura nessa regi√£o de mem√≥ria ocorram com baixa lat√™ncia, minimizando o uso da mem√≥ria global e da hierarquia de *caches*.
3.  **Streams:** O *pre-fetching* pode ser implementado com o uso de *streams*, em opera√ß√µes ass√≠ncronas, onde a transfer√™ncia e a computa√ß√£o podem ocorrer em paralelo, e a utiliza√ß√£o de m√∫ltiplas *streams* permite que as transfer√™ncias e opera√ß√µes ocorram de forma sobreposta.
4. **Redu√ß√£o da Lat√™ncia:** O pre-fetching pode reduzir a lat√™ncia do acesso √† mem√≥ria, j√° que ao ter os dados dispon√≠veis em mem√≥ria mais r√°pida, a necessidade de acesso √† mem√≥ria global √© reduzida.

A combina√ß√£o de *pre-fetching* com outras t√©cnicas, como o acesso coalescente √† mem√≥ria, e a utiliza√ß√£o da mem√≥ria compartilhada e da mem√≥ria constante, pode levar a um desempenho maior para os kernels CUDA para convolu√ß√£o.

**Corol√°rio 7:** *O pre-fetching de dados, em conjunto com o uso da mem√≥ria compartilhada, de acesso coalescente e da mem√≥ria constante, √© uma abordagem para reduzir a lat√™ncia de acesso e otimizar o desempenho de kernels CUDA para convolu√ß√£o, atrav√©s da utiliza√ß√£o mais eficiente dos recursos do hardware.*

### Dedu√ß√£o Te√≥rica Complexa: Modelagem do Tempo de Execu√ß√£o da Convolu√ß√£o com Redu√ß√£o de Acessos √† DRAM

```mermaid
graph LR
    A[Start] --> B{Load Input Data (Tiled)};
    B --> C{Load Convolution Mask};
    C --> D{Compute Convolution (Shared Memory)};
    D --> E{Write Output Data};
    E --> F[End];
    style B fill:#afa,stroke:#333,stroke-width:1px
    style C fill:#afa,stroke:#333,stroke-width:1px
    style E fill:#afa,stroke:#333,stroke-width:1px
```

O **tempo de execu√ß√£o** de uma convolu√ß√£o com a **redu√ß√£o do acesso √† DRAM** pode ser modelado levando em considera√ß√£o o tempo gasto na leitura de dados da mem√≥ria global, o tempo gasto na computa√ß√£o da convolu√ß√£o, e o tempo gasto na transfer√™ncia de dados entre a mem√≥ria global e os n√≠veis de mem√≥ria mais r√°pidos, como o *cache* e a mem√≥ria compartilhada.

O tempo de execu√ß√£o pode ser modelado como:

$$
T_{kernel} = T_{memoria} + T_{computacao}
$$

Onde $T_{memoria}$ representa o tempo gasto no acesso √† mem√≥ria e $T_{computacao}$ representa o tempo gasto na computa√ß√£o da convolu√ß√£o.

**Lemma 8:** *O tempo de execu√ß√£o de um kernel de convolu√ß√£o √© composto pelo tempo de acesso √† mem√≥ria, que √© influenciado pelo n√∫mero de acessos √† DRAM e o tempo para computar a convolu√ß√£o. A redu√ß√£o do tempo gasto em qualquer uma dessas etapas permite um desempenho maior.*

**Prova:** A lat√™ncia do acesso √† mem√≥ria √© um gargalo no desempenho, e tamb√©m o n√∫mero de opera√ß√µes computacionais tem um impacto direto no tempo de execu√ß√£o do kernel. A redu√ß√£o do tempo em cada etapa leva a um menor tempo de execu√ß√£o total. $\blacksquare$

O tempo para acessar a mem√≥ria, $T_{memoria}$, pode ser modelado como:
$$
T_{memoria} = N_{global}*Lat_{global} + \frac{Data_{global}}{BW_{global}} +  N_{shared}*Lat_{shared} + \frac{Data_{shared}}{BW_{shared}} + T_{cache}
$$
Onde $N_{global}$ representa o n√∫mero de acessos √† mem√≥ria global, $Lat_{global}$ a lat√™ncia do acesso, $Data_{global}$ a quantidade de dados acessados na mem√≥ria global, $BW_{global}$ a largura de banda da mem√≥ria global, $N_{shared}$ o n√∫mero de acessos √† mem√≥ria compartilhada, $Lat_{shared}$ a lat√™ncia da mem√≥ria compartilhada, $Data_{shared}$ a quantidade de dados acessados da mem√≥ria compartilhada, e $T_{cache}$ o tempo de acesso ao *cache* da mem√≥ria constante. O tempo para o c√°lculo da convolu√ß√£o, $T_{computacao}$ , pode ser modelado como:
$$
T_{computacao} =  \frac{N_{op}}{P} * T_{op}
$$

Onde $N_{op}$ representa o n√∫mero de opera√ß√µes, P o n√∫mero de threads e  $T_{op}$ o tempo de uma opera√ß√£o.

A escolha da arquitetura do kernel, da forma como o acesso √† mem√≥ria √© organizado, e da utiliza√ß√£o da mem√≥ria compartilhada e da mem√≥ria constante afeta diretamente o tempo para acesso √† mem√≥ria, e a modelagem do tempo de execu√ß√£o permite analisar como as otimiza√ß√µes influenciam o desempenho do kernel.

**Corol√°rio 8:** *O modelo do tempo de execu√ß√£o da convolu√ß√£o com redu√ß√£o de acessos √† DRAM permite analisar os diferentes componentes que influenciam o desempenho do kernel, e como a redu√ß√£o do acesso √† mem√≥ria global, o uso da mem√≥ria compartilhada e do cache, e a otimiza√ß√£o do c√≥digo, contribuem para a redu√ß√£o do tempo total de execu√ß√£o.*

### Conclus√£o

(Nota: N√£o conclua o cap√≠tulo at√© que o usu√°rio solicite.)

### Refer√™ncias

[^1]: "In the next several chapters, we will discuss a set of important parallel computation patterns. These patterns are the basis of many parallel algorithms that appear in applications." *(Trecho de <Parallel Patterns: Convolution>)*

[^2]: "Mathematically, convolution is an array operation where each output data element is a weighted sum of a collection of neighboring input elements. The weights used in the weighted sum calculation are defined by an input mask array, commonly referred to as the convolution kernel." *(Trecho de <Parallel Patterns: Convolution>)*
[^3]: "Because convolution is defined in terms of neighboring elements, boundary conditions naturally exist for output elements that are close to the ends of an array." *(Trecho de <Parallel Patterns: Convolution>)*

[^4]: "Kernel functions access constant memory variables as global variables. Thus, their pointers do not need to be passed to the kernel as parameters." *(Trecho de <Parallel Patterns: Convolution>)*

[^5]: "For image processing and computer vision, input data is usually in 2D form, with pixels in an x-y space. Image convolutions are also two dimensional." *(Trecho de <Parallel Patterns: Convolution>)*

[^6]: "A more serious problem is memory bandwidth. The ratio of floating-point arithmetic calculation to global memory accesses is only about 1.0 in the kernel." *(Trecho de <Parallel Patterns: Convolution>)*

[^7]: "The CUDA programming model allows programmers to declare a variable in the constant memory. Like global memory variables, constant memory variables are also visible to all thread blocks. The main difference is that a constant memory variable cannot be changed by threads during kernel execution. Furthermore, the size of the constant memory can vary from device to device." *(Trecho de <Parallel Patterns: Convolution>)*

[^8]:  "We will discuss two input data tiling strategies for reducing the total number of global memory accesses." *(Trecho de <Parallel Patterns: Convolution>)*

[^9]:  "Constant memory variables play an interesting role in using caches in massively parallel processors. Since they are not changed during kernel execution, there is no cache coherence issue during the execution of a kernel." *(Trecho de <Parallel Patterns: Convolution>)*
[^10]:  "Furthermore, the design of caches in these processors is typically optimized to broadcast a value to a large number of threads." *(Trecho de <Parallel Patterns: Convolution>)*

[^11]: "A major design issue with using caches in a massively parallel processor is cache coherence, which arises when one or more processor cores modify cached data." *(Trecho de <Parallel Patterns: Convolution>)*
[^12]: "To mitigate the effect of memory bottleneck, modern processors commonly employ on-chip cache memories, or caches, to reduce the number of variables that need to be accessed from DRAM." *(Trecho de <Parallel Patterns: Convolution>)*
[^13]:  "In modern processors, accessing a variable from DRAM takes hundreds if not thousands of clock cycles." *(Trecho de <Parallel Patterns: Convolution>)*
[^14]: "Unlike CUDA shared memory, or scratchpad memories in general, caches are 'transparent‚Äô to programs." *(Trecho de <Parallel Patterns: Convolution>)*
[^15]:  "Like global memory variables, constant memory variables are also located in DRAM." *(Trecho de <Parallel Patterns: Convolution>)*

Deseja que eu continue com as pr√≥ximas se√ß√µes?
